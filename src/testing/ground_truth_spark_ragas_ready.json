{
  "test_suite": "RAG System - Spark Project",
  "version": "2.0",
  "created": "2025-11-12",
  "description": "Test suite with actual code fragments in ground_truth_contexts",
  "questions": [
    {
      "id": "Q001",
      "question": "Describe ContextWaiter class",
      "category": "definition",
      "ground_truth_contexts": [
        "private[streaming] class ContextWaiter { private val lock = new ReentrantLock() private val condition = lock.newCondition() // Guarded by \"lock\" private var error: Throwable = null // Guarded by \"lock\" private var stopped: Boolean = false def notifyError(e: Throwable): Unit = { lock.lock() try { error = e condition.signalAll() } finally { lock.unlock() } } def notifyStop(): Unit = { lock.lock() try { stopped = true condition.signalAll() } finally { lock.unlock() } } /** * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or * `false` if the waiting time detectably elapsed before return from the method. */ def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }",
        "*/ class StreamingContext private[streaming] ( _sc: SparkContext, _cp: Checkpoint, _batchDur: Duration ) extends Logging { /** * Create a StreamingContext using an existing SparkContext. * @param sparkContext existing SparkContext * @param batchDuration the time interval at which streaming data will be divided into batches */ def this(sparkContext: SparkContext, batchDuration: Duration) = { this(sparkContext, null, batchDuration) } /** * Create a StreamingContext by providing the configuration necessary for a new SparkContext. * @param conf a org.apache.spark.SparkConf object specifying Spark parameters * @param batchDuration the time interval at which streaming data will be divided into batches */ def this(conf: SparkConf, batchDuration: Duration) = { this(StreamingContext.createNewSparkContext(conf), null, batchDuration) } /** * Create a StreamingContext by providing the details necessary for creating a new SparkContext. * @param master cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName a name for your job, to display on the cluster web UI * @param batchDuration the time interval at which streaming data will be divided into batches */ def this( master: String, appName: String, batchDuration: Duration, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment), null, batchDuration) } /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory * @param hadoopConf Optional, configuration object if necessary for reading from * HDFS compatible filesystems */ def this(path: String, hadoopConf: Configuration) = this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null) /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory */ def this(path: String) = this(path, SparkHadoopUtil.get.conf) /** * Recreate a StreamingContext from a checkpoint file using an existing SparkContext. * @param path Path to the directory that was specified as the checkpoint directory * @param sparkContext Existing SparkContext */ def this(path: String, sparkContext: SparkContext) = { this( sparkContext, CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull, null) } require(_sc != null || _cp != null, \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\") private[streaming] val isCheckpointPresent: Boolean = _cp != null private[streaming] val sc: SparkContext = { if (_sc != null) { _sc } else if (isCheckpointPresent) { SparkContext.getOrCreate(_cp.createSparkConf()) } else { throw new SparkException(\"Cannot create StreamingContext without a SparkContext\") } } if (sc.conf.get(\"spark.master\") == \"local\" || sc.conf.get(\"spark.master\") == \"local[1]\") { logWarning(\"spark.master should be set as local[n], n > 1 in local mode if you have receivers\" + \" to get data, otherwise Spark jobs will not get resources to process the received data.\") } private[streaming] val conf = sc.conf private[streaming] val env = sc.env private[streaming] val graph: DStreamGraph = { if (isCheckpointPresent) { _cp.graph.setContext(this) _cp.graph.restoreCheckpointData() _cp.graph } else { require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\") val newGraph = new DStreamGraph() newGraph.setBatchDuration(_batchDur) newGraph } } private val nextInputStreamId = new AtomicInteger(0) private[streaming] var checkpointDir: String = { if (isCheckpointPresent) { sc.setCheckpointDir(_cp.checkpointDir) _cp.checkpointDir } else { null } } private[streaming] val checkpointDuration: Duration = { if (isCheckpointPresent) _cp.checkpointDuration else graph.batchDuration } private[streaming] val scheduler = new JobScheduler(this) private[streaming] val waiter = new ContextWaiter private[streaming] val progressListener = new StreamingJobProgressListener(this) private[streaming] val uiTab: Option[StreamingTab] = sparkContext.ui match { case Some(ui) => Some(new StreamingTab(this, ui)) case None => None } /* Initializing a streamingSource to register metrics */ private val streamingSource = new StreamingSource(this) private var state: StreamingContextState = INITIALIZED private val startSite = new AtomicReference[CallSite](null) // Copy of thread-local properties from SparkContext. These properties will be set in all tasks // submitted by this StreamingContext after start. private[streaming] val savedProperties = new AtomicReference[Properties](new Properties) private[streaming] def getStartSite(): CallSite = startSite.get() private var shutdownHookRef: AnyRef = _ conf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint) /** * Return the associated Spark context */ def sparkContext: SparkContext = sc /** * Set each DStream in this context to remember RDDs it generated in the last given duration. * DStreams remember RDDs only for a limited duration of time and release them for garbage * collection. This method allows the developer to specify how long to remember the RDDs ( * if the developer wishes to query old data outside the DStream computation). * @param duration Minimum duration that each DStream should remember its RDDs */ def remember(duration: Duration): Unit = { graph.remember(duration) } /** * Set the context to periodically checkpoint the DStream operations for driver * fault-tolerance. * @param directory HDFS-compatible directory where the checkpoint data will be reliably stored. * Note that this must be a fault-tolerant file system like HDFS. */ def checkpoint(directory: String): Unit = { if (directory != null) { val path = new Path(directory) val fs = path.getFileSystem(sparkContext.hadoopConfiguration) fs.mkdirs(path) val fullPath = fs.getFileStatus(path).getPath().toString sc.setCheckpointDir(fullPath) checkpointDir = fullPath } else { checkpointDir = null } } private[streaming] def isCheckpointingEnabled: Boolean = { checkpointDir != null } private[streaming] def initialCheckpoint: Checkpoint = { if (isCheckpointPresent) _cp else null } private[streaming] def getNewInputStreamId() = nextInputStreamId.getAndIncrement() /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body. */ private[streaming] def withScope[U](body: => U): U = sparkContext.withScope(body) /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body. */ private[streaming] def withNamedScope[U](name: String)(body: => U): U = { RDDOperationScope.withScope(sc, name, allowNesting = false, ignoreParent = false)(body) } /** * Create an input stream with any arbitrary user implemented receiver. * Find more details at https://spark.apache.org/docs/latest/streaming-custom-receivers.html * @param receiver Custom implementation of Receiver */ def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T] = { withNamedScope(\"receiver stream\") { new PluggableInputDStream[T](this, receiver) } } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\\n` delimited * lines. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @see [[socketStream]] */ def socketTextStream( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[String] = withNamedScope(\"socket text stream\") { socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel) } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes it interpreted as object using the given * converter. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param converter Function to convert the byte stream to objects * @param storageLevel Storage level to use for storing the received objects * @tparam T Type of the objects received (after converting bytes to objects) */ def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel ): ReceiverInputDStream[T] = { new SocketInputDStream[T](this, hostname, port, converter, storageLevel) } /** * Create an input stream from network source hostname:port, where data is received * as serialized blocks (serialized using the Spark's serializer) that can be directly * pushed into the block manager without deserializing them. This is the most efficient * way to receive data. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @tparam T Type of the objects in the received blocks */ def rawSocketStream[T: ClassTag]( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[T] = withNamedScope(\"raw socket stream\") { new RawInputDStream[T](this, hostname, port, storageLevel) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file */ def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file */ def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @param conf Hadoop configuration * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file */ def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean, conf: Configuration): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly, Option(conf)) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as text files (using key as LongWritable, value * as Text and input format as TextInputFormat). Files must be written to the * monitored directory by \"moving\" them from another location within the same * file system. File names starting with . are ignored. * The text files must be encoded as UTF-8. * * @param directory HDFS directory to monitor for new file */ def textFileStream(directory: String): DStream[String] = withNamedScope(\"text file stream\") { fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as flat binary files, assuming a fixed length per record, * generating one byte array per record. Files must be written to the monitored directory * by \"moving\" them from another location within the same file system. File names * starting with . are ignored. * * @param directory HDFS directory to monitor for new file * @param recordLength length of each record in bytes * * @note We ensure that the byte array for each record in the * resulting RDDs of the DStream has the provided record length. */ def binaryRecordsStream( directory: String, recordLength: Int): DStream[Array[Byte]] = withNamedScope(\"binary records stream\") { val conf = _sc.hadoopConfiguration conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = fileStream[LongWritable, BytesWritable, FixedLengthBinaryInputFormat]( directory, FileInputDStream.defaultFilter: Path => Boolean, newFilesOnly = true, conf) br.map { case (k, v) => val bytes = v.copyBytes() require(bytes.length == recordLength, \"Byte array does not have correct length. \" + s\"${bytes.length} did not equal recordLength: $recordLength\") bytes } } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing. */ def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean = true ): InputDStream[T] = { queueStream(queue, oneAtATime, sc.makeRDD(Seq.empty[T], 1)) } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @param defaultRDD Default RDD is returned by the DStream when the queue is empty. * Set as null if no RDD should be returned when empty * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing. */ def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T] ): InputDStream[T] = { new QueueInputDStream(this, queue, oneAtATime, defaultRDD) } /** * Create a unified DStream from multiple DStreams of the same type and same slide duration. */ def union[T: ClassTag](streams: Seq[DStream[T]]): DStream[T] = withScope { new UnionDStream[T](streams.toArray) } /** * Create a new DStream in which each RDD is generated by applying a function on RDDs of * the DStreams. */ def transform[T: ClassTag]( dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T] ): DStream[T] = withScope { new TransformedDStream[T](dstreams, sparkContext.clean(transformFunc)) } /** * Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for * receiving system events related to streaming. */ def addStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.addListener(streamingListener) } def removeStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.removeListener(streamingListener) } private def validate(): Unit = { assert(graph != null, \"Graph is null\") graph.validate() require( !isCheckpointingEnabled || checkpointDuration != null, \"Checkpoint directory has been set, but the graph checkpointing interval has \" + \"not been set. Please use StreamingContext.checkpoint() to set the interval.\" ) // Verify whether the DStream checkpoint is serializable if (isCheckpointingEnabled) { val checkpoint = new Checkpoint(this, Time(0)) try { Checkpoint.serialize(checkpoint, conf) } catch { case e: NotSerializableException => throw new NotSerializableException( \"DStream checkpointing has been enabled but the DStreams with their functions \" + \"are not serializable\\n\" + SerializationDebugger.improveException(checkpoint, e).getMessage() ) } } if (Utils.isDynamicAllocationEnabled(sc.conf) || ExecutorAllocationManager.isDynamicAllocationEnabled(conf)) { logWarning(\"Dynamic Allocation is enabled for this application. \" + \"Enabling Dynamic allocation for Spark Streaming applications can cause data loss if \" + \"Write Ahead Log is not enabled for non-replayable sources. \" + \"See the programming guide for details on how to enable the Write Ahead Log.\") } } /** * :: DeveloperApi :: * * Return the current state of the context. The context can be in three possible states - * * - StreamingContextState.INITIALIZED - The context has been created, but not started yet. * Input DStreams, transformations and output operations can be created on the context. * - StreamingContextState.ACTIVE - The context has been started, and not stopped. * Input DStreams, transformations and output operations cannot be created on the context. * - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more. */ @DeveloperApi def getState(): StreamingContextState = synchronized { state } /** * Start the execution of the streams. * * @throws IllegalStateException if the StreamingContext is already stopped. */ def start(): Unit = synchronized { state match { case INITIALIZED => startSite.set(DStream.getCreationSite()) StreamingContext.ACTIVATION_LOCK.synchronized { StreamingContext.assertNoOtherContextIsActive() try { validate() registerProgressListener() // Start the streaming scheduler in a new thread, so that thread local properties // like call sites and job groups can be reset without affecting those of the // current thread. ThreadUtils.runInNewThread(\"streaming-start\") { sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") savedProperties.set(Utils.cloneProperties(sparkContext.localProperties.get())) scheduler.start() } state = StreamingContextState.ACTIVE scheduler.listenerBus.post( StreamingListenerStreamingStarted(System.currentTimeMillis())) } catch { case NonFatal(e) => logError(\"Error starting the context, marking it as stopped\", e) scheduler.stop(false) state = StreamingContextState.STOPPED throw e } StreamingContext.setActiveContext(this) } logDebug(\"Adding shutdown hook\") // force eager creation of logger shutdownHookRef = ShutdownHookManager.addShutdownHook( StreamingContext.SHUTDOWN_HOOK_PRIORITY)(() => stopOnShutdown()) // Registering Streaming Metrics at the start of the StreamingContext assert(env.metricsSystem != null) env.metricsSystem.registerSource(streamingSource) uiTab.foreach(_.attach()) logInfo(\"StreamingContext started\") case ACTIVE => logWarning(\"StreamingContext has already been started\") case STOPPED => throw new IllegalStateException(\"StreamingContext has already been stopped\") } } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. */ def awaitTermination(): Unit = { waiter.waitForStopOrError() } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. * * @param timeout time to wait in milliseconds * @return `true` if it's stopped; or throw the reported error during the execution; or `false` * if the waiting time elapsed before returning from the method. */ def awaitTerminationOrTimeout(timeout: Long): Boolean = { waiter.waitForStopOrError(timeout) } /** * Stop the execution of the streams immediately (does not wait for all received data * to be processed). By default, if `stopSparkContext` is not specified, the underlying * SparkContext will also be stopped. This implicit behavior can be configured using the * SparkConf configuration spark.streaming.stopSparkContextByDefault. * * @param stopSparkContext If true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. */ def stop( stopSparkContext: Boolean = conf.getBoolean(\"spark.streaming.stopSparkContextByDefault\", true) ): Unit = synchronized { stop(stopSparkContext, false) } /** * Stop the execution of the streams, with option of ensuring all received data * has been processed. * * @param stopSparkContext if true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. * @param stopGracefully if true, stops gracefully by waiting for the processing of all * received data to be completed */ def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit = { var shutdownHookRefToRemove: AnyRef = null if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop StreamingContext within listener bus thread.\") } synchronized { // The state should always be Stopped after calling `stop()`, even if we haven't started yet state match { case INITIALIZED => logWarning(\"StreamingContext has not been started yet\") state = STOPPED case STOPPED => logWarning(\"StreamingContext has already been stopped\") state = STOPPED case ACTIVE => // It's important that we don't set state = STOPPED until the very end of this case, // since we need to ensure that we're still able to call `stop()` to recover from // a partially-stopped StreamingContext which resulted from this `stop()` call being // interrupted. See SPARK-12001 for more details. Because the body of this case can be // executed twice in the case of a partial stop, all methods called here need to be // idempotent. Utils.tryLogNonFatalError { scheduler.stop(stopGracefully) } // Removing the streamingSource to de-register the metrics on stop() Utils.tryLogNonFatalError { env.metricsSystem.removeSource(streamingSource) } Utils.tryLogNonFatalError { uiTab.foreach(_.detach()) } Utils.tryLogNonFatalError { unregisterProgressListener() } StreamingContext.setActiveContext(null) Utils.tryLogNonFatalError { waiter.notifyStop() } if (shutdownHookRef != null) { shutdownHookRefToRemove = shutdownHookRef shutdownHookRef = null } logInfo(\"StreamingContext stopped successfully\") state = STOPPED } } if (shutdownHookRefToRemove != null) { ShutdownHookManager.removeShutdownHook(shutdownHookRefToRemove) } // Even if we have already stopped, we still need to attempt to stop the SparkContext because // a user might stop(stopSparkContext = false) and then call stop(stopSparkContext = true). if (stopSparkContext) sc.stop() } private def stopOnShutdown(): Unit = { val stopGracefully = conf.get(STOP_GRACEFULLY_ON_SHUTDOWN) logInfo(s\"Invoking stop(stopGracefully=$stopGracefully) from shutdown hook\") // Do not stop SparkContext, let its own shutdown hook stop it stop(stopSparkContext = false, stopGracefully = stopGracefully) } private def registerProgressListener(): Unit = { addStreamingListener(progressListener) sc.addSparkListener(progressListener) sc.ui.foreach(_.setStreamingJobProgressListener(progressListener)) } private def unregisterProgressListener(): Unit = { removeStreamingListener(progressListener) sc.removeSparkListener(progressListener) sc.ui.foreach(_.clearStreamingJobProgressListener()) } } /** * StreamingContext object contains a number of utility functions related to the * StreamingContext class. */ object StreamingContext extends Logging { /** * Lock that guards activation of a StreamingContext as well as access to the singleton active * StreamingContext in getActiveOrCreate(). */ private val ACTIVATION_LOCK = new Object() private val SHUTDOWN_HOOK_PRIORITY = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY + 1 private val activeContext = new AtomicReference[StreamingContext](null) private def assertNoOtherContextIsActive(): Unit = { ACTIVATION_LOCK.synchronized { if (activeContext.get() != null) { throw new IllegalStateException( \"Only one StreamingContext may be started in this JVM. \" + \"Currently running StreamingContext was started at\" + activeContext.get.getStartSite().longForm) } } } private def setActiveContext(ssc: StreamingContext): Unit = { ACTIVATION_LOCK.synchronized { activeContext.set(ssc) } } /** * Get the currently active context, if there is one. Active means started but not stopped. */ def getActive(): Option[StreamingContext] = { ACTIVATION_LOCK.synchronized { Option(activeContext.get()) } } /** * Either return the \"active\" StreamingContext (that is, started but not stopped), or create a * new StreamingContext that is * @param creatingFunc Function to create a new StreamingContext */ def getActiveOrCreate(creatingFunc: () => StreamingContext): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { creatingFunc() } } } /** * Either get the currently active StreamingContext (that is, started but not stopped), * OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data * does not exist in the provided, then create a new StreamingContext by calling the provided * `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error. */ def getActiveOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { getOrCreate(checkpointPath, creatingFunc, hadoopConf, createOnError) } } } /** * Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. * If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be * recreated from the checkpoint data. If the data does not exist, then the StreamingContext * will be created by called the provided `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error. */ def getOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), hadoopConf, createOnError) checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc()) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to StreamingContext. */ def jarOfClass(cls: Class[_]): Option[String] = SparkContext.jarOfClass(cls) private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = { new SparkContext(conf) } private[streaming] def createNewSparkContext( master: String, appName: String, sparkHome: String, jars: Seq[String], environment: Map[String, String] ): SparkContext = { val conf = SparkContext.updatedConf( new SparkConf(), master, appName, sparkHome, jars, environment) new SparkContext(conf) } private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { var result = time.milliseconds.toString if (prefix != null && prefix.length > 0) { result = s\"$prefix-$result\" } if (suffix != null && suffix.length > 0) { result = s\"$result.$suffix\" } result } } private class StreamingContextPythonHelper { /** * This is a private method only for Python to implement `getOrCreate`. */ def tryRecoverFromCheckpoint(checkpointPath: String): Option[StreamingContext] = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), SparkHadoopUtil.get.conf, ignoreReadError = false) checkpointOption.map(new StreamingContext(null, _, null)) } }",
        "*/ def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }"
      ],
      "key_entities": [
        "org/apache/spark/streaming/ContextWaiter#",
        "org/apache/spark/streaming/StreamingContext#",
        "org/apache/spark/streaming/ContextWaiter#waitForStopOrError()."
      ],
      "claude_stats": {
        "answer": "The `ContextWaiter` class is a private streaming utility class that manages synchronization and signaling for stopping or error conditions in a streaming context.\n\nThe class maintains three key fields:\n- `lock` - a `ReentrantLock` for thread synchronization\n- `condition` - a `Condition` object created from the lock for thread coordination\n- `error` - a `Throwable` that stores any reported error (guarded by lock)\n- `stopped` - a `Boolean` flag indicating whether the context has stopped (guarded by lock)\n\nThe class provides three core methods:\n\n**`notifyError(e: Throwable)`** - Acquires the lock, stores the provided error in the `error` field, signals all waiting threads via `condition.signalAll()`, and releases the lock.\n\n**`notifyStop()`** - Acquires the lock, sets the `stopped` flag to true, signals all waiting threads, and releases the lock.\n\n**`waitForStopOrError(timeout: Long = -1)`** - Blocks the calling thread until either the context stops or an error occurs. It accepts an optional timeout in milliseconds (defaults to -1 for no timeout). If no timeout is specified, it waits indefinitely until `stopped` is true or `error` is non-null. With a timeout, it waits for the specified duration. If an error was reported, it throws that error. Returns `true` if stopped, or `false` if the timeout elapsed without stopping or error.\n\nBased on the `StreamingContext` code, I can see that `ContextWaiter` is instantiated as `private[streaming] val waiter = new ContextWaiter` and is used to coordinate the shutdown process and allow waiting for context termination.",
        "prompt_time": 0.253,
        "answer_time": 4,
        "tokens": 402,
        "context_tokens": 8153,
        "used_context": [
          "## CLASS: org/apache/spark/streaming/ContextWaiter#\nprivate[streaming] class ContextWaiter { private val lock = new ReentrantLock() private val condition = lock.newCondition() // Guarded by \"lock\" private var error: Throwable = null // Guarded by \"lock\" private var stopped: Boolean = false def notifyError(e: Throwable): Unit = { lock.lock() try { error = e condition.signalAll() } finally { lock.unlock() } } def notifyStop(): Unit = { lock.lock() try { stopped = true condition.signalAll() } finally { lock.unlock() } } /** * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or * `false` if the waiting time detectably elapsed before return from the method.  def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }",
          "## CLASS: org/apache/spark/streaming/StreamingContext#\n class StreamingContext private[streaming] ( _sc: SparkContext, _cp: Checkpoint, _batchDur: Duration ) extends Logging { /** * Create a StreamingContext using an existing SparkContext. * @param sparkContext existing SparkContext * @param batchDuration the time interval at which streaming data will be divided into batches  def this(sparkContext: SparkContext, batchDuration: Duration) = { this(sparkContext, null, batchDuration) } /** * Create a StreamingContext by providing the configuration necessary for a new SparkContext. * @param conf a org.apache.spark.SparkConf object specifying Spark parameters * @param batchDuration the time interval at which streaming data will be divided into batches  def this(conf: SparkConf, batchDuration: Duration) = { this(StreamingContext.createNewSparkContext(conf), null, batchDuration) } /** * Create a StreamingContext by providing the details necessary for creating a new SparkContext. * @param master cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName a name for your job, to display on the cluster web UI * @param batchDuration the time interval at which streaming data will be divided into batches  def this( master: String, appName: String, batchDuration: Duration, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment), null, batchDuration) } /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory * @param hadoopConf Optional, configuration object if necessary for reading from * HDFS compatible filesystems  def this(path: String, hadoopConf: Configuration) = this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null) /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory  def this(path: String) = this(path, SparkHadoopUtil.get.conf) /** * Recreate a StreamingContext from a checkpoint file using an existing SparkContext. * @param path Path to the directory that was specified as the checkpoint directory * @param sparkContext Existing SparkContext  def this(path: String, sparkContext: SparkContext) = { this( sparkContext, CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull, null) } require(_sc != null || _cp != null, \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\") private[streaming] val isCheckpointPresent: Boolean = _cp != null private[streaming] val sc: SparkContext = { if (_sc != null) { _sc } else if (isCheckpointPresent) { SparkContext.getOrCreate(_cp.createSparkConf()) } else { throw new SparkException(\"Cannot create StreamingContext without a SparkContext\") } } if (sc.conf.get(\"spark.master\") == \"local\" || sc.conf.get(\"spark.master\") == \"local[1]\") { logWarning(\"spark.master should be set as local[n], n > 1 in local mode if you have receivers\" + \" to get data, otherwise Spark jobs will not get resources to process the received data.\") } private[streaming] val conf = sc.conf private[streaming] val env = sc.env private[streaming] val graph: DStreamGraph = { if (isCheckpointPresent) { _cp.graph.setContext(this) _cp.graph.restoreCheckpointData() _cp.graph } else { require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\") val newGraph = new DStreamGraph() newGraph.setBatchDuration(_batchDur) newGraph } } private val nextInputStreamId = new AtomicInteger(0) private[streaming] var checkpointDir: String = { if (isCheckpointPresent) { sc.setCheckpointDir(_cp.checkpointDir) _cp.checkpointDir } else { null } } private[streaming] val checkpointDuration: Duration = { if (isCheckpointPresent) _cp.checkpointDuration else graph.batchDuration } private[streaming] val scheduler = new JobScheduler(this) private[streaming] val waiter = new ContextWaiter private[streaming] val progressListener = new StreamingJobProgressListener(this) private[streaming] val uiTab: Option[StreamingTab] = sparkContext.ui match { case Some(ui) => Some(new StreamingTab(this, ui)) case None => None } /* Initializing a streamingSource to register metrics  private val streamingSource = new StreamingSource(this) private var state: StreamingContextState = INITIALIZED private val startSite = new AtomicReference[CallSite](null) // Copy of thread-local properties from SparkContext. These properties will be set in all tasks // submitted by this StreamingContext after start. private[streaming] val savedProperties = new AtomicReference[Properties](new Properties) private[streaming] def getStartSite(): CallSite = startSite.get() private var shutdownHookRef: AnyRef = _ conf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint) /** * Return the associated Spark context  def sparkContext: SparkContext = sc /** * Set each DStream in this context to remember RDDs it generated in the last given duration. * DStreams remember RDDs only for a limited duration of time and release them for garbage * collection. This method allows the developer to specify how long to remember the RDDs ( * if the developer wishes to query old data outside the DStream computation). * @param duration Minimum duration that each DStream should remember its RDDs  def remember(duration: Duration): Unit = { graph.remember(duration) } /** * Set the context to periodically checkpoint the DStream operations for driver * fault-tolerance. * @param directory HDFS-compatible directory where the checkpoint data will be reliably stored. * Note that this must be a fault-tolerant file system like HDFS.  def checkpoint(directory: String): Unit = { if (directory != null) { val path = new Path(directory) val fs = path.getFileSystem(sparkContext.hadoopConfiguration) fs.mkdirs(path) val fullPath = fs.getFileStatus(path).getPath().toString sc.setCheckpointDir(fullPath) checkpointDir = fullPath } else { checkpointDir = null } } private[streaming] def isCheckpointingEnabled: Boolean = { checkpointDir != null } private[streaming] def initialCheckpoint: Checkpoint = { if (isCheckpointPresent) _cp else null } private[streaming] def getNewInputStreamId() = nextInputStreamId.getAndIncrement() /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withScope[U](body: => U): U = sparkContext.withScope(body) /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withNamedScope[U](name: String)(body: => U): U = { RDDOperationScope.withScope(sc, name, allowNesting = false, ignoreParent = false)(body) } /** * Create an input stream with any arbitrary user implemented receiver. * Find more details at https://spark.apache.org/docs/latest/streaming-custom-receivers.html * @param receiver Custom implementation of Receiver  def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T] = { withNamedScope(\"receiver stream\") { new PluggableInputDStream[T](this, receiver) } } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\\n` delimited * lines. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @see [[socketStream]]  def socketTextStream( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[String] = withNamedScope(\"socket text stream\") { socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel) } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes it interpreted as object using the given * converter. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param converter Function to convert the byte stream to objects * @param storageLevel Storage level to use for storing the received objects * @tparam T Type of the objects received (after converting bytes to objects)  def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel ): ReceiverInputDStream[T] = { new SocketInputDStream[T](this, hostname, port, converter, storageLevel) } /** * Create an input stream from network source hostname:port, where data is received * as serialized blocks (serialized using the Spark's serializer) that can be directly * pushed into the block manager without deserializing them. This is the most efficient * way to receive data. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @tparam T Type of the objects in the received blocks  def rawSocketStream[T: ClassTag]( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[T] = withNamedScope(\"raw socket stream\") { new RawInputDStream[T](this, hostname, port, storageLevel) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @param conf Hadoop configuration * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean, conf: Configuration): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly, Option(conf)) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as text files (using key as LongWritable, value * as Text and input format as TextInputFormat). Files must be written to the * monitored directory by \"moving\" them from another location within the same * file system. File names starting with . are ignored. * The text files must be encoded as UTF-8. * * @param directory HDFS directory to monitor for new file  def textFileStream(directory: String): DStream[String] = withNamedScope(\"text file stream\") { fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as flat binary files, assuming a fixed length per record, * generating one byte array per record. Files must be written to the monitored directory * by \"moving\" them from another location within the same file system. File names * starting with . are ignored. * * @param directory HDFS directory to monitor for new file * @param recordLength length of each record in bytes * * @note We ensure that the byte array for each record in the * resulting RDDs of the DStream has the provided record length.  def binaryRecordsStream( directory: String, recordLength: Int): DStream[Array[Byte]] = withNamedScope(\"binary records stream\") { val conf = _sc.hadoopConfiguration conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = fileStream[LongWritable, BytesWritable, FixedLengthBinaryInputFormat]( directory, FileInputDStream.defaultFilter: Path => Boolean, newFilesOnly = true, conf) br.map { case (k, v) => val bytes = v.copyBytes() require(bytes.length == recordLength, \"Byte array does not have correct length. \" + s\"${bytes.length} did not equal recordLength: $recordLength\") bytes } } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean = true ): InputDStream[T] = { queueStream(queue, oneAtATime, sc.makeRDD(Seq.empty[T], 1)) } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @param defaultRDD Default RDD is returned by the DStream when the queue is empty. * Set as null if no RDD should be returned when empty * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T] ): InputDStream[T] = { new QueueInputDStream(this, queue, oneAtATime, defaultRDD) } /** * Create a unified DStream from multiple DStreams of the same type and same slide duration.  def union[T: ClassTag](streams: Seq[DStream[T]]): DStream[T] = withScope { new UnionDStream[T](streams.toArray) } /** * Create a new DStream in which each RDD is generated by applying a function on RDDs of * the DStreams.  def transform[T: ClassTag]( dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T] ): DStream[T] = withScope { new TransformedDStream[T](dstreams, sparkContext.clean(transformFunc)) } /** * Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for * receiving system events related to streaming.  def addStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.addListener(streamingListener) } def removeStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.removeListener(streamingListener) } private def validate(): Unit = { assert(graph != null, \"Graph is null\") graph.validate() require( !isCheckpointingEnabled || checkpointDuration != null, \"Checkpoint directory has been set, but the graph checkpointing interval has \" + \"not been set. Please use StreamingContext.checkpoint() to set the interval.\" ) // Verify whether the DStream checkpoint is serializable if (isCheckpointingEnabled) { val checkpoint = new Checkpoint(this, Time(0)) try { Checkpoint.serialize(checkpoint, conf) } catch { case e: NotSerializableException => throw new NotSerializableException( \"DStream checkpointing has been enabled but the DStreams with their functions \" + \"are not serializable\\n\" + SerializationDebugger.improveException(checkpoint, e).getMessage() ) } } if (Utils.isDynamicAllocationEnabled(sc.conf) || ExecutorAllocationManager.isDynamicAllocationEnabled(conf)) { logWarning(\"Dynamic Allocation is enabled for this application. \" + \"Enabling Dynamic allocation for Spark Streaming applications can cause data loss if \" + \"Write Ahead Log is not enabled for non-replayable sources. \" + \"See the programming guide for details on how to enable the Write Ahead Log.\") } } /** * :: DeveloperApi :: * * Return the current state of the context. The context can be in three possible states - * * - StreamingContextState.INITIALIZED - The context has been created, but not started yet. * Input DStreams, transformations and output operations can be created on the context. * - StreamingContextState.ACTIVE - The context has been started, and not stopped. * Input DStreams, transformations and output operations cannot be created on the context. * - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more.  @DeveloperApi def getState(): StreamingContextState = synchronized { state } /** * Start the execution of the streams. * * @throws IllegalStateException if the StreamingContext is already stopped.  def start(): Unit = synchronized { state match { case INITIALIZED => startSite.set(DStream.getCreationSite()) StreamingContext.ACTIVATION_LOCK.synchronized { StreamingContext.assertNoOtherContextIsActive() try { validate() registerProgressListener() // Start the streaming scheduler in a new thread, so that thread local properties // like call sites and job groups can be reset without affecting those of the // current thread. ThreadUtils.runInNewThread(\"streaming-start\") { sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") savedProperties.set(Utils.cloneProperties(sparkContext.localProperties.get())) scheduler.start() } state = StreamingContextState.ACTIVE scheduler.listenerBus.post( StreamingListenerStreamingStarted(System.currentTimeMillis())) } catch { case NonFatal(e) => logError(\"Error starting the context, marking it as stopped\", e) scheduler.stop(false) state = StreamingContextState.STOPPED throw e } StreamingContext.setActiveContext(this) } logDebug(\"Adding shutdown hook\") // force eager creation of logger shutdownHookRef = ShutdownHookManager.addShutdownHook( StreamingContext.SHUTDOWN_HOOK_PRIORITY)(() => stopOnShutdown()) // Registering Streaming Metrics at the start of the StreamingContext assert(env.metricsSystem != null) env.metricsSystem.registerSource(streamingSource) uiTab.foreach(_.attach()) logInfo(\"StreamingContext started\") case ACTIVE => logWarning(\"StreamingContext has already been started\") case STOPPED => throw new IllegalStateException(\"StreamingContext has already been stopped\") } } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread.  def awaitTermination(): Unit = { waiter.waitForStopOrError() } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. * * @param timeout time to wait in milliseconds * @return `true` if it's stopped; or throw the reported error during the execution; or `false` * if the waiting time elapsed before returning from the method.  def awaitTerminationOrTimeout(timeout: Long): Boolean = { waiter.waitForStopOrError(timeout) } /** * Stop the execution of the streams immediately (does not wait for all received data * to be processed). By default, if `stopSparkContext` is not specified, the underlying * SparkContext will also be stopped. This implicit behavior can be configured using the * SparkConf configuration spark.streaming.stopSparkContextByDefault. * * @param stopSparkContext If true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started.  def stop( stopSparkContext: Boolean = conf.getBoolean(\"spark.streaming.stopSparkContextByDefault\", true) ): Unit = synchronized { stop(stopSparkContext, false) } /** * Stop the execution of the streams, with option of ensuring all received data * has been processed. * * @param stopSparkContext if true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. * @param stopGracefully if true, stops gracefully by waiting for the processing of all * received data to be completed  def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit = { var shutdownHookRefToRemove: AnyRef = null if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop StreamingContext within listener bus thread.\") } synchronized { // The state should always be Stopped after calling `stop()`, even if we haven't started yet state match { case INITIALIZED => logWarning(\"StreamingContext has not been started yet\") state = STOPPED case STOPPED => logWarning(\"StreamingContext has already been stopped\") state = STOPPED case ACTIVE => // It's important that we don't set state = STOPPED until the very end of this case, // since we need to ensure that we're still able to call `stop()` to recover from // a partially-stopped StreamingContext which resulted from this `stop()` call being // interrupted. See SPARK-12001 for more details. Because the body of this case can be // executed twice in the case of a partial stop, all methods called here need to be // idempotent. Utils.tryLogNonFatalError { scheduler.stop(stopGracefully) } // Removing the streamingSource to de-register the metrics on stop() Utils.tryLogNonFatalError { env.metricsSystem.removeSource(streamingSource) } Utils.tryLogNonFatalError { uiTab.foreach(_.detach()) } Utils.tryLogNonFatalError { unregisterProgressListener() } StreamingContext.setActiveContext(null) Utils.tryLogNonFatalError { waiter.notifyStop() } if (shutdownHookRef != null) { shutdownHookRefToRemove = shutdownHookRef shutdownHookRef = null } logInfo(\"StreamingContext stopped successfully\") state = STOPPED } } if (shutdownHookRefToRemove != null) { ShutdownHookManager.removeShutdownHook(shutdownHookRefToRemove) } // Even if we have already stopped, we still need to attempt to stop the SparkContext because // a user might stop(stopSparkContext = false) and then call stop(stopSparkContext = true). if (stopSparkContext) sc.stop() } private def stopOnShutdown(): Unit = { val stopGracefully = conf.get(STOP_GRACEFULLY_ON_SHUTDOWN) logInfo(s\"Invoking stop(stopGracefully=$stopGracefully) from shutdown hook\") // Do not stop SparkContext, let its own shutdown hook stop it stop(stopSparkContext = false, stopGracefully = stopGracefully) } private def registerProgressListener(): Unit = { addStreamingListener(progressListener) sc.addSparkListener(progressListener) sc.ui.foreach(_.setStreamingJobProgressListener(progressListener)) } private def unregisterProgressListener(): Unit = { removeStreamingListener(progressListener) sc.removeSparkListener(progressListener) sc.ui.foreach(_.clearStreamingJobProgressListener()) } } /** * StreamingContext object contains a number of utility functions related to the * StreamingContext class.  object StreamingContext extends Logging { /** * Lock that guards activation of a StreamingContext as well as access to the singleton active * StreamingContext in getActiveOrCreate().  private val ACTIVATION_LOCK = new Object() private val SHUTDOWN_HOOK_PRIORITY = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY + 1 private val activeContext = new AtomicReference[StreamingContext](null) private def assertNoOtherContextIsActive(): Unit = { ACTIVATION_LOCK.synchronized { if (activeContext.get() != null) { throw new IllegalStateException( \"Only one StreamingContext may be started in this JVM. \" + \"Currently running StreamingContext was started at\" + activeContext.get.getStartSite().longForm) } } } private def setActiveContext(ssc: StreamingContext): Unit = { ACTIVATION_LOCK.synchronized { activeContext.set(ssc) } } /** * Get the currently active context, if there is one. Active means started but not stopped.  def getActive(): Option[StreamingContext] = { ACTIVATION_LOCK.synchronized { Option(activeContext.get()) } } /** * Either return the \"active\" StreamingContext (that is, started but not stopped), or create a * new StreamingContext that is * @param creatingFunc Function to create a new StreamingContext  def getActiveOrCreate(creatingFunc: () => StreamingContext): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { creatingFunc() } } } /** * Either get the currently active StreamingContext (that is, started but not stopped), * OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data * does not exist in the provided, then create a new StreamingContext by calling the provided * `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getActiveOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { getOrCreate(checkpointPath, creatingFunc, hadoopConf, createOnError) } } } /** * Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. * If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be * recreated from the checkpoint data. If the data does not exist, then the StreamingContext * will be created by called the provided `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), hadoopConf, createOnError) checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc()) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to StreamingContext.  def jarOfClass(cls: Class[_]): Option[String] = SparkContext.jarOfClass(cls) private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = { new SparkContext(conf) } private[streaming] def createNewSparkContext( master: String, appName: String, sparkHome: String, jars: Seq[String], environment: Map[String, String] ): SparkContext = { val conf = SparkContext.updatedConf( new SparkConf(), master, appName, sparkHome, jars, environment) new SparkContext(conf) } private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { var result = time.milliseconds.toString if (prefix != null && prefix.length > 0) { result = s\"$prefix-$result\" } if (suffix != null && suffix.length > 0) { result = s\"$result.$suffix\" } result } } private class StreamingContextPythonHelper { /** * This is a private method only for Python to implement `getOrCreate`.  def tryRecoverFromCheckpoint(checkpointPath: String): Option[StreamingContext] = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), SparkHadoopUtil.get.conf, ignoreReadError = false) checkpointOption.map(new StreamingContext(null, _, null)) } }",
          "## METHOD: org/apache/spark/streaming/ContextWaiter#waitForStopOrError().\ndef waitForStopOrError(timeout: Long;\n def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }\n"
        ]
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "ContextWaiter is a Scala class coordinating wait/notify for streaming context stop or error. It uses a ReentrantLock and Condition to manage state changes and waiting threads. The class provides methods to notify errors, notify stop, and wait for stop or error with optional timeout.",
          "prompt_time": 0.23,
          "answer_time": 59,
          "context_tokens": 7024,
          "used_context": [
            "## CLASS: org/apache/spark/streaming/ContextWaiter#\nprivate[streaming] class ContextWaiter { private val lock = new ReentrantLock() private val condition = lock.newCondition() // Guarded by \"lock\" private var error: Throwable = null // Guarded by \"lock\" private var stopped: Boolean = false def notifyError(e: Throwable): Unit = { lock.lock() try { error = e condition.signalAll() } finally { lock.unlock() } } def notifyStop(): Unit = { lock.lock() try { stopped = true condition.signalAll() } finally { lock.unlock() } } /** * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or * `false` if the waiting time detectably elapsed before return from the method.  def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }",
            "## CLASS: org/apache/spark/streaming/StreamingContext#\n class StreamingContext private[streaming] ( _sc: SparkContext, _cp: Checkpoint, _batchDur: Duration ) extends Logging { /** * Create a StreamingContext using an existing SparkContext. * @param sparkContext existing SparkContext * @param batchDuration the time interval at which streaming data will be divided into batches  def this(sparkContext: SparkContext, batchDuration: Duration) = { this(sparkContext, null, batchDuration) } /** * Create a StreamingContext by providing the configuration necessary for a new SparkContext. * @param conf a org.apache.spark.SparkConf object specifying Spark parameters * @param batchDuration the time interval at which streaming data will be divided into batches  def this(conf: SparkConf, batchDuration: Duration) = { this(StreamingContext.createNewSparkContext(conf), null, batchDuration) } /** * Create a StreamingContext by providing the details necessary for creating a new SparkContext. * @param master cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName a name for your job, to display on the cluster web UI * @param batchDuration the time interval at which streaming data will be divided into batches  def this( master: String, appName: String, batchDuration: Duration, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment), null, batchDuration) } /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory * @param hadoopConf Optional, configuration object if necessary for reading from * HDFS compatible filesystems  def this(path: String, hadoopConf: Configuration) = this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null) /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory  def this(path: String) = this(path, SparkHadoopUtil.get.conf) /** * Recreate a StreamingContext from a checkpoint file using an existing SparkContext. * @param path Path to the directory that was specified as the checkpoint directory * @param sparkContext Existing SparkContext  def this(path: String, sparkContext: SparkContext) = { this( sparkContext, CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull, null) } require(_sc != null || _cp != null, \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\") private[streaming] val isCheckpointPresent: Boolean = _cp != null private[streaming] val sc: SparkContext = { if (_sc != null) { _sc } else if (isCheckpointPresent) { SparkContext.getOrCreate(_cp.createSparkConf()) } else { throw new SparkException(\"Cannot create StreamingContext without a SparkContext\") } } if (sc.conf.get(\"spark.master\") == \"local\" || sc.conf.get(\"spark.master\") == \"local[1]\") { logWarning(\"spark.master should be set as local[n], n > 1 in local mode if you have receivers\" + \" to get data, otherwise Spark jobs will not get resources to process the received data.\") } private[streaming] val conf = sc.conf private[streaming] val env = sc.env private[streaming] val graph: DStreamGraph = { if (isCheckpointPresent) { _cp.graph.setContext(this) _cp.graph.restoreCheckpointData() _cp.graph } else { require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\") val newGraph = new DStreamGraph() newGraph.setBatchDuration(_batchDur) newGraph } } private val nextInputStreamId = new AtomicInteger(0) private[streaming] var checkpointDir: String = { if (isCheckpointPresent) { sc.setCheckpointDir(_cp.checkpointDir) _cp.checkpointDir } else { null } } private[streaming] val checkpointDuration: Duration = { if (isCheckpointPresent) _cp.checkpointDuration else graph.batchDuration } private[streaming] val scheduler = new JobScheduler(this) private[streaming] val waiter = new ContextWaiter private[streaming] val progressListener = new StreamingJobProgressListener(this) private[streaming] val uiTab: Option[StreamingTab] = sparkContext.ui match { case Some(ui) => Some(new StreamingTab(this, ui)) case None => None } /* Initializing a streamingSource to register metrics  private val streamingSource = new StreamingSource(this) private var state: StreamingContextState = INITIALIZED private val startSite = new AtomicReference[CallSite](null) // Copy of thread-local properties from SparkContext. These properties will be set in all tasks // submitted by this StreamingContext after start. private[streaming] val savedProperties = new AtomicReference[Properties](new Properties) private[streaming] def getStartSite(): CallSite = startSite.get() private var shutdownHookRef: AnyRef = _ conf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint) /** * Return the associated Spark context  def sparkContext: SparkContext = sc /** * Set each DStream in this context to remember RDDs it generated in the last given duration. * DStreams remember RDDs only for a limited duration of time and release them for garbage * collection. This method allows the developer to specify how long to remember the RDDs ( * if the developer wishes to query old data outside the DStream computation). * @param duration Minimum duration that each DStream should remember its RDDs  def remember(duration: Duration): Unit = { graph.remember(duration) } /** * Set the context to periodically checkpoint the DStream operations for driver * fault-tolerance. * @param directory HDFS-compatible directory where the checkpoint data will be reliably stored. * Note that this must be a fault-tolerant file system like HDFS.  def checkpoint(directory: String): Unit = { if (directory != null) { val path = new Path(directory) val fs = path.getFileSystem(sparkContext.hadoopConfiguration) fs.mkdirs(path) val fullPath = fs.getFileStatus(path).getPath().toString sc.setCheckpointDir(fullPath) checkpointDir = fullPath } else { checkpointDir = null } } private[streaming] def isCheckpointingEnabled: Boolean = { checkpointDir != null } private[streaming] def initialCheckpoint: Checkpoint = { if (isCheckpointPresent) _cp else null } private[streaming] def getNewInputStreamId() = nextInputStreamId.getAndIncrement() /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withScope[U](body: => U): U = sparkContext.withScope(body) /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withNamedScope[U](name: String)(body: => U): U = { RDDOperationScope.withScope(sc, name, allowNesting = false, ignoreParent = false)(body) } /** * Create an input stream with any arbitrary user implemented receiver. * Find more details at https://spark.apache.org/docs/latest/streaming-custom-receivers.html * @param receiver Custom implementation of Receiver  def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T] = { withNamedScope(\"receiver stream\") { new PluggableInputDStream[T](this, receiver) } } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\\n` delimited * lines. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @see [[socketStream]]  def socketTextStream( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[String] = withNamedScope(\"socket text stream\") { socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel) } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes it interpreted as object using the given * converter. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param converter Function to convert the byte stream to objects * @param storageLevel Storage level to use for storing the received objects * @tparam T Type of the objects received (after converting bytes to objects)  def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel ): ReceiverInputDStream[T] = { new SocketInputDStream[T](this, hostname, port, converter, storageLevel) } /** * Create an input stream from network source hostname:port, where data is received * as serialized blocks (serialized using the Spark's serializer) that can be directly * pushed into the block manager without deserializing them. This is the most efficient * way to receive data. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @tparam T Type of the objects in the received blocks  def rawSocketStream[T: ClassTag]( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[T] = withNamedScope(\"raw socket stream\") { new RawInputDStream[T](this, hostname, port, storageLevel) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @param conf Hadoop configuration * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean, conf: Configuration): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly, Option(conf)) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as text files (using key as LongWritable, value * as Text and input format as TextInputFormat). Files must be written to the * monitored directory by \"moving\" them from another location within the same * file system. File names starting with . are ignored. * The text files must be encoded as UTF-8. * * @param directory HDFS directory to monitor for new file  def textFileStream(directory: String): DStream[String] = withNamedScope(\"text file stream\") { fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as flat binary files, assuming a fixed length per record, * generating one byte array per record. Files must be written to the monitored directory * by \"moving\" them from another location within the same file system. File names * starting with . are ignored. * * @param directory HDFS directory to monitor for new file * @param recordLength length of each record in bytes * * @note We ensure that the byte array for each record in the * resulting RDDs of the DStream has the provided record length.  def binaryRecordsStream( directory: String, recordLength: Int): DStream[Array[Byte]] = withNamedScope(\"binary records stream\") { val conf = _sc.hadoopConfiguration conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = fileStream[LongWritable, BytesWritable, FixedLengthBinaryInputFormat]( directory, FileInputDStream.defaultFilter: Path => Boolean, newFilesOnly = true, conf) br.map { case (k, v) => val bytes = v.copyBytes() require(bytes.length == recordLength, \"Byte array does not have correct length. \" + s\"${bytes.length} did not equal recordLength: $recordLength\") bytes } } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean = true ): InputDStream[T] = { queueStream(queue, oneAtATime, sc.makeRDD(Seq.empty[T], 1)) } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @param defaultRDD Default RDD is returned by the DStream when the queue is empty. * Set as null if no RDD should be returned when empty * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T] ): InputDStream[T] = { new QueueInputDStream(this, queue, oneAtATime, defaultRDD) } /** * Create a unified DStream from multiple DStreams of the same type and same slide duration.  def union[T: ClassTag](streams: Seq[DStream[T]]): DStream[T] = withScope { new UnionDStream[T](streams.toArray) } /** * Create a new DStream in which each RDD is generated by applying a function on RDDs of * the DStreams.  def transform[T: ClassTag]( dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T] ): DStream[T] = withScope { new TransformedDStream[T](dstreams, sparkContext.clean(transformFunc)) } /** * Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for * receiving system events related to streaming.  def addStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.addListener(streamingListener) } def removeStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.removeListener(streamingListener) } private def validate(): Unit = { assert(graph != null, \"Graph is null\") graph.validate() require( !isCheckpointingEnabled || checkpointDuration != null, \"Checkpoint directory has been set, but the graph checkpointing interval has \" + \"not been set. Please use StreamingContext.checkpoint() to set the interval.\" ) // Verify whether the DStream checkpoint is serializable if (isCheckpointingEnabled) { val checkpoint = new Checkpoint(this, Time(0)) try { Checkpoint.serialize(checkpoint, conf) } catch { case e: NotSerializableException => throw new NotSerializableException( \"DStream checkpointing has been enabled but the DStreams with their functions \" + \"are not serializable\\n\" + SerializationDebugger.improveException(checkpoint, e).getMessage() ) } } if (Utils.isDynamicAllocationEnabled(sc.conf) || ExecutorAllocationManager.isDynamicAllocationEnabled(conf)) { logWarning(\"Dynamic Allocation is enabled for this application. \" + \"Enabling Dynamic allocation for Spark Streaming applications can cause data loss if \" + \"Write Ahead Log is not enabled for non-replayable sources. \" + \"See the programming guide for details on how to enable the Write Ahead Log.\") } } /** * :: DeveloperApi :: * * Return the current state of the context. The context can be in three possible states - * * - StreamingContextState.INITIALIZED - The context has been created, but not started yet. * Input DStreams, transformations and output operations can be created on the context. * - StreamingContextState.ACTIVE - The context has been started, and not stopped. * Input DStreams, transformations and output operations cannot be created on the context. * - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more.  @DeveloperApi def getState(): StreamingContextState = synchronized { state } /** * Start the execution of the streams. * * @throws IllegalStateException if the StreamingContext is already stopped.  def start(): Unit = synchronized { state match { case INITIALIZED => startSite.set(DStream.getCreationSite()) StreamingContext.ACTIVATION_LOCK.synchronized { StreamingContext.assertNoOtherContextIsActive() try { validate() registerProgressListener() // Start the streaming scheduler in a new thread, so that thread local properties // like call sites and job groups can be reset without affecting those of the // current thread. ThreadUtils.runInNewThread(\"streaming-start\") { sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") savedProperties.set(Utils.cloneProperties(sparkContext.localProperties.get())) scheduler.start() } state = StreamingContextState.ACTIVE scheduler.listenerBus.post( StreamingListenerStreamingStarted(System.currentTimeMillis())) } catch { case NonFatal(e) => logError(\"Error starting the context, marking it as stopped\", e) scheduler.stop(false) state = StreamingContextState.STOPPED throw e } StreamingContext.setActiveContext(this) } logDebug(\"Adding shutdown hook\") // force eager creation of logger shutdownHookRef = ShutdownHookManager.addShutdownHook( StreamingContext.SHUTDOWN_HOOK_PRIORITY)(() => stopOnShutdown()) // Registering Streaming Metrics at the start of the StreamingContext assert(env.metricsSystem != null) env.metricsSystem.registerSource(streamingSource) uiTab.foreach(_.attach()) logInfo(\"StreamingContext started\") case ACTIVE => logWarning(\"StreamingContext has already been started\") case STOPPED => throw new IllegalStateException(\"StreamingContext has already been stopped\") } } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread.  def awaitTermination(): Unit = { waiter.waitForStopOrError() } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. * * @param timeout time to wait in milliseconds * @return `true` if it's stopped; or throw the reported error during the execution; or `false` * if the waiting time elapsed before returning from the method.  def awaitTerminationOrTimeout(timeout: Long): Boolean = { waiter.waitForStopOrError(timeout) } /** * Stop the execution of the streams immediately (does not wait for all received data * to be processed). By default, if `stopSparkContext` is not specified, the underlying * SparkContext will also be stopped. This implicit behavior can be configured using the * SparkConf configuration spark.streaming.stopSparkContextByDefault. * * @param stopSparkContext If true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started.  def stop( stopSparkContext: Boolean = conf.getBoolean(\"spark.streaming.stopSparkContextByDefault\", true) ): Unit = synchronized { stop(stopSparkContext, false) } /** * Stop the execution of the streams, with option of ensuring all received data * has been processed. * * @param stopSparkContext if true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. * @param stopGracefully if true, stops gracefully by waiting for the processing of all * received data to be completed  def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit = { var shutdownHookRefToRemove: AnyRef = null if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop StreamingContext within listener bus thread.\") } synchronized { // The state should always be Stopped after calling `stop()`, even if we haven't started yet state match { case INITIALIZED => logWarning(\"StreamingContext has not been started yet\") state = STOPPED case STOPPED => logWarning(\"StreamingContext has already been stopped\") state = STOPPED case ACTIVE => // It's important that we don't set state = STOPPED until the very end of this case, // since we need to ensure that we're still able to call `stop()` to recover from // a partially-stopped StreamingContext which resulted from this `stop()` call being // interrupted. See SPARK-12001 for more details. Because the body of this case can be // executed twice in the case of a partial stop, all methods called here need to be // idempotent. Utils.tryLogNonFatalError { scheduler.stop(stopGracefully) } // Removing the streamingSource to de-register the metrics on stop() Utils.tryLogNonFatalError { env.metricsSystem.removeSource(streamingSource) } Utils.tryLogNonFatalError { uiTab.foreach(_.detach()) } Utils.tryLogNonFatalError { unregisterProgressListener() } StreamingContext.setActiveContext(null) Utils.tryLogNonFatalError { waiter.notifyStop() } if (shutdownHookRef != null) { shutdownHookRefToRemove = shutdownHookRef shutdownHookRef = null } logInfo(\"StreamingContext stopped successfully\") state = STOPPED } } if (shutdownHookRefToRemove != null) { ShutdownHookManager.removeShutdownHook(shutdownHookRefToRemove) } // Even if we have already stopped, we still need to attempt to stop the SparkContext because // a user might stop(stopSparkContext = false) and then call stop(stopSparkContext = true). if (stopSparkContext) sc.stop() } private def stopOnShutdown(): Unit = { val stopGracefully = conf.get(STOP_GRACEFULLY_ON_SHUTDOWN) logInfo(s\"Invoking stop(stopGracefully=$stopGracefully) from shutdown hook\") // Do not stop SparkContext, let its own shutdown hook stop it stop(stopSparkContext = false, stopGracefully = stopGracefully) } private def registerProgressListener(): Unit = { addStreamingListener(progressListener) sc.addSparkListener(progressListener) sc.ui.foreach(_.setStreamingJobProgressListener(progressListener)) } private def unregisterProgressListener(): Unit = { removeStreamingListener(progressListener) sc.removeSparkListener(progressListener) sc.ui.foreach(_.clearStreamingJobProgressListener()) } } /** * StreamingContext object contains a number of utility functions related to the * StreamingContext class.  object StreamingContext extends Logging { /** * Lock that guards activation of a StreamingContext as well as access to the singleton active * StreamingContext in getActiveOrCreate().  private val ACTIVATION_LOCK = new Object() private val SHUTDOWN_HOOK_PRIORITY = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY + 1 private val activeContext = new AtomicReference[StreamingContext](null) private def assertNoOtherContextIsActive(): Unit = { ACTIVATION_LOCK.synchronized { if (activeContext.get() != null) { throw new IllegalStateException( \"Only one StreamingContext may be started in this JVM. \" + \"Currently running StreamingContext was started at\" + activeContext.get.getStartSite().longForm) } } } private def setActiveContext(ssc: StreamingContext): Unit = { ACTIVATION_LOCK.synchronized { activeContext.set(ssc) } } /** * Get the currently active context, if there is one. Active means started but not stopped.  def getActive(): Option[StreamingContext] = { ACTIVATION_LOCK.synchronized { Option(activeContext.get()) } } /** * Either return the \"active\" StreamingContext (that is, started but not stopped), or create a * new StreamingContext that is * @param creatingFunc Function to create a new StreamingContext  def getActiveOrCreate(creatingFunc: () => StreamingContext): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { creatingFunc() } } } /** * Either get the currently active StreamingContext (that is, started but not stopped), * OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data * does not exist in the provided, then create a new StreamingContext by calling the provided * `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getActiveOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { getOrCreate(checkpointPath, creatingFunc, hadoopConf, createOnError) } } } /** * Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. * If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be * recreated from the checkpoint data. If the data does not exist, then the StreamingContext * will be created by called the provided `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), hadoopConf, createOnError) checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc()) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to StreamingContext.  def jarOfClass(cls: Class[_]): Option[String] = SparkContext.jarOfClass(cls) private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = { new SparkContext(conf) } private[streaming] def createNewSparkContext( master: String, appName: String, sparkHome: String, jars: Seq[String], environment: Map[String, String] ): SparkContext = { val conf = SparkContext.updatedConf( new SparkConf(), master, appName, sparkHome, jars, environment) new SparkContext(conf) } private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { var result = time.milliseconds.toString if (prefix != null && prefix.length > 0) { result = s\"$prefix-$result\" } if (suffix != null && suffix.length > 0) { result = s\"$result.$suffix\" } result } } private class StreamingContextPythonHelper { /** * This is a private method only for Python to implement `getOrCreate`.  def tryRecoverFromCheckpoint(checkpointPath: String): Option[StreamingContext] = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), SparkHadoopUtil.get.conf, ignoreReadError = false) checkpointOption.map(new StreamingContext(null, _, null)) } }",
            "## METHOD: org/apache/spark/streaming/ContextWaiter#waitForStopOrError().\ndef waitForStopOrError(timeout: Long;\n def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }\n"
          ],
          "tokens": 55,
          "more_info": "Used same query 2 times, I didn't ask for full solution in .md file so answer is very basic"
        },
        "without_mcp": {
          "answer": "# ContextWaiter (Spark Streaming)\n\nThis document describes the internal class `org.apache.spark.streaming.ContextWaiter` found in the Spark Streaming module.\n\nAlthough `ContextWaiter` is marked as `private[streaming]` (internal API), understanding it can help contributors and advanced users reason about how `StreamingContext` coordinates shutdown and failure signaling across threads.\n\n\n## Purpose\n\n`ContextWaiter` is a small synchronization utility used by `StreamingContext` to:\n\n- Block a thread until the streaming context either stops successfully, or an error is reported from another thread.\n- Optionally bound the waiting time with a timeout.\n- Propagate the first terminal error that occurs in the streaming context to waiting threads by throwing it.\n\nIn other words, it centralizes the wait/notify logic for the StreamingContext lifecycle, so that threads can efficiently and safely wait for either a normal stop or a failure.\n\n\n## Location\n\nFile: `streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala`\n\nPackage: `org.apache.spark.streaming`\n\n\n## Design and Threading Model\n\n`ContextWaiter` uses:\n\n- `java.util.concurrent.locks.ReentrantLock` to guard its internal state.\n- A `Condition` (created from the same lock) to coordinate wait/notify among threads.\n\nIt maintains two guarded fields:\n\n- `error: Throwable`  the terminal error reported via `notifyError`. Null when no error has been reported.\n- `stopped: Boolean`  true after `notifyStop` is called; false initially.\n\nAll reads/writes to these fields happen under the same lock, ensuring visibility and safety across threads.\n\n\n## API Overview\n\n- `def notifyError(e: Throwable): Unit`\n  - Records a terminal error and signals all waiters.\n\n- `def notifyStop(): Unit`\n  - Marks the context as stopped and signals all waiters.\n\n- `def waitForStopOrError(timeout: Long = -1): Boolean`\n  - Waits until one of the following happens:\n    1) `notifyStop()` is called (normal termination), or\n    2) `notifyError(e)` is called (failure), or\n    3) The specified timeout elapses (when `timeout >= 0`).\n  - Behavior:\n    - If an error has been reported, the method throws that `Throwable`.\n    - If the context has stopped, returns `true`.\n    - If the timeout elapses without stop or error, returns `false`.\n  - Timeout units: milliseconds. A negative timeout (default `-1`) means wait indefinitely.\n\n\n## Detailed Behavior\n\n- Waiting indefinitely\n  - When `timeout < 0`, the method repeatedly calls `condition.await()` while `!stopped && error == null`.\n\n- Waiting with timeout\n  - Converts the provided millisecond `timeout` to nanoseconds and loops using `condition.awaitNanos(nanos)` while `!stopped && error == null && nanos > 0`.\n  - If the loop exits due to `nanos <= 0`, the method considers this a timeout and returns `stopped` (which will be `false` if neither stop nor error occurred).\n\n- Error precedence\n  - If `error` is non-null after waiting, the method throws the stored `Throwable` instead of returning a boolean. This ensures failures are not silently ignored by waiters.\n\n- Stop vs. timeout return value\n  - The final return value is `stopped`. After a timeout without stop or error, `stopped` remains `false` and the method returns `false`.\n\n- Signaling\n  - Both `notifyError` and `notifyStop` signal all waiting threads (`condition.signalAll()`), ensuring no waiter remains blocked once a terminal state is reached or a failure occurs.\n\n\n## Typical Usage in StreamingContext\n\n`StreamingContext` creates and owns a `ContextWaiter` instance:\n\n```scala\nprivate[streaming] val waiter = new ContextWaiter\n```\n\n- When the streaming job network stops cleanly, `StreamingContext` (or one of its coordinator threads) calls `waiter.notifyStop()`.\n- If a background thread detects a fatal error (e.g., receiver failure not recoverable, scheduler crash), it calls `waiter.notifyError(e)`.\n- External callers (e.g., `StreamingContext.awaitTermination()` variants) block by calling `waiter.waitForStopOrError(...)` and respond accordingly:\n  - Return normally when `true` is returned (stopped),\n  - Re-throw the failure if an exception is raised by `waitForStopOrError`.\n\n\n## Edge Cases and Guarantees\n\n- Idempotency\n  - Multiple calls to `notifyStop()` are safe; `stopped` simply remains `true` and waiters continue to be signaled.\n  - Multiple calls to `notifyError()` will overwrite the previous `error` reference with the latest `Throwable`. In practice, the first call typically determines the failure observed by waiters, because they wake and throw immediately.\n\n- Memory visibility\n  - Because all accesses to `stopped` and `error` are guarded by the same `ReentrantLock`, updates are visible to other threads after they acquire the lock.\n\n- Spurious wakeups\n  - The `while`-loop form around `await`/`awaitNanos` safely handles spurious wakeups.\n\n- Timeout precision\n  - Uses `awaitNanos` to reduce drift; however, like all timers, actual wake-up may be later than requested due to scheduling.\n\n\n## Minimal Example\n\nThe following pseudo-usage shows how a component might use `ContextWaiter`:\n\n```scala\nval waiter = new ContextWaiter\n\nval t = new Thread(new Runnable {\n  override def run(): Unit = {\n    try {\n      // do streaming work ...\n      // on success:\n      waiter.notifyStop()\n    } catch {\n      case t: Throwable => waiter.notifyError(t)\n    }\n  }\n})\n\nt.start()\n\ntry {\n  val stopped = waiter.waitForStopOrError(30000) // 30s\n  if (!stopped) {\n    // handle timeout\n  }\n} catch {\n  case e: Throwable =>\n    // handle failure\n}\n```\n\n\n## Source Code (Spark 3.3.0)\n\n```scala\nprivate[streaming] class ContextWaiter {\n  private val lock = new ReentrantLock()\n  private val condition = lock.newCondition()\n\n  // Guarded by \"lock\"\n  private var error: Throwable = null\n\n  // Guarded by \"lock\"\n  private var stopped: Boolean = false\n\n  def notifyError(e: Throwable): Unit = {\n    lock.lock()\n    try {\n      error = e\n      condition.signalAll()\n    } finally {\n      lock.unlock()\n    }\n  }\n\n  def notifyStop(): Unit = {\n    lock.lock()\n    try {\n      stopped = true\n      condition.signalAll()\n    } finally {\n      lock.unlock()\n    }\n  }\n\n  /**\n   * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or\n   * `false` if the waiting time detectably elapsed before return from the method.\n   */\n  def waitForStopOrError(timeout: Long = -1): Boolean = {\n    lock.lock()\n    try {\n      if (timeout < 0) {\n        while (!stopped && error == null) {\n          condition.await()\n        }\n      } else {\n        var nanos = TimeUnit.MILLISECONDS.toNanos(timeout)\n        while (!stopped && error == null && nanos > 0) {\n          nanos = condition.awaitNanos(nanos)\n        }\n      }\n      if (error != null) throw error\n      stopped\n    } finally {\n      lock.unlock()\n    }\n  }\n}\n```\n\n\n## When to Read or Modify\n\n- Read this class when working on:\n  - `StreamingContext.awaitTermination*` methods,\n  - error propagation or shutdown sequences in Spark Streaming,\n  - thread coordination between receivers/schedulers and the driver.\n\n- Modify with care:\n  - This class is intentionally small and battle-tested. Any change should maintain the same semantics (error-first throw, proper signaling, and lock-guarded state).\n",
          "answer_time": 58,
          "tokens": 1693,
          "more_info": "Added `return full answer in .md file` at the end of question"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 3,
          "reasoning": "Answer A provides comprehensive detail with clear structure, explaining each field and method thoroughly with specific implementation details. Answer B is accurate but much more concise, lacking the detailed explanations of method behaviors, field purposes, and context usage that make Answer A significantly more helpful for understanding the class."
        },
        "mcp_vs_no_mcp": {
          "winner": "without_mcp",
          "score_a": 4,
          "score_b": 5,
          "reasoning": "Both answers accurately describe the ContextWaiter class, but Answer B provides superior structure and completeness. Answer A gives a solid technical overview with correct details about the implementation. However, Answer B excels with professional documentation formatting, comprehensive sections covering purpose, location, design rationale, detailed behavioral explanations, and practical usage context within StreamingContext. Answer B's organization with clear headings, code examples, and thorough coverage of edge cases (timeout handling, error precedence) makes it significantly more valuable for understanding both the technical implementation and practical usage of the class."
        }
      }
    },
    {
      "id": "Q002",
      "question": "Describe PythonRunner Object and all methods that use that object",
      "category": "definition",
      "ground_truth_contexts": [
        "} } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark. */ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
        "mem.map(_ / cores) } def compute( inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): Iterator[OUT] = { val startTime = System.currentTimeMillis val env = SparkEnv.get // Get the executor cores and pyspark memory, they are passed via the local properties when // the user specified them in a ResourceProfile. val execCoresProp = Option(context.getLocalProperty(EXECUTOR_CORES_LOCAL_PROPERTY)) val memoryMb = Option(context.getLocalProperty(PYSPARK_MEMORY_LOCAL_PROPERTY)).map(_.toLong) val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\") // if OMP_NUM_THREADS is not explicitly set, override it with the number of cores if (conf.getOption(\"spark.executorEnv.OMP_NUM_THREADS\").isEmpty) { // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool // see https://github.com/numpy/numpy/issues/10455 execCoresProp.foreach(envVars.put(\"OMP_NUM_THREADS\", _)) } envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread if (reuseWorker) { envVars.put(\"SPARK_REUSE_WORKER\", \"1\") } if (simplifiedTraceback) { envVars.put(\"SPARK_SIMPLIFIED_TRACEBACK\", \"1\") } // SPARK-30299 this could be wrong with standalone mode when executor // cores might not be correct because it defaults to all cores on the box. val execCores = execCoresProp.map(_.toInt).getOrElse(conf.get(EXECUTOR_CORES)) val workerMemoryMb = getWorkerMemoryMb(memoryMb, execCores) if (workerMemoryMb.isDefined) { envVars.put(\"PYSPARK_EXECUTOR_MEMORY_MB\", workerMemoryMb.get.toString) } envVars.put(\"SPARK_AUTH_SOCKET_TIMEOUT\", authSocketTimeout.toString) envVars.put(\"SPARK_BUFFER_SIZE\", bufferSize.toString) if (faultHandlerEnabled) { envVars.put(\"PYTHON_FAULTHANDLER_DIR\", BasePythonRunner.faultHandlerLogDir.toString) } val (worker: Socket, pid: Option[Int]) = env.createPythonWorker( pythonExec, envVars.asScala.toMap) // Whether is the worker released into idle pool or closed. When any codes try to release or // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make // sure there is only one winner that is going to release or close the worker. val releasedOrClosed = new AtomicBoolean(false) // Start a thread to feed the process input from our parent's iterator val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context) context.addTaskCompletionListener[Unit] { _ => writerThread.shutdownOnTaskCompletion() if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) { try { worker.close() } catch { case e: Exception => logWarning(\"Failed to close worker socket\", e) } } } writerThread.start() new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start() if (reuseWorker) { val key = (worker, context.taskAttemptId) // SPARK-35009: avoid creating multiple monitor threads for the same python worker // and task context if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(SparkEnv.get, worker, context).start() } } else { new MonitorThread(SparkEnv.get, worker, context).start() } // Return an iterator that read lines from the process's stdout val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize)) val stdoutIterator = newReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) new InterruptibleIterator(context, stdoutIterator) } protected def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): WriterThread protected def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[OUT] /** * The thread responsible for writing the data from the PythonRDD's parent iterator to the * Python process. */ abstract class WriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext) extends Thread(s\"stdout writer for $pythonExec\") { @volatile private var _exception: Throwable = null private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala)) setDaemon(true) /** Contains the throwable thrown while writing the parent iterator to the Python process. */ def exception: Option[Throwable] = Option(_exception) /** * Terminates the writer thread and waits for it to exit, ignoring any exceptions that may occur * due to cleanup. */ def shutdownOnTaskCompletion(): Unit = { assert(context.isCompleted) this.interrupt() // Task completion listeners that run after this method returns may invalidate // `inputIterator`. For example, when `inputIterator` was generated by the off-heap vectorized // reader, a task completion listener will free the underlying off-heap buffers. If the writer // thread is still running when `inputIterator` is invalidated, it can cause a use-after-free // bug that crashes the executor (SPARK-33277). Therefore this method must wait for the writer // thread to exit before returning. this.join() } /** * Writes a command section to the stream connected to the Python worker. */ protected def writeCommand(dataOut: DataOutputStream): Unit /** * Writes input data to the stream connected to the Python worker. */ protected def writeIteratorToStream(dataOut: DataOutputStream): Unit override def run(): Unit = Utils.logUncaughtExceptions { try { TaskContext.setTaskContext(context) val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize) val dataOut = new DataOutputStream(stream) // Partition index dataOut.writeInt(partitionIndex) // Python version of driver PythonRDD.writeUTF(pythonVer, dataOut) // Init a ServerSocket to accept method calls from Python side. val isBarrier = context.isInstanceOf[BarrierTaskContext] if (isBarrier) { serverSocket = Some(new ServerSocket(/* port */ 0, /* backlog */ 1, InetAddress.getByName(\"localhost\"))) // A call to accept() for ServerSocket shall block infinitely. serverSocket.foreach(_.setSoTimeout(0)) new Thread(\"accept-connections\") { setDaemon(true) override def run(): Unit = { while (!serverSocket.get.isClosed()) { var sock: Socket = null try { sock = serverSocket.get.accept() // Wait for function call from python side. sock.setSoTimeout(10000) authHelper.authClient(sock) val input = new DataInputStream(sock.getInputStream()) val requestMethod = input.readInt() // The BarrierTaskContext function may wait infinitely, socket shall not timeout // before the function finishes. sock.setSoTimeout(0) requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => barrierAndServe(requestMethod, sock) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => val length = input.readInt() val message = new Array[Byte](length) input.readFully(message) barrierAndServe(requestMethod, sock, new String(message, UTF_8)) case _ => val out = new DataOutputStream(new BufferedOutputStream( sock.getOutputStream)) writeUTF(BarrierTaskContextMessageProtocol.ERROR_UNRECOGNIZED_FUNCTION, out) } } catch { case e: SocketException if e.getMessage.contains(\"Socket closed\") => // It is possible that the ServerSocket is not closed, but the native socket // has already been closed, we shall catch and silently ignore this case. } finally { if (sock != null) { sock.close() } } } } }.start() } val secret = if (isBarrier) { authHelper.secret } else { \"\" } // Close ServerSocket on task completion. serverSocket.foreach { server => context.addTaskCompletionListener[Unit](_ => server.close()) } val boundPort: Int = serverSocket.map(_.getLocalPort).getOrElse(0) if (boundPort == -1) { val message = \"ServerSocket failed to bind to Java side.\" logError(message) throw new SparkException(message) } else if (isBarrier) { logDebug(s\"Started ServerSocket on port $boundPort.\") } // Write out the TaskContextInfo dataOut.writeBoolean(isBarrier) dataOut.writeInt(boundPort) val secretBytes = secret.getBytes(UTF_8) dataOut.writeInt(secretBytes.length) dataOut.write(secretBytes, 0, secretBytes.length) dataOut.writeInt(context.stageId()) dataOut.writeInt(context.partitionId()) dataOut.writeInt(context.attemptNumber()) dataOut.writeLong(context.taskAttemptId()) dataOut.writeInt(context.cpus()) val resources = context.resources() dataOut.writeInt(resources.size) resources.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v.name, dataOut) dataOut.writeInt(v.addresses.size) v.addresses.foreach { case addr => PythonRDD.writeUTF(addr, dataOut) } } val localProps = context.getLocalProperties.asScala dataOut.writeInt(localProps.size) localProps.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v, dataOut) } // sparkFilesDir PythonRDD.writeUTF(SparkFiles.getRootDirectory(), dataOut) // Python includes (*.zip and *.egg files) dataOut.writeInt(pythonIncludes.size) for (include <- pythonIncludes) { PythonRDD.writeUTF(include, dataOut) } // Broadcast variables val oldBids = PythonRDD.getWorkerBroadcasts(worker) val newBids = broadcastVars.map(_.id).toSet // number of different broadcasts val toRemove = oldBids.diff(newBids) val addedBids = newBids.diff(oldBids) val cnt = toRemove.size + addedBids.size val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty dataOut.writeBoolean(needsDecryptionServer) dataOut.writeInt(cnt) def sendBidsToRemove(): Unit = { for (bid <- toRemove) { // remove the broadcast from worker dataOut.writeLong(-bid - 1) // bid >= 0 oldBids.remove(bid) } } if (needsDecryptionServer) { // if there is encryption, we setup a server which reads the encrypted files, and sends // the decrypted data to python val idsAndFiles = broadcastVars.flatMap { broadcast => if (!oldBids.contains(broadcast.id)) { Some((broadcast.id, broadcast.value.path)) } else { None } } val server = new EncryptedPythonBroadcastServer(env, idsAndFiles) dataOut.writeInt(server.port) logTrace(s\"broadcast decryption server setup on ${server.port}\") PythonRDD.writeUTF(server.secret, dataOut) sendBidsToRemove() idsAndFiles.foreach { case (id, _) => // send new broadcast dataOut.writeLong(id) oldBids.add(id) } dataOut.flush() logTrace(\"waiting for python to read decrypted broadcast data from server\") server.waitTillBroadcastDataSent() logTrace(\"done sending decrypted data to python\") } else { sendBidsToRemove() for (broadcast <- broadcastVars) { if (!oldBids.contains(broadcast.id)) { // send new broadcast dataOut.writeLong(broadcast.id) PythonRDD.writeUTF(broadcast.value.path, dataOut) oldBids.add(broadcast.id) } } } dataOut.flush() dataOut.writeInt(evalType) writeCommand(dataOut) writeIteratorToStream(dataOut) dataOut.writeInt(SpecialLengths.END_OF_STREAM) dataOut.flush() } catch { case t: Throwable if (NonFatal(t) || t.isInstanceOf[Exception]) => if (context.isCompleted || context.isInterrupted) { logDebug(\"Exception/NonFatal Error thrown after task completion (likely due to \" + \"cleanup)\", t) if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } else { // We must avoid throwing exceptions/NonFatals here, because the thread uncaught // exception handler will kill the whole executor (see // org.apache.spark.executor.Executor). _exception = t if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } } } /** * Gateway to call BarrierTaskContext methods. */ def barrierAndServe(requestMethod: Int, sock: Socket, message: String = \"\"): Unit = { require( serverSocket.isDefined, \"No available ServerSocket to redirect the BarrierTaskContext method call.\" ) val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream)) try { val messages = requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => context.asInstanceOf[BarrierTaskContext].barrier() Array(BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => context.asInstanceOf[BarrierTaskContext].allGather(message) } out.writeInt(messages.length) messages.foreach(writeUTF(_, out)) } catch { case e: SparkException => writeUTF(e.getMessage, out) } finally { out.close() } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } } abstract class ReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext) extends Iterator[OUT] { private var nextObj: OUT = _ private var eos = false override def hasNext: Boolean = nextObj != null || { if (!eos) { nextObj = read() hasNext } else { false } } override def next(): OUT = { if (hasNext) { val obj = nextObj nextObj = null.asInstanceOf[OUT] obj } else { Iterator.empty.next() } } /** * Reads next object from the stream. * When the stream reaches end of data, needs to process the following sections, * and then returns null. */ protected def read(): OUT protected def handleTimingData(): Unit = { // Timing data from worker val bootTime = stream.readLong() val initTime = stream.readLong() val finishTime = stream.readLong() val boot = bootTime - startTime val init = initTime - bootTime val finish = finishTime - initTime val total = finishTime - startTime logInfo(\"Times: total = %s, boot = %s, init = %s, finish = %s\".format(total, boot, init, finish)) val memoryBytesSpilled = stream.readLong() val diskBytesSpilled = stream.readLong() context.taskMetrics.incMemoryBytesSpilled(memoryBytesSpilled) context.taskMetrics.incDiskBytesSpilled(diskBytesSpilled) } protected def handlePythonException(): PythonException = { // Signals that an exception has been thrown in python val exLength = stream.readInt() val obj = new Array[Byte](exLength) stream.readFully(obj) new PythonException(new String(obj, StandardCharsets.UTF_8), writerThread.exception.getOrElse(null)) } protected def handleEndOfDataSection(): Unit = { // We've finished the data section of the output, but we can still // read some accumulator updates: val numAccumulatorUpdates = stream.readInt() (1 to numAccumulatorUpdates).foreach { _ => val updateLen = stream.readInt() val update = new Array[Byte](updateLen) stream.readFully(update) maybeAccumulator.foreach(_.add(update)) } // Check whether the worker is ready to be re-used. if (stream.readInt() == SpecialLengths.END_OF_STREAM) { if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) { env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker) } } eos = true } protected val handleException: PartialFunction[Throwable, OUT] = { case e: Exception if context.isInterrupted => logDebug(\"Exception thrown after task interruption\", e) throw new TaskKilledException(context.getKillReason().getOrElse(\"unknown reason\")) case e: Exception if writerThread.exception.isDefined => logError(\"Python worker exited unexpectedly (crashed)\", e) logError(\"This may have been caused by a prior exception:\", writerThread.exception.get) throw writerThread.exception.get case eof: EOFException if faultHandlerEnabled && pid.isDefined && JavaFiles.exists(BasePythonRunner.faultHandlerLogPath(pid.get)) => val path = BasePythonRunner.faultHandlerLogPath(pid.get) val error = String.join(\"\\n\", JavaFiles.readAllLines(path)) + \"\\n\" JavaFiles.deleteIfExists(path) throw new SparkException(s\"Python worker exited unexpectedly (crashed): $error\", eof) case eof: EOFException => throw new SparkException(\"Python worker exited unexpectedly (crashed)\", eof) } } /** * It is necessary to have a monitor thread for python workers if the user cancels with * interrupts disabled. In that case we will need to explicitly kill the worker, otherwise the * threads can block indefinitely. */ class MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(s\"Worker Monitor for $pythonExec\") { /** How long to wait before killing the python worker if a task cannot be interrupted. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) private def monitorWorker(): Unit = { // Kill the worker if it is interrupted, checking until task completion. // TODO: This has a race condition if interruption occurs, as completed may still become true. while (!context.isInterrupted && !context.isCompleted) { Thread.sleep(2000) } if (!context.isCompleted) { Thread.sleep(taskKillTimeout) if (!context.isCompleted) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning(s\"Incomplete task $taskName interrupted: Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark. */ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
        "} } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark. */ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
        "override def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = { val runner = PythonRunner(func) runner.compute(firstParent.iterator(split, context), split.index, context) } @transient protected lazy override val isBarrier_ : Boolean = isFromBarrier || dependencies.exists(_.rdd.isBarrier()) } /** * A wrapper for a Python function, contains all necessary context to run the function in Python * runner. */ private[spark] case class PythonFunction( command: Seq[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) { def this( command: Array[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) = { this(command.toSeq, envVars, pythonIncludes, pythonExec, pythonVer, broadcastVars, accumulator) } } /** * A wrapper for chained Python functions (from bottom to top). * @param funcs */ private[spark] case class ChainedPythonFunctions(funcs: Seq[PythonFunction]) /** Thrown for exceptions in user Python code. */ private[spark] class PythonException(msg: String, cause: Throwable) extends RuntimeException(msg, cause) /** * Form an RDD[(Array[Byte], Array[Byte])] from key-value pairs returned from Python. * This is used by PySpark's shuffle operations. */ private class PairwiseRDD(prev: RDD[Array[Byte]]) extends RDD[(Long, Array[Byte])](prev) { override def getPartitions: Array[Partition] = prev.partitions override val partitioner: Option[Partitioner] = prev.partitioner override def compute(split: Partition, context: TaskContext): Iterator[(Long, Array[Byte])] = prev.iterator(split, context).grouped(2).map { case Seq(a, b) => (Utils.deserializeLongValue(a), b) case x => throw new SparkException(\"PairwiseRDD: unexpected value: \" + x) } val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this) } private[spark] object PythonRDD extends Logging { // remember the broadcasts sent to each worker private val workerBroadcasts = new mutable.WeakHashMap[Socket, mutable.Set[Long]]() // Authentication helper used when serving iterator data. private lazy val authHelper = { val conf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf()) new SocketAuthHelper(conf) } def getWorkerBroadcasts(worker: Socket): mutable.Set[Long] = { synchronized { workerBroadcasts.getOrElseUpdate(worker, new mutable.HashSet[Long]()) } } /** * Return an RDD of values from an RDD of (Long, Array[Byte]), with preservePartitions=true * * This is useful for PySpark to have the partitioner after partitionBy() */ def valueOfPair(pair: JavaPairRDD[Long, Array[Byte]]): JavaRDD[Array[Byte]] = { pair.rdd.mapPartitions(it => it.map(_._2), true) } /** * Adapter for calling SparkContext#runJob from Python. * * This method will serve an iterator of an array that contains all elements in the RDD * (effectively a collect()), but allows you to run on a certain subset of partitions, * or to enable local execution. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def runJob( sc: SparkContext, rdd: JavaRDD[Array[Byte]], partitions: JArrayList[Int]): Array[Any] = { type ByteArray = Array[Byte] type UnrolledPartition = Array[ByteArray] val allPartitions: Array[UnrolledPartition] = sc.runJob(rdd, (x: Iterator[ByteArray]) => x.toArray, partitions.asScala.toSeq) val flattenedPartition: UnrolledPartition = Array.concat(allPartitions: _*) serveIterator(flattenedPartition.iterator, s\"serve RDD ${rdd.id} with partitions ${partitions.asScala.mkString(\",\")}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def collectAndServe[T](rdd: RDD[T]): Array[Any] = { serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * This method is similar with `PythonRDD.collectAndServe`, but user can specify job group id, * job description, and interruptOnCancel option. */ def collectAndServeWithJobGroup[T]( rdd: RDD[T], groupId: String, description: String, interruptOnCancel: Boolean): Array[Any] = { val sc = rdd.sparkContext sc.setJobGroup(groupId, description, interruptOnCancel) serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to create a local RDD iterator and serve it via socket. Partitions are * are collected as separate jobs, by order of index. Partition data is first requested by a * non-zero integer to start a collection job. The response is prefaced by an integer with 1 * meaning partition data will be served, 0 meaning the local iterator has been consumed, * and -1 meaning an error occurred during collection. This function is used by * pyspark.rdd._local_iterator_from_socket(). * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def toLocalIteratorAndServe[T](rdd: RDD[T], prefetchPartitions: Boolean = false): Array[Any] = { val handleFunc = (sock: Socket) => { val out = new DataOutputStream(sock.getOutputStream) val in = new DataInputStream(sock.getInputStream) Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Collects a partition on each iteration val collectPartitionIter = rdd.partitions.indices.iterator.map { i => var result: Array[Any] = null rdd.sparkContext.submitJob( rdd, (iter: Iterator[Any]) => iter.toArray, Seq(i), // The partition we are evaluating (_, res: Array[Any]) => result = res, result) } val prefetchIter = collectPartitionIter.buffered // Write data until iteration is complete, client stops iteration, or error occurs var complete = false while (!complete) { // Read request for data, value of zero will stop iteration or non-zero to continue if (in.readInt() == 0) { complete = true } else if (prefetchIter.hasNext) { // Client requested more data, attempt to collect the next partition val partitionFuture = prefetchIter.next() // Cause the next job to be submitted if prefetchPartitions is enabled. if (prefetchPartitions) { prefetchIter.headOption } val partitionArray = ThreadUtils.awaitResult(partitionFuture, Duration.Inf) // Send response there is a partition to read out.writeInt(1) // Write the next object and signal end of data for this iteration writeIteratorToStream(partitionArray.iterator, out) out.writeInt(SpecialLengths.END_OF_DATA_SECTION) out.flush() } else { // Send response there are no more partitions to read and close out.writeInt(0) complete = true } } })(catchBlock = { // Send response that an error occurred, original exception is re-thrown out.writeInt(-1) }, finallyBlock = { out.close() in.close() }) } val server = new SocketFuncServer(authHelper, \"serve toLocalIterator\", handleFunc) Array(server.port, server.secret, server) } def readRDDFromFile( sc: JavaSparkContext, filename: String, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromFile(sc, filename, parallelism) } def readRDDFromInputStream( sc: SparkContext, in: InputStream, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromInputStream(sc, in, parallelism) } def setupBroadcast(path: String): PythonBroadcast = { new PythonBroadcast(path) } def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream): Unit = { def write(obj: Any): Unit = obj match { case null => dataOut.writeInt(SpecialLengths.NULL) case arr: Array[Byte] => dataOut.writeInt(arr.length) dataOut.write(arr) case str: String => writeUTF(str, dataOut) case stream: PortableDataStream => write(stream.toArray()) case (key, value) => write(key) write(value) case other => throw new SparkException(\"Unexpected element type \" + other.getClass) } iter.foreach(write) } /** * Create an RDD from a path using [[org.apache.hadoop.mapred.SequenceFileInputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def sequenceFile[K, V]( sc: JavaSparkContext, path: String, keyClassMaybeNull: String, valueClassMaybeNull: String, keyConverterClass: String, valueConverterClass: String, minSplits: Int, batchSize: Int): JavaRDD[Array[Byte]] = { val keyClass = Option(keyClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val valueClass = Option(valueClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val rdd = sc.sc.sequenceFile[K, V](path, kc, vc, minSplits) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration())) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map that is * passed in from Python, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def newAPIHadoopRDDFromClassNames[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration): RDD[(K, V)] = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.newAPIHadoopFile[K, V, F](path.get, fc, kc, vc, conf) } else { sc.sc.newAPIHadoopRDD[K, V, F](conf, fc, kc, vc) } } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def hadoopFile[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map * that is passed in from Python, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def hadoopRDD[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def hadoopRDDFromClassNames[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration) = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.hadoopFile(path.get, fc, kc, vc) } else { sc.sc.hadoopRDD(new JobConf(conf), fc, kc, vc) } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(StandardCharsets.UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } /** * Create a socket server and a background thread to serve the data in `items`, * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it tries to serialize all the data in `items` * and send them into this connection. * * The thread will terminate after all the data are sent or any exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def serveIterator(items: Iterator[_], threadName: String): Array[Any] = { serveToStream(threadName) { out => writeIteratorToStream(items, new DataOutputStream(out)) } } /** * Create a socket server and background thread to execute the writeFunc * with the given OutputStream. * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it will execute the block of code and pass in * the socket output stream. * * The thread will terminate after the block of code is executed or any * exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ private[spark] def serveToStream( threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = { SocketAuthServer.serveToStream(threadName, authHelper)(writeFunc) } private def getMergedConf(confAsMap: java.util.HashMap[String, String], baseConf: Configuration): Configuration = { val conf = PythonHadoopUtil.mapToConf(confAsMap) PythonHadoopUtil.mergeConfs(baseConf, conf) } private def inferKeyValueTypes[K, V, KK, VV](rdd: RDD[(K, V)], keyConverterClass: String = null, valueConverterClass: String = null): (Class[_ <: KK], Class[_ <: VV]) = { // Peek at an element to figure out key/value types. Since Writables are not serializable, // we cannot call first() on the converted RDD. Instead, we call first() on the original RDD // and then convert locally. val (key, value) = rdd.first() val (kc, vc) = getKeyValueConverters[K, V, KK, VV]( keyConverterClass, valueConverterClass, new JavaToWritableConverter) (kc.convert(key).getClass, vc.convert(value).getClass) } private def getKeyValueTypes[K, V](keyClass: String, valueClass: String): Option[(Class[K], Class[V])] = { for { k <- Option(keyClass) v <- Option(valueClass) } yield (Utils.classForName(k), Utils.classForName(v)) } private def getKeyValueConverters[K, V, KK, VV]( keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[_, _]): (Converter[K, KK], Converter[V, VV]) = { val keyConverter = Converter.getInstance(Option(keyConverterClass), defaultConverter.asInstanceOf[Converter[K, KK]]) val valueConverter = Converter.getInstance(Option(valueConverterClass), defaultConverter.asInstanceOf[Converter[V, VV]]) (keyConverter, valueConverter) } /** * Convert an RDD of key-value pairs from internal types to serializable types suitable for * output, or vice versa. */ private def convertRDD[K, V](rdd: RDD[(K, V)], keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[Any, Any]): RDD[(Any, Any)] = { val (kc, vc) = getKeyValueConverters[K, V, Any, Any](keyConverterClass, valueConverterClass, defaultConverter) PythonHadoopUtil.convertRDD(rdd, kc, vc) } /** * Output a Python RDD of key-value pairs as a Hadoop SequenceFile using the Writable types * we convert from the RDD's key and value types. Note that keys and values can't be * [[org.apache.hadoop.io.Writable]] types already, since Writables are not Java * `Serializable` and we can't peek at them. The `path` can be on any Hadoop file system. */ def saveAsSequenceFile[C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, compressionCodecClass: String): Unit = { saveAsHadoopFile( pyRDD, batchSerialized, path, \"org.apache.hadoop.mapred.SequenceFileOutputFormat\", null, null, null, null, new java.util.HashMap(), compressionCodecClass) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using old Hadoop * `OutputFormat` in mapred package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD. */ def saveAsHadoopFile[F <: OutputFormat[_, _], C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], compressionCodecClass: String): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val codec = Option(compressionCodecClass).map(Utils.classForName(_).asInstanceOf[Class[C]]) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName[F](outputFormatClass) converted.saveAsHadoopFile(path, kc, vc, fc, new JobConf(mergedConf), codec = codec) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using new Hadoop * `OutputFormat` in mapreduce package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD. */ def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String]): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName(outputFormatClass).asInstanceOf[Class[F]] converted.saveAsNewAPIHadoopFile(path, kc, vc, fc, mergedConf) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using a Hadoop conf * converted from the passed-in `confAsMap`. The conf should set relevant output params ( * e.g., output path, output format, etc), in the same way as it would be configured for * a Hadoop MapReduce job. Both old and new Hadoop OutputFormat APIs are supported * (mapred vs. mapreduce). Keys/values are converted for output using either user specified * converters or, by default, [[org.apache.spark.api.python.JavaToWritableConverter]]. */ def saveAsHadoopDataset( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, confAsMap: java.util.HashMap[String, String], keyConverterClass: String, valueConverterClass: String, useNewAPI: Boolean): Unit = { val conf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized), keyConverterClass, valueConverterClass, new JavaToWritableConverter) if (useNewAPI) { converted.saveAsNewAPIHadoopDataset(conf) } else { converted.saveAsHadoopDataset(new JobConf(conf)) } } } private class BytesToString extends org.apache.spark.api.java.function.Function[Array[Byte], String] { override def call(arr: Array[Byte]) : String = new String(arr, StandardCharsets.UTF_8) } /** * Internal class that acts as an `AccumulatorV2` for Python accumulators. Inside, it * collects a list of pickled strings that we pass to Python through a socket. */ private[spark] class PythonAccumulatorV2( @transient private val serverHost: String, private val serverPort: Int, private val secretToken: String) extends CollectionAccumulator[Array[Byte]] with Logging{ Utils.checkHost(serverHost) val bufferSize = SparkEnv.get.conf.get(BUFFER_SIZE) /** * We try to reuse a single Socket to transfer accumulator updates, as they are all added * by the DAGScheduler's single-threaded RpcEndpoint anyway. */ @transient private var socket: Socket = _ private def openSocket(): Socket = synchronized { if (socket == null || socket.isClosed) { socket = new Socket(serverHost, serverPort) logInfo(s\"Connected to AccumulatorServer at host: $serverHost port: $serverPort\") // send the secret just for the initial authentication when opening a new connection socket.getOutputStream.write(secretToken.getBytes(StandardCharsets.UTF_8)) } socket } // Need to override so the types match with PythonFunction override def copyAndReset(): PythonAccumulatorV2 = { new PythonAccumulatorV2(serverHost, serverPort, secretToken) } override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit = synchronized { val otherPythonAccumulator = other.asInstanceOf[PythonAccumulatorV2] // This conditional isn't strictly speaking needed - merging only currently happens on the // driver program - but that isn't guaranteed so incase this changes. if (serverHost == null) { // We are on the worker super.merge(otherPythonAccumulator) } else { // This happens on the master, where we pass the updates to Python through a socket val socket = openSocket() val in = socket.getInputStream val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize)) val values = other.value out.writeInt(values.size) for (array <- values.asScala) { out.writeInt(array.length) out.write(array) } out.flush() // Wait for a byte from the Python side as an acknowledgement val byteRead = in.read() if (byteRead == -1) { throw new SparkException(\"EOF reached before Python server acknowledged\") } } } } private[spark] class PythonBroadcast(@transient var path: String) extends Serializable with Logging { // id of the Broadcast variable which wrapped this PythonBroadcast private var broadcastId: Long = _ private var encryptionServer: SocketAuthServer[Unit] = null private var decryptionServer: SocketAuthServer[Unit] = null /** * Read data from disks, then copy it to `out` */ private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { out.writeLong(broadcastId) val in = new FileInputStream(new File(path)) try { Utils.copyStream(in, out) } finally { in.close() } } /** * Write data into disk and map it to a broadcast block. */ private def readObject(in: ObjectInputStream): Unit = { broadcastId = in.readLong() val blockId = BroadcastBlockId(broadcastId, \"python\") val blockManager = SparkEnv.get.blockManager val diskBlockManager = blockManager.diskBlockManager if (!diskBlockManager.containsBlock(blockId)) { Utils.tryOrIOException { val dir = new File(Utils.getLocalDir(SparkEnv.get.conf)) val file = File.createTempFile(\"broadcast\", \"\", dir) val out = new FileOutputStream(file) Utils.tryWithSafeFinally { val size = Utils.copyStream(in, out) val ct = implicitly[ClassTag[Object]] // SPARK-28486: map broadcast file to a broadcast block, so that it could be // cleared by unpersist/destroy rather than gc(previously). val blockStoreUpdater = blockManager. TempFileBasedBlockStoreUpdater(blockId, StorageLevel.DISK_ONLY, ct, file, size) blockStoreUpdater.save() } { out.close() } } } path = diskBlockManager.getFile(blockId).getAbsolutePath } def setBroadcastId(bid: Long): Unit = { this.broadcastId = bid } def setupEncryptionServer(): Array[Any] = { encryptionServer = new SocketAuthServer[Unit](\"broadcast-encrypt-server\") { override def handleConnection(sock: Socket): Unit = { val env = SparkEnv.get val in = sock.getInputStream() val abspath = new File(path).getAbsolutePath val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath)) DechunkedInputStream.dechunkAndCopyToOutput(in, out) } } Array(encryptionServer.port, encryptionServer.secret) } def setupDecryptionServer(): Array[Any] = { decryptionServer = new SocketAuthServer[Unit](\"broadcast-decrypt-server-for-driver\") { override def handleConnection(sock: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream())) Utils.tryWithSafeFinally { val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } out.flush() } { JavaUtils.closeQuietly(out) } } } Array(decryptionServer.port, decryptionServer.secret) } def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult() def waitTillDataReceived(): Unit = encryptionServer.getResult() } /** * The inverse of pyspark's ChunkedStream for sending data of unknown size. * * We might be serializing a really large object from python -- we don't want * python to buffer the whole thing in memory, nor can it write to a file, * so we don't know the length in advance. So python writes it in chunks, each chunk * preceded by a length, till we get a \"length\" of -1 which serves as EOF. * * Tested from python tests. */ private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging { private val din = new DataInputStream(wrapped) private var remainingInChunk = din.readInt() override def read(): Int = { val into = new Array[Byte](1) val n = read(into, 0, 1) if (n == -1) { -1 } else { // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted // as an EOF val b = into(0) if (b < 0) { 256 + b } else { b } } } override def read(dest: Array[Byte], off: Int, len: Int): Int = { if (remainingInChunk == -1) { return -1 } var destSpace = len var destPos = off while (destSpace > 0 && remainingInChunk != -1) { val toCopy = math.min(remainingInChunk, destSpace) val read = din.read(dest, destPos, toCopy) destPos += read destSpace -= read remainingInChunk -= read if (remainingInChunk == 0) { remainingInChunk = din.readInt() } } assert(destSpace == 0 || remainingInChunk == -1) return destPos - off } override def close(): Unit = wrapped.close() } private[spark] object DechunkedInputStream { /** * Dechunks the input, copies to output, and closes both input and the output safely. */ def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = { val dechunked = new DechunkedInputStream(chunked) Utils.tryWithSafeFinally { Utils.copyStream(dechunked, out) } { JavaUtils.closeQuietly(out) JavaUtils.closeQuietly(dechunked) } } } /** * Sends decrypted broadcast data to python worker. See [[PythonRunner]] for entire protocol. */ private[spark] class EncryptedPythonBroadcastServer( val env: SparkEnv, val idsAndFiles: Seq[(Long, String)]) extends SocketAuthServer[Unit](\"broadcast-decrypt-server\") with Logging { override def handleConnection(socket: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream())) var socketIn: InputStream = null // send the broadcast id, then the decrypted data. We don't need to send the length, the // the python pickle module just needs a stream. Utils.tryWithSafeFinally { (idsAndFiles).foreach { case (id, path) => out.writeLong(id) val in = env.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } } logTrace(\"waiting for python to accept broadcast data over socket\") out.flush() socketIn = socket.getInputStream() socketIn.read() logTrace(\"done serving broadcast data\") } { JavaUtils.closeQuietly(socketIn) JavaUtils.closeQuietly(out) } } def waitTillBroadcastDataSent(): Unit = { getResult() } } /** * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python * over a socket. This is used in preference to writing data to a file when encryption is enabled. */ private[spark] abstract class PythonRDDServer extends SocketAuthServer[JavaRDD[Array[Byte]]](\"pyspark-parallelize-server\") { def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = { val in = sock.getInputStream() val dechunkedInput: InputStream = new DechunkedInputStream(in) streamToRDD(dechunkedInput) } protected def streamToRDD(input: InputStream): RDD[Array[Byte]] } private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int) extends PythonRDDServer { override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = { PythonRDD.readRDDFromInputStream(sc, input, parallelism) } }"
      ],
      "key_entities": [
        "org/apache/spark/api/python/PythonRunner.",
        "org/apache/spark/api/python/BasePythonRunner#compute().",
        "org/apache/spark/api/python/BasePythonRunner#MonitorThread#run().",
        "org/apache/spark/api/python/PythonRDD#compute()."
      ],
      "claude_stats": {
        "answer": "Based on the code provided, I can describe the PythonRunner Object and the methods that use it:\n\n## PythonRunner Object\n\nThe `PythonRunner` object is a companion object (singleton) located in the package `org.apache.spark.api.python`. It serves as a factory and tracking utility for Python worker execution in Apache Spark.\n\n**Key Components:**\n\n1. **Static State:**\n   - Contains a `ConcurrentHashMap` named `runningMonitorThreads` that tracks already running worker monitor threads. It stores pairs of `(Socket, Long)` representing worker sockets and task attempt IDs.\n\n2. **Factory Method:**\n   - `apply(func: PythonFunction): PythonRunner` - This is the main factory method that creates a new `PythonRunner` instance. It wraps the provided `PythonFunction` inside a `ChainedPythonFunctions` sequence and returns a new `PythonRunner`.\n\n## Methods That Use PythonRunner Object\n\nBased on the visible code, here are the methods that use the PythonRunner Object:\n\n### 1. **BasePythonRunner#compute()**\nThis method uses `PythonRunner.runningMonitorThreads` to:\n- Check if a monitor thread already exists for a worker-task pair using `runningMonitorThreads.add(key)`\n- Prevent creating multiple monitor threads for the same Python worker and task context\n- This occurs when `reuseWorker` is enabled\n\n### 2. **BasePythonRunner.MonitorThread#run()**\nThis method uses `PythonRunner.runningMonitorThreads` to:\n- Remove the worker-task pair from the running monitor threads set using `runningMonitorThreads.remove(key)`\n- This cleanup happens in the `finally` block after monitoring is complete, but only when `reuseWorker` is enabled\n\n### 3. **PythonForeachWriter Class**\nThis class uses `PythonRunner` to:\n- Create a Python runner instance via the `apply` factory method: `PythonRunner(func)`\n- Execute Python code for foreach operations on streaming data\n- The runner is initialized lazily and used to process input data iterators\n\n### 4. **PythonRDD#compute()**\nThis method uses `PythonRunner` to:\n- Create a runner instance via `PythonRunner(func)`\n- Execute the runner's `compute` method to process partition data\n- Bridge Scala RDD operations with Python worker processes\n\nThe PythonRunner Object essentially acts as a centralized coordination point for managing Python worker lifecycle and preventing resource conflicts when workers are reused across tasks.",
        "prompt_time": 2.7,
        "answer_time": 33,
        "tokens": 0,
        "context_tokens": 15912,
        "used_context": [
          "## OBJECT: org/apache/spark/api/python/PythonRunner.\n} } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner ",
          "## METHOD: org/apache/spark/api/python/BasePythonRunner#compute().\nmem.map(_ / cores) } def compute( inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): Iterator[OUT] = { val startTime = System.currentTimeMillis val env = SparkEnv.get // Get the executor cores and pyspark memory, they are passed via the local properties when // the user specified them in a ResourceProfile. val execCoresProp = Option(context.getLocalProperty(EXECUTOR_CORES_LOCAL_PROPERTY)) val memoryMb = Option(context.getLocalProperty(PYSPARK_MEMORY_LOCAL_PROPERTY)).map(_.toLong) val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\") // if OMP_NUM_THREADS is not explicitly set, override it with the number of cores if (conf.getOption(\"spark.executorEnv.OMP_NUM_THREADS\").isEmpty) { // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool // see https://github.com/numpy/numpy/issues/10455 execCoresProp.foreach(envVars.put(\"OMP_NUM_THREADS\", _)) } envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread if (reuseWorker) { envVars.put(\"SPARK_REUSE_WORKER\", \"1\") } if (simplifiedTraceback) { envVars.put(\"SPARK_SIMPLIFIED_TRACEBACK\", \"1\") } // SPARK-30299 this could be wrong with standalone mode when executor // cores might not be correct because it defaults to all cores on the box. val execCores = execCoresProp.map(_.toInt).getOrElse(conf.get(EXECUTOR_CORES)) val workerMemoryMb = getWorkerMemoryMb(memoryMb, execCores) if (workerMemoryMb.isDefined) { envVars.put(\"PYSPARK_EXECUTOR_MEMORY_MB\", workerMemoryMb.get.toString) } envVars.put(\"SPARK_AUTH_SOCKET_TIMEOUT\", authSocketTimeout.toString) envVars.put(\"SPARK_BUFFER_SIZE\", bufferSize.toString) if (faultHandlerEnabled) { envVars.put(\"PYTHON_FAULTHANDLER_DIR\", BasePythonRunner.faultHandlerLogDir.toString) } val (worker: Socket, pid: Option[Int]) = env.createPythonWorker( pythonExec, envVars.asScala.toMap) // Whether is the worker released into idle pool or closed. When any codes try to release or // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make // sure there is only one winner that is going to release or close the worker. val releasedOrClosed = new AtomicBoolean(false) // Start a thread to feed the process input from our parent's iterator val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context) context.addTaskCompletionListener[Unit] { _ => writerThread.shutdownOnTaskCompletion() if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) { try { worker.close() } catch { case e: Exception => logWarning(\"Failed to close worker socket\", e) } } } writerThread.start() new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start() if (reuseWorker) { val key = (worker, context.taskAttemptId) // SPARK-35009: avoid creating multiple monitor threads for the same python worker // and task context if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(SparkEnv.get, worker, context).start() } } else { new MonitorThread(SparkEnv.get, worker, context).start() } // Return an iterator that read lines from the process's stdout val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize)) val stdoutIterator = newReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) new InterruptibleIterator(context, stdoutIterator) } protected def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): WriterThread protected def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[OUT] /** * The thread responsible for writing the data from the PythonRDD's parent iterator to the * Python process.  abstract class WriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext) extends Thread(s\"stdout writer for $pythonExec\") { @volatile private var _exception: Throwable = null private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala)) setDaemon(true) /** Contains the throwable thrown while writing the parent iterator to the Python process.  def exception: Option[Throwable] = Option(_exception) /** * Terminates the writer thread and waits for it to exit, ignoring any exceptions that may occur * due to cleanup.  def shutdownOnTaskCompletion(): Unit = { assert(context.isCompleted) this.interrupt() // Task completion listeners that run after this method returns may invalidate // `inputIterator`. For example, when `inputIterator` was generated by the off-heap vectorized // reader, a task completion listener will free the underlying off-heap buffers. If the writer // thread is still running when `inputIterator` is invalidated, it can cause a use-after-free // bug that crashes the executor (SPARK-33277). Therefore this method must wait for the writer // thread to exit before returning. this.join() } /** * Writes a command section to the stream connected to the Python worker.  protected def writeCommand(dataOut: DataOutputStream): Unit /** * Writes input data to the stream connected to the Python worker.  protected def writeIteratorToStream(dataOut: DataOutputStream): Unit override def run(): Unit = Utils.logUncaughtExceptions { try { TaskContext.setTaskContext(context) val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize) val dataOut = new DataOutputStream(stream) // Partition index dataOut.writeInt(partitionIndex) // Python version of driver PythonRDD.writeUTF(pythonVer, dataOut) // Init a ServerSocket to accept method calls from Python side. val isBarrier = context.isInstanceOf[BarrierTaskContext] if (isBarrier) { serverSocket = Some(new ServerSocket(/* port  0, /* backlog  1, InetAddress.getByName(\"localhost\"))) // A call to accept() for ServerSocket shall block infinitely. serverSocket.foreach(_.setSoTimeout(0)) new Thread(\"accept-connections\") { setDaemon(true) override def run(): Unit = { while (!serverSocket.get.isClosed()) { var sock: Socket = null try { sock = serverSocket.get.accept() // Wait for function call from python side. sock.setSoTimeout(10000) authHelper.authClient(sock) val input = new DataInputStream(sock.getInputStream()) val requestMethod = input.readInt() // The BarrierTaskContext function may wait infinitely, socket shall not timeout // before the function finishes. sock.setSoTimeout(0) requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => barrierAndServe(requestMethod, sock) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => val length = input.readInt() val message = new Array[Byte](length) input.readFully(message) barrierAndServe(requestMethod, sock, new String(message, UTF_8)) case _ => val out = new DataOutputStream(new BufferedOutputStream( sock.getOutputStream)) writeUTF(BarrierTaskContextMessageProtocol.ERROR_UNRECOGNIZED_FUNCTION, out) } } catch { case e: SocketException if e.getMessage.contains(\"Socket closed\") => // It is possible that the ServerSocket is not closed, but the native socket // has already been closed, we shall catch and silently ignore this case. } finally { if (sock != null) { sock.close() } } } } }.start() } val secret = if (isBarrier) { authHelper.secret } else { \"\" } // Close ServerSocket on task completion. serverSocket.foreach { server => context.addTaskCompletionListener[Unit](_ => server.close()) } val boundPort: Int = serverSocket.map(_.getLocalPort).getOrElse(0) if (boundPort == -1) { val message = \"ServerSocket failed to bind to Java side.\" logError(message) throw new SparkException(message) } else if (isBarrier) { logDebug(s\"Started ServerSocket on port $boundPort.\") } // Write out the TaskContextInfo dataOut.writeBoolean(isBarrier) dataOut.writeInt(boundPort) val secretBytes = secret.getBytes(UTF_8) dataOut.writeInt(secretBytes.length) dataOut.write(secretBytes, 0, secretBytes.length) dataOut.writeInt(context.stageId()) dataOut.writeInt(context.partitionId()) dataOut.writeInt(context.attemptNumber()) dataOut.writeLong(context.taskAttemptId()) dataOut.writeInt(context.cpus()) val resources = context.resources() dataOut.writeInt(resources.size) resources.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v.name, dataOut) dataOut.writeInt(v.addresses.size) v.addresses.foreach { case addr => PythonRDD.writeUTF(addr, dataOut) } } val localProps = context.getLocalProperties.asScala dataOut.writeInt(localProps.size) localProps.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v, dataOut) } // sparkFilesDir PythonRDD.writeUTF(SparkFiles.getRootDirectory(), dataOut) // Python includes (*.zip and *.egg files) dataOut.writeInt(pythonIncludes.size) for (include <- pythonIncludes) { PythonRDD.writeUTF(include, dataOut) } // Broadcast variables val oldBids = PythonRDD.getWorkerBroadcasts(worker) val newBids = broadcastVars.map(_.id).toSet // number of different broadcasts val toRemove = oldBids.diff(newBids) val addedBids = newBids.diff(oldBids) val cnt = toRemove.size + addedBids.size val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty dataOut.writeBoolean(needsDecryptionServer) dataOut.writeInt(cnt) def sendBidsToRemove(): Unit = { for (bid <- toRemove) { // remove the broadcast from worker dataOut.writeLong(-bid - 1) // bid >= 0 oldBids.remove(bid) } } if (needsDecryptionServer) { // if there is encryption, we setup a server which reads the encrypted files, and sends // the decrypted data to python val idsAndFiles = broadcastVars.flatMap { broadcast => if (!oldBids.contains(broadcast.id)) { Some((broadcast.id, broadcast.value.path)) } else { None } } val server = new EncryptedPythonBroadcastServer(env, idsAndFiles) dataOut.writeInt(server.port) logTrace(s\"broadcast decryption server setup on ${server.port}\") PythonRDD.writeUTF(server.secret, dataOut) sendBidsToRemove() idsAndFiles.foreach { case (id, _) => // send new broadcast dataOut.writeLong(id) oldBids.add(id) } dataOut.flush() logTrace(\"waiting for python to read decrypted broadcast data from server\") server.waitTillBroadcastDataSent() logTrace(\"done sending decrypted data to python\") } else { sendBidsToRemove() for (broadcast <- broadcastVars) { if (!oldBids.contains(broadcast.id)) { // send new broadcast dataOut.writeLong(broadcast.id) PythonRDD.writeUTF(broadcast.value.path, dataOut) oldBids.add(broadcast.id) } } } dataOut.flush() dataOut.writeInt(evalType) writeCommand(dataOut) writeIteratorToStream(dataOut) dataOut.writeInt(SpecialLengths.END_OF_STREAM) dataOut.flush() } catch { case t: Throwable if (NonFatal(t) || t.isInstanceOf[Exception]) => if (context.isCompleted || context.isInterrupted) { logDebug(\"Exception/NonFatal Error thrown after task completion (likely due to \" + \"cleanup)\", t) if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } else { // We must avoid throwing exceptions/NonFatals here, because the thread uncaught // exception handler will kill the whole executor (see // org.apache.spark.executor.Executor). _exception = t if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } } } /** * Gateway to call BarrierTaskContext methods.  def barrierAndServe(requestMethod: Int, sock: Socket, message: String = \"\"): Unit = { require( serverSocket.isDefined, \"No available ServerSocket to redirect the BarrierTaskContext method call.\" ) val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream)) try { val messages = requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => context.asInstanceOf[BarrierTaskContext].barrier() Array(BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => context.asInstanceOf[BarrierTaskContext].allGather(message) } out.writeInt(messages.length) messages.foreach(writeUTF(_, out)) } catch { case e: SparkException => writeUTF(e.getMessage, out) } finally { out.close() } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } } abstract class ReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext) extends Iterator[OUT] { private var nextObj: OUT = _ private var eos = false override def hasNext: Boolean = nextObj != null || { if (!eos) { nextObj = read() hasNext } else { false } } override def next(): OUT = { if (hasNext) { val obj = nextObj nextObj = null.asInstanceOf[OUT] obj } else { Iterator.empty.next() } } /** * Reads next object from the stream. * When the stream reaches end of data, needs to process the following sections, * and then returns null.  protected def read(): OUT protected def handleTimingData(): Unit = { // Timing data from worker val bootTime = stream.readLong() val initTime = stream.readLong() val finishTime = stream.readLong() val boot = bootTime - startTime val init = initTime - bootTime val finish = finishTime - initTime val total = finishTime - startTime logInfo(\"Times: total = %s, boot = %s, init = %s, finish = %s\".format(total, boot, init, finish)) val memoryBytesSpilled = stream.readLong() val diskBytesSpilled = stream.readLong() context.taskMetrics.incMemoryBytesSpilled(memoryBytesSpilled) context.taskMetrics.incDiskBytesSpilled(diskBytesSpilled) } protected def handlePythonException(): PythonException = { // Signals that an exception has been thrown in python val exLength = stream.readInt() val obj = new Array[Byte](exLength) stream.readFully(obj) new PythonException(new String(obj, StandardCharsets.UTF_8), writerThread.exception.getOrElse(null)) } protected def handleEndOfDataSection(): Unit = { // We've finished the data section of the output, but we can still // read some accumulator updates: val numAccumulatorUpdates = stream.readInt() (1 to numAccumulatorUpdates).foreach { _ => val updateLen = stream.readInt() val update = new Array[Byte](updateLen) stream.readFully(update) maybeAccumulator.foreach(_.add(update)) } // Check whether the worker is ready to be re-used. if (stream.readInt() == SpecialLengths.END_OF_STREAM) { if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) { env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker) } } eos = true } protected val handleException: PartialFunction[Throwable, OUT] = { case e: Exception if context.isInterrupted => logDebug(\"Exception thrown after task interruption\", e) throw new TaskKilledException(context.getKillReason().getOrElse(\"unknown reason\")) case e: Exception if writerThread.exception.isDefined => logError(\"Python worker exited unexpectedly (crashed)\", e) logError(\"This may have been caused by a prior exception:\", writerThread.exception.get) throw writerThread.exception.get case eof: EOFException if faultHandlerEnabled && pid.isDefined && JavaFiles.exists(BasePythonRunner.faultHandlerLogPath(pid.get)) => val path = BasePythonRunner.faultHandlerLogPath(pid.get) val error = String.join(\"\\n\", JavaFiles.readAllLines(path)) + \"\\n\" JavaFiles.deleteIfExists(path) throw new SparkException(s\"Python worker exited unexpectedly (crashed): $error\", eof) case eof: EOFException => throw new SparkException(\"Python worker exited unexpectedly (crashed)\", eof) } } /** * It is necessary to have a monitor thread for python workers if the user cancels with * interrupts disabled. In that case we will need to explicitly kill the worker, otherwise the * threads can block indefinitely.  class MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(s\"Worker Monitor for $pythonExec\") { /** How long to wait before killing the python worker if a task cannot be interrupted.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) private def monitorWorker(): Unit = { // Kill the worker if it is interrupted, checking until task completion. // TODO: This has a race condition if interruption occurs, as completed may still become true. while (!context.isInterrupted && !context.isCompleted) { Thread.sleep(2000) } if (!context.isCompleted) { Thread.sleep(taskKillTimeout) if (!context.isCompleted) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning(s\"Incomplete task $taskName interrupted: Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }\n",
          "## CLASS: org/apache/spark/sql/execution/python/PythonForeachWriter#\nclass PythonForeachWriter(func: PythonFunction, schema: StructType) extends ForeachWriter[UnsafeRow] { private lazy val context = TaskContext.get() private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer( context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length) private lazy val inputRowIterator = buffer.iterator private lazy val inputByteIterator = { EvaluatePython.registerPicklers() val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) } new SerDeUtil.AutoBatchedPickler(objIterator) } private lazy val pythonRunner = { PythonRunner(func) } private lazy val outputIterator = pythonRunner.compute(inputByteIterator, context.partitionId(), context) override def open(partitionId: Long, version: Long): Boolean = { outputIterator // initialize everything TaskContext.get.addTaskCompletionListener[Unit] { _ => buffer.close() } true } override def process(value: UnsafeRow): Unit = { buffer.add(value) } override def close(errorOrNull: Throwable): Unit = { buffer.allRowsAdded() if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one } } object PythonForeachWriter { /** * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeachWriter. * It is designed to be used with only 1 writer thread (i.e. JVM task thread) and only 1 reader * thread (i.e. PythonRunner writing thread that reads from the buffer and writes to the Python * worker stdin). Adds to the buffer are non-blocking, and reads through the buffer's iterator * are blocking, that is, it blocks until new data is available or all data has been added. * * Internally, it uses a [[HybridRowQueue]] to buffer the rows in a practically unlimited queue * across memory and local disk. However, HybridRowQueue is designed to be used only with * EvalPythonExec where the reader is always behind the writer, that is, the reader does not * try to read n+1 rows if the writer has only written n rows at any point of time. This * assumption is not true for PythonForeachWriter where rows may be added at a different rate as * they are consumed by the python worker. Hence, to maintain the invariant of the reader being * behind the writer while using HybridRowQueue, the buffer does the following * - Keeps a count of the rows in the HybridRowQueue * - Blocks the buffer's consuming iterator when the count is 0 so that the reader does not * try to read more rows than what has been written. * * The implementation of the blocking iterator (ReentrantLock, Condition, etc.) has been borrowed * from that of ArrayBlockingQueue.  class UnsafeRowBuffer(taskMemoryManager: TaskMemoryManager, tempDir: File, numFields: Int) extends Logging { private val queue = HybridRowQueue(taskMemoryManager, tempDir, numFields) private val lock = new ReentrantLock() private val unblockRemove = lock.newCondition() // All of these are guarded by `lock` private var count = 0L private var allAdded = false private var exception: Throwable = null val iterator = new NextIterator[UnsafeRow] { override protected def getNext(): UnsafeRow = { val row = remove() if (row == null) finished = true row } override protected def close(): Unit = { } } def add(row: UnsafeRow): Unit = withLock { assert(queue.add(row), s\"Failed to add row to HybridRowQueue while sending data to Python\" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count += 1 unblockRemove.signal() logTrace(s\"Added $row, $count left\") } private def remove(): UnsafeRow = withLock { while (count == 0 && !allAdded && exception == null) { unblockRemove.await(100, TimeUnit.MILLISECONDS) } // If there was any error in the adding thread, then rethrow it in the removing thread if (exception != null) throw exception if (count > 0) { val row = queue.remove() assert(row != null, \"HybridRowQueue.remove() returned null \" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count -= 1 logTrace(s\"Removed $row, $count left\") row } else { null } } def allRowsAdded(): Unit = withLock { allAdded = true unblockRemove.signal() } def close(): Unit = { queue.close() } private def withLock[T](f: => T): T = { lock.lockInterruptibly() try { f } catch { case e: Throwable => if (exception == null) exception = e throw e } finally { lock.unlock() } } } }",
          "## METHOD: org/apache/spark/api/python/PythonRunner.apply().\n* * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
          "## METHOD: org/apache/spark/api/python/BasePythonRunner#MonitorThread#run().\n} } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
          "## METHOD: org/apache/spark/api/python/PythonRDD#compute().\noverride def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = { val runner = PythonRunner(func) runner.compute(firstParent.iterator(split, context), split.index, context) } @transient protected lazy override val isBarrier_ : Boolean = isFromBarrier || dependencies.exists(_.rdd.isBarrier()) } /** * A wrapper for a Python function, contains all necessary context to run the function in Python * runner.  private[spark] case class PythonFunction( command: Seq[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) { def this( command: Array[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) = { this(command.toSeq, envVars, pythonIncludes, pythonExec, pythonVer, broadcastVars, accumulator) } } /** * A wrapper for chained Python functions (from bottom to top). * @param funcs  private[spark] case class ChainedPythonFunctions(funcs: Seq[PythonFunction]) /** Thrown for exceptions in user Python code.  private[spark] class PythonException(msg: String, cause: Throwable) extends RuntimeException(msg, cause) /** * Form an RDD[(Array[Byte], Array[Byte])] from key-value pairs returned from Python. * This is used by PySpark's shuffle operations.  private class PairwiseRDD(prev: RDD[Array[Byte]]) extends RDD[(Long, Array[Byte])](prev) { override def getPartitions: Array[Partition] = prev.partitions override val partitioner: Option[Partitioner] = prev.partitioner override def compute(split: Partition, context: TaskContext): Iterator[(Long, Array[Byte])] = prev.iterator(split, context).grouped(2).map { case Seq(a, b) => (Utils.deserializeLongValue(a), b) case x => throw new SparkException(\"PairwiseRDD: unexpected value: \" + x) } val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this) } private[spark] object PythonRDD extends Logging { // remember the broadcasts sent to each worker private val workerBroadcasts = new mutable.WeakHashMap[Socket, mutable.Set[Long]]() // Authentication helper used when serving iterator data. private lazy val authHelper = { val conf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf()) new SocketAuthHelper(conf) } def getWorkerBroadcasts(worker: Socket): mutable.Set[Long] = { synchronized { workerBroadcasts.getOrElseUpdate(worker, new mutable.HashSet[Long]()) } } /** * Return an RDD of values from an RDD of (Long, Array[Byte]), with preservePartitions=true * * This is useful for PySpark to have the partitioner after partitionBy()  def valueOfPair(pair: JavaPairRDD[Long, Array[Byte]]): JavaRDD[Array[Byte]] = { pair.rdd.mapPartitions(it => it.map(_._2), true) } /** * Adapter for calling SparkContext#runJob from Python. * * This method will serve an iterator of an array that contains all elements in the RDD * (effectively a collect()), but allows you to run on a certain subset of partitions, * or to enable local execution. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def runJob( sc: SparkContext, rdd: JavaRDD[Array[Byte]], partitions: JArrayList[Int]): Array[Any] = { type ByteArray = Array[Byte] type UnrolledPartition = Array[ByteArray] val allPartitions: Array[UnrolledPartition] = sc.runJob(rdd, (x: Iterator[ByteArray]) => x.toArray, partitions.asScala.toSeq) val flattenedPartition: UnrolledPartition = Array.concat(allPartitions: _*) serveIterator(flattenedPartition.iterator, s\"serve RDD ${rdd.id} with partitions ${partitions.asScala.mkString(\",\")}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def collectAndServe[T](rdd: RDD[T]): Array[Any] = { serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * This method is similar with `PythonRDD.collectAndServe`, but user can specify job group id, * job description, and interruptOnCancel option.  def collectAndServeWithJobGroup[T]( rdd: RDD[T], groupId: String, description: String, interruptOnCancel: Boolean): Array[Any] = { val sc = rdd.sparkContext sc.setJobGroup(groupId, description, interruptOnCancel) serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to create a local RDD iterator and serve it via socket. Partitions are * are collected as separate jobs, by order of index. Partition data is first requested by a * non-zero integer to start a collection job. The response is prefaced by an integer with 1 * meaning partition data will be served, 0 meaning the local iterator has been consumed, * and -1 meaning an error occurred during collection. This function is used by * pyspark.rdd._local_iterator_from_socket(). * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def toLocalIteratorAndServe[T](rdd: RDD[T], prefetchPartitions: Boolean = false): Array[Any] = { val handleFunc = (sock: Socket) => { val out = new DataOutputStream(sock.getOutputStream) val in = new DataInputStream(sock.getInputStream) Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Collects a partition on each iteration val collectPartitionIter = rdd.partitions.indices.iterator.map { i => var result: Array[Any] = null rdd.sparkContext.submitJob( rdd, (iter: Iterator[Any]) => iter.toArray, Seq(i), // The partition we are evaluating (_, res: Array[Any]) => result = res, result) } val prefetchIter = collectPartitionIter.buffered // Write data until iteration is complete, client stops iteration, or error occurs var complete = false while (!complete) { // Read request for data, value of zero will stop iteration or non-zero to continue if (in.readInt() == 0) { complete = true } else if (prefetchIter.hasNext) { // Client requested more data, attempt to collect the next partition val partitionFuture = prefetchIter.next() // Cause the next job to be submitted if prefetchPartitions is enabled. if (prefetchPartitions) { prefetchIter.headOption } val partitionArray = ThreadUtils.awaitResult(partitionFuture, Duration.Inf) // Send response there is a partition to read out.writeInt(1) // Write the next object and signal end of data for this iteration writeIteratorToStream(partitionArray.iterator, out) out.writeInt(SpecialLengths.END_OF_DATA_SECTION) out.flush() } else { // Send response there are no more partitions to read and close out.writeInt(0) complete = true } } })(catchBlock = { // Send response that an error occurred, original exception is re-thrown out.writeInt(-1) }, finallyBlock = { out.close() in.close() }) } val server = new SocketFuncServer(authHelper, \"serve toLocalIterator\", handleFunc) Array(server.port, server.secret, server) } def readRDDFromFile( sc: JavaSparkContext, filename: String, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromFile(sc, filename, parallelism) } def readRDDFromInputStream( sc: SparkContext, in: InputStream, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromInputStream(sc, in, parallelism) } def setupBroadcast(path: String): PythonBroadcast = { new PythonBroadcast(path) } def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream): Unit = { def write(obj: Any): Unit = obj match { case null => dataOut.writeInt(SpecialLengths.NULL) case arr: Array[Byte] => dataOut.writeInt(arr.length) dataOut.write(arr) case str: String => writeUTF(str, dataOut) case stream: PortableDataStream => write(stream.toArray()) case (key, value) => write(key) write(value) case other => throw new SparkException(\"Unexpected element type \" + other.getClass) } iter.foreach(write) } /** * Create an RDD from a path using [[org.apache.hadoop.mapred.SequenceFileInputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def sequenceFile[K, V]( sc: JavaSparkContext, path: String, keyClassMaybeNull: String, valueClassMaybeNull: String, keyConverterClass: String, valueConverterClass: String, minSplits: Int, batchSize: Int): JavaRDD[Array[Byte]] = { val keyClass = Option(keyClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val valueClass = Option(valueClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val rdd = sc.sc.sequenceFile[K, V](path, kc, vc, minSplits) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration())) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map that is * passed in from Python, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def newAPIHadoopRDDFromClassNames[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration): RDD[(K, V)] = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.newAPIHadoopFile[K, V, F](path.get, fc, kc, vc, conf) } else { sc.sc.newAPIHadoopRDD[K, V, F](conf, fc, kc, vc) } } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopFile[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map * that is passed in from Python, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopRDD[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def hadoopRDDFromClassNames[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration) = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.hadoopFile(path.get, fc, kc, vc) } else { sc.sc.hadoopRDD(new JobConf(conf), fc, kc, vc) } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(StandardCharsets.UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } /** * Create a socket server and a background thread to serve the data in `items`, * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it tries to serialize all the data in `items` * and send them into this connection. * * The thread will terminate after all the data are sent or any exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def serveIterator(items: Iterator[_], threadName: String): Array[Any] = { serveToStream(threadName) { out => writeIteratorToStream(items, new DataOutputStream(out)) } } /** * Create a socket server and background thread to execute the writeFunc * with the given OutputStream. * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it will execute the block of code and pass in * the socket output stream. * * The thread will terminate after the block of code is executed or any * exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  private[spark] def serveToStream( threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = { SocketAuthServer.serveToStream(threadName, authHelper)(writeFunc) } private def getMergedConf(confAsMap: java.util.HashMap[String, String], baseConf: Configuration): Configuration = { val conf = PythonHadoopUtil.mapToConf(confAsMap) PythonHadoopUtil.mergeConfs(baseConf, conf) } private def inferKeyValueTypes[K, V, KK, VV](rdd: RDD[(K, V)], keyConverterClass: String = null, valueConverterClass: String = null): (Class[_ <: KK], Class[_ <: VV]) = { // Peek at an element to figure out key/value types. Since Writables are not serializable, // we cannot call first() on the converted RDD. Instead, we call first() on the original RDD // and then convert locally. val (key, value) = rdd.first() val (kc, vc) = getKeyValueConverters[K, V, KK, VV]( keyConverterClass, valueConverterClass, new JavaToWritableConverter) (kc.convert(key).getClass, vc.convert(value).getClass) } private def getKeyValueTypes[K, V](keyClass: String, valueClass: String): Option[(Class[K], Class[V])] = { for { k <- Option(keyClass) v <- Option(valueClass) } yield (Utils.classForName(k), Utils.classForName(v)) } private def getKeyValueConverters[K, V, KK, VV]( keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[_, _]): (Converter[K, KK], Converter[V, VV]) = { val keyConverter = Converter.getInstance(Option(keyConverterClass), defaultConverter.asInstanceOf[Converter[K, KK]]) val valueConverter = Converter.getInstance(Option(valueConverterClass), defaultConverter.asInstanceOf[Converter[V, VV]]) (keyConverter, valueConverter) } /** * Convert an RDD of key-value pairs from internal types to serializable types suitable for * output, or vice versa.  private def convertRDD[K, V](rdd: RDD[(K, V)], keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[Any, Any]): RDD[(Any, Any)] = { val (kc, vc) = getKeyValueConverters[K, V, Any, Any](keyConverterClass, valueConverterClass, defaultConverter) PythonHadoopUtil.convertRDD(rdd, kc, vc) } /** * Output a Python RDD of key-value pairs as a Hadoop SequenceFile using the Writable types * we convert from the RDD's key and value types. Note that keys and values can't be * [[org.apache.hadoop.io.Writable]] types already, since Writables are not Java * `Serializable` and we can't peek at them. The `path` can be on any Hadoop file system.  def saveAsSequenceFile[C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, compressionCodecClass: String): Unit = { saveAsHadoopFile( pyRDD, batchSerialized, path, \"org.apache.hadoop.mapred.SequenceFileOutputFormat\", null, null, null, null, new java.util.HashMap(), compressionCodecClass) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using old Hadoop * `OutputFormat` in mapred package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsHadoopFile[F <: OutputFormat[_, _], C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], compressionCodecClass: String): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val codec = Option(compressionCodecClass).map(Utils.classForName(_).asInstanceOf[Class[C]]) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName[F](outputFormatClass) converted.saveAsHadoopFile(path, kc, vc, fc, new JobConf(mergedConf), codec = codec) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using new Hadoop * `OutputFormat` in mapreduce package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String]): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName(outputFormatClass).asInstanceOf[Class[F]] converted.saveAsNewAPIHadoopFile(path, kc, vc, fc, mergedConf) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using a Hadoop conf * converted from the passed-in `confAsMap`. The conf should set relevant output params ( * e.g., output path, output format, etc), in the same way as it would be configured for * a Hadoop MapReduce job. Both old and new Hadoop OutputFormat APIs are supported * (mapred vs. mapreduce). Keys/values are converted for output using either user specified * converters or, by default, [[org.apache.spark.api.python.JavaToWritableConverter]].  def saveAsHadoopDataset( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, confAsMap: java.util.HashMap[String, String], keyConverterClass: String, valueConverterClass: String, useNewAPI: Boolean): Unit = { val conf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized), keyConverterClass, valueConverterClass, new JavaToWritableConverter) if (useNewAPI) { converted.saveAsNewAPIHadoopDataset(conf) } else { converted.saveAsHadoopDataset(new JobConf(conf)) } } } private class BytesToString extends org.apache.spark.api.java.function.Function[Array[Byte], String] { override def call(arr: Array[Byte]) : String = new String(arr, StandardCharsets.UTF_8) } /** * Internal class that acts as an `AccumulatorV2` for Python accumulators. Inside, it * collects a list of pickled strings that we pass to Python through a socket.  private[spark] class PythonAccumulatorV2( @transient private val serverHost: String, private val serverPort: Int, private val secretToken: String) extends CollectionAccumulator[Array[Byte]] with Logging{ Utils.checkHost(serverHost) val bufferSize = SparkEnv.get.conf.get(BUFFER_SIZE) /** * We try to reuse a single Socket to transfer accumulator updates, as they are all added * by the DAGScheduler's single-threaded RpcEndpoint anyway.  @transient private var socket: Socket = _ private def openSocket(): Socket = synchronized { if (socket == null || socket.isClosed) { socket = new Socket(serverHost, serverPort) logInfo(s\"Connected to AccumulatorServer at host: $serverHost port: $serverPort\") // send the secret just for the initial authentication when opening a new connection socket.getOutputStream.write(secretToken.getBytes(StandardCharsets.UTF_8)) } socket } // Need to override so the types match with PythonFunction override def copyAndReset(): PythonAccumulatorV2 = { new PythonAccumulatorV2(serverHost, serverPort, secretToken) } override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit = synchronized { val otherPythonAccumulator = other.asInstanceOf[PythonAccumulatorV2] // This conditional isn't strictly speaking needed - merging only currently happens on the // driver program - but that isn't guaranteed so incase this changes. if (serverHost == null) { // We are on the worker super.merge(otherPythonAccumulator) } else { // This happens on the master, where we pass the updates to Python through a socket val socket = openSocket() val in = socket.getInputStream val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize)) val values = other.value out.writeInt(values.size) for (array <- values.asScala) { out.writeInt(array.length) out.write(array) } out.flush() // Wait for a byte from the Python side as an acknowledgement val byteRead = in.read() if (byteRead == -1) { throw new SparkException(\"EOF reached before Python server acknowledged\") } } } } private[spark] class PythonBroadcast(@transient var path: String) extends Serializable with Logging { // id of the Broadcast variable which wrapped this PythonBroadcast private var broadcastId: Long = _ private var encryptionServer: SocketAuthServer[Unit] = null private var decryptionServer: SocketAuthServer[Unit] = null /** * Read data from disks, then copy it to `out`  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { out.writeLong(broadcastId) val in = new FileInputStream(new File(path)) try { Utils.copyStream(in, out) } finally { in.close() } } /** * Write data into disk and map it to a broadcast block.  private def readObject(in: ObjectInputStream): Unit = { broadcastId = in.readLong() val blockId = BroadcastBlockId(broadcastId, \"python\") val blockManager = SparkEnv.get.blockManager val diskBlockManager = blockManager.diskBlockManager if (!diskBlockManager.containsBlock(blockId)) { Utils.tryOrIOException { val dir = new File(Utils.getLocalDir(SparkEnv.get.conf)) val file = File.createTempFile(\"broadcast\", \"\", dir) val out = new FileOutputStream(file) Utils.tryWithSafeFinally { val size = Utils.copyStream(in, out) val ct = implicitly[ClassTag[Object]] // SPARK-28486: map broadcast file to a broadcast block, so that it could be // cleared by unpersist/destroy rather than gc(previously). val blockStoreUpdater = blockManager. TempFileBasedBlockStoreUpdater(blockId, StorageLevel.DISK_ONLY, ct, file, size) blockStoreUpdater.save() } { out.close() } } } path = diskBlockManager.getFile(blockId).getAbsolutePath } def setBroadcastId(bid: Long): Unit = { this.broadcastId = bid } def setupEncryptionServer(): Array[Any] = { encryptionServer = new SocketAuthServer[Unit](\"broadcast-encrypt-server\") { override def handleConnection(sock: Socket): Unit = { val env = SparkEnv.get val in = sock.getInputStream() val abspath = new File(path).getAbsolutePath val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath)) DechunkedInputStream.dechunkAndCopyToOutput(in, out) } } Array(encryptionServer.port, encryptionServer.secret) } def setupDecryptionServer(): Array[Any] = { decryptionServer = new SocketAuthServer[Unit](\"broadcast-decrypt-server-for-driver\") { override def handleConnection(sock: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream())) Utils.tryWithSafeFinally { val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } out.flush() } { JavaUtils.closeQuietly(out) } } } Array(decryptionServer.port, decryptionServer.secret) } def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult() def waitTillDataReceived(): Unit = encryptionServer.getResult() } /** * The inverse of pyspark's ChunkedStream for sending data of unknown size. * * We might be serializing a really large object from python -- we don't want * python to buffer the whole thing in memory, nor can it write to a file, * so we don't know the length in advance. So python writes it in chunks, each chunk * preceded by a length, till we get a \"length\" of -1 which serves as EOF. * * Tested from python tests.  private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging { private val din = new DataInputStream(wrapped) private var remainingInChunk = din.readInt() override def read(): Int = { val into = new Array[Byte](1) val n = read(into, 0, 1) if (n == -1) { -1 } else { // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted // as an EOF val b = into(0) if (b < 0) { 256 + b } else { b } } } override def read(dest: Array[Byte], off: Int, len: Int): Int = { if (remainingInChunk == -1) { return -1 } var destSpace = len var destPos = off while (destSpace > 0 && remainingInChunk != -1) { val toCopy = math.min(remainingInChunk, destSpace) val read = din.read(dest, destPos, toCopy) destPos += read destSpace -= read remainingInChunk -= read if (remainingInChunk == 0) { remainingInChunk = din.readInt() } } assert(destSpace == 0 || remainingInChunk == -1) return destPos - off } override def close(): Unit = wrapped.close() } private[spark] object DechunkedInputStream { /** * Dechunks the input, copies to output, and closes both input and the output safely.  def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = { val dechunked = new DechunkedInputStream(chunked) Utils.tryWithSafeFinally { Utils.copyStream(dechunked, out) } { JavaUtils.closeQuietly(out) JavaUtils.closeQuietly(dechunked) } } } /** * Sends decrypted broadcast data to python worker. See [[PythonRunner]] for entire protocol.  private[spark] class EncryptedPythonBroadcastServer( val env: SparkEnv, val idsAndFiles: Seq[(Long, String)]) extends SocketAuthServer[Unit](\"broadcast-decrypt-server\") with Logging { override def handleConnection(socket: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream())) var socketIn: InputStream = null // send the broadcast id, then the decrypted data. We don't need to send the length, the // the python pickle module just needs a stream. Utils.tryWithSafeFinally { (idsAndFiles).foreach { case (id, path) => out.writeLong(id) val in = env.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } } logTrace(\"waiting for python to accept broadcast data over socket\") out.flush() socketIn = socket.getInputStream() socketIn.read() logTrace(\"done serving broadcast data\") } { JavaUtils.closeQuietly(socketIn) JavaUtils.closeQuietly(out) } } def waitTillBroadcastDataSent(): Unit = { getResult() } } /** * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python * over a socket. This is used in preference to writing data to a file when encryption is enabled.  private[spark] abstract class PythonRDDServer extends SocketAuthServer[JavaRDD[Array[Byte]]](\"pyspark-parallelize-server\") { def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = { val in = sock.getInputStream() val dechunkedInput: InputStream = new DechunkedInputStream(in) streamToRDD(dechunkedInput) } protected def streamToRDD(input: InputStream): RDD[Array[Byte]] } private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int) extends PythonRDDServer { override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = { PythonRDD.readRDDFromInputStream(sc, input, parallelism) } }"
        ],
        "more_info": "Used ANY in relation_types instead of call_by"
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### PythonRunner object and methods that use it\n\nScope limited strictly to the code present in this repository. All paths and signatures are taken verbatim from the sources.\n\n1) Definition: PythonRunner companion object\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala\n- Fully-qualified name: org.apache.spark.api.python.PythonRunner (object)\n- Visibility: private[spark]\n- Members:\n  - val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]()\n    - Type: java.util.Set[(Socket, Long)] created via ConcurrentHashMap.newKeySet\n    - Purpose in code: tracks already running worker monitor threads for (worker socket, taskAttemptId) pairs.\n  - def apply(func: PythonFunction): PythonRunner\n    - Body: new PythonRunner(Seq(ChainedPythonFunctions(Seq(func))))\n    - Role: convenience constructor that wraps a single PythonFunction into a ChainedPythonFunctions sequence and instantiates a PythonRunner (class).\n\n2) Methods that reference the PythonRunner object\n\nA. org.apache.spark.api.python.PythonRDD#compute\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala\n- Signature:\n  override def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]]\n- Usage of PythonRunner object:\n  - val runner = PythonRunner(func)\n  - Calls the companion object's apply to create a PythonRunner, then invokes runner.compute(...) to execute.\n- Exact snippet (abridged, preserving the relevant lines):\n  val runner = PythonRunner(func)\n  runner.compute(firstParent.iterator(split, context), split.index, context)\n\nB. org.apache.spark.sql.execution.python.PythonForeachWriter (class)  lazy vals that invoke PythonRunner.apply\n- File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonForeachWriter.scala\n- Declarations:\n  - private lazy val pythonRunner = { PythonRunner(func) }\n  - private lazy val outputIterator =\n      pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n- How it is used by methods:\n  - open(partitionId: Long, version: Long): Boolean triggers initialization of lazy vals by referencing outputIterator.\n  - close(errorOrNull: Throwable): accesses outputIterator (hasNext/next()).\n- Methods in this class that indirectly use the PythonRunner object through these lazy vals:\n  - override def open(partitionId: Long, version: Long): Boolean\n  - override def close(errorOrNull: Throwable): Unit\n\nC. org.apache.spark.api.python.BasePythonRunner#compute  references PythonRunner.runningMonitorThreads\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala (inside BasePythonRunner)\n- Signature:\n  def compute(\n      inputIterator: Iterator[IN],\n      partitionIndex: Int,\n      context: TaskContext): Iterator[OUT]\n- Usage of PythonRunner object:\n  - When reuseWorker is true, it constructs a key of type (Socket, Long) and calls\n    if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(...).start() }\n- This avoids creating multiple monitor threads for the same python worker and task context.\n\nD. org.apache.spark.api.python.BasePythonRunner#MonitorThread#run  references PythonRunner.runningMonitorThreads\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala (inside BasePythonRunner)\n- Class: MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(...)\n- Method signature:\n  override def run(): Unit\n- Usage of PythonRunner object:\n  - In the finally block, when reuseWorker is true, it removes the key from the set:\n    val key = (worker, context.taskAttemptId)\n    PythonRunner.runningMonitorThreads.remove(key)\n\nNotes\n- The term \"PythonRunner\" refers to two distinct entities in the same file:\n  1) The companion object described in section 1.\n  2) The class private[spark] class PythonRunner(...) which extends BasePythonRunner[...] and implements the concrete writer/reader behavior. This document focuses on the object and the methods that reference the object per the request.\n- All behaviors and relationships above are stated exactly as visible in code without external assumptions.",
          "prompt_time": 0.191,
          "answer_time": 197,
          "context_tokens": 15912,
          "used_context": [
            "## OBJECT: org/apache/spark/api/python/PythonRunner.\n} } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner ",
            "## METHOD: org/apache/spark/api/python/BasePythonRunner#compute().\nmem.map(_ / cores) } def compute( inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): Iterator[OUT] = { val startTime = System.currentTimeMillis val env = SparkEnv.get // Get the executor cores and pyspark memory, they are passed via the local properties when // the user specified them in a ResourceProfile. val execCoresProp = Option(context.getLocalProperty(EXECUTOR_CORES_LOCAL_PROPERTY)) val memoryMb = Option(context.getLocalProperty(PYSPARK_MEMORY_LOCAL_PROPERTY)).map(_.toLong) val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\") // if OMP_NUM_THREADS is not explicitly set, override it with the number of cores if (conf.getOption(\"spark.executorEnv.OMP_NUM_THREADS\").isEmpty) { // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool // see https://github.com/numpy/numpy/issues/10455 execCoresProp.foreach(envVars.put(\"OMP_NUM_THREADS\", _)) } envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread if (reuseWorker) { envVars.put(\"SPARK_REUSE_WORKER\", \"1\") } if (simplifiedTraceback) { envVars.put(\"SPARK_SIMPLIFIED_TRACEBACK\", \"1\") } // SPARK-30299 this could be wrong with standalone mode when executor // cores might not be correct because it defaults to all cores on the box. val execCores = execCoresProp.map(_.toInt).getOrElse(conf.get(EXECUTOR_CORES)) val workerMemoryMb = getWorkerMemoryMb(memoryMb, execCores) if (workerMemoryMb.isDefined) { envVars.put(\"PYSPARK_EXECUTOR_MEMORY_MB\", workerMemoryMb.get.toString) } envVars.put(\"SPARK_AUTH_SOCKET_TIMEOUT\", authSocketTimeout.toString) envVars.put(\"SPARK_BUFFER_SIZE\", bufferSize.toString) if (faultHandlerEnabled) { envVars.put(\"PYTHON_FAULTHANDLER_DIR\", BasePythonRunner.faultHandlerLogDir.toString) } val (worker: Socket, pid: Option[Int]) = env.createPythonWorker( pythonExec, envVars.asScala.toMap) // Whether is the worker released into idle pool or closed. When any codes try to release or // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make // sure there is only one winner that is going to release or close the worker. val releasedOrClosed = new AtomicBoolean(false) // Start a thread to feed the process input from our parent's iterator val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context) context.addTaskCompletionListener[Unit] { _ => writerThread.shutdownOnTaskCompletion() if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) { try { worker.close() } catch { case e: Exception => logWarning(\"Failed to close worker socket\", e) } } } writerThread.start() new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start() if (reuseWorker) { val key = (worker, context.taskAttemptId) // SPARK-35009: avoid creating multiple monitor threads for the same python worker // and task context if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(SparkEnv.get, worker, context).start() } } else { new MonitorThread(SparkEnv.get, worker, context).start() } // Return an iterator that read lines from the process's stdout val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize)) val stdoutIterator = newReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) new InterruptibleIterator(context, stdoutIterator) } protected def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): WriterThread protected def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[OUT] /** * The thread responsible for writing the data from the PythonRDD's parent iterator to the * Python process.  abstract class WriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext) extends Thread(s\"stdout writer for $pythonExec\") { @volatile private var _exception: Throwable = null private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala)) setDaemon(true) /** Contains the throwable thrown while writing the parent iterator to the Python process.  def exception: Option[Throwable] = Option(_exception) /** * Terminates the writer thread and waits for it to exit, ignoring any exceptions that may occur * due to cleanup.  def shutdownOnTaskCompletion(): Unit = { assert(context.isCompleted) this.interrupt() // Task completion listeners that run after this method returns may invalidate // `inputIterator`. For example, when `inputIterator` was generated by the off-heap vectorized // reader, a task completion listener will free the underlying off-heap buffers. If the writer // thread is still running when `inputIterator` is invalidated, it can cause a use-after-free // bug that crashes the executor (SPARK-33277). Therefore this method must wait for the writer // thread to exit before returning. this.join() } /** * Writes a command section to the stream connected to the Python worker.  protected def writeCommand(dataOut: DataOutputStream): Unit /** * Writes input data to the stream connected to the Python worker.  protected def writeIteratorToStream(dataOut: DataOutputStream): Unit override def run(): Unit = Utils.logUncaughtExceptions { try { TaskContext.setTaskContext(context) val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize) val dataOut = new DataOutputStream(stream) // Partition index dataOut.writeInt(partitionIndex) // Python version of driver PythonRDD.writeUTF(pythonVer, dataOut) // Init a ServerSocket to accept method calls from Python side. val isBarrier = context.isInstanceOf[BarrierTaskContext] if (isBarrier) { serverSocket = Some(new ServerSocket(/* port  0, /* backlog  1, InetAddress.getByName(\"localhost\"))) // A call to accept() for ServerSocket shall block infinitely. serverSocket.foreach(_.setSoTimeout(0)) new Thread(\"accept-connections\") { setDaemon(true) override def run(): Unit = { while (!serverSocket.get.isClosed()) { var sock: Socket = null try { sock = serverSocket.get.accept() // Wait for function call from python side. sock.setSoTimeout(10000) authHelper.authClient(sock) val input = new DataInputStream(sock.getInputStream()) val requestMethod = input.readInt() // The BarrierTaskContext function may wait infinitely, socket shall not timeout // before the function finishes. sock.setSoTimeout(0) requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => barrierAndServe(requestMethod, sock) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => val length = input.readInt() val message = new Array[Byte](length) input.readFully(message) barrierAndServe(requestMethod, sock, new String(message, UTF_8)) case _ => val out = new DataOutputStream(new BufferedOutputStream( sock.getOutputStream)) writeUTF(BarrierTaskContextMessageProtocol.ERROR_UNRECOGNIZED_FUNCTION, out) } } catch { case e: SocketException if e.getMessage.contains(\"Socket closed\") => // It is possible that the ServerSocket is not closed, but the native socket // has already been closed, we shall catch and silently ignore this case. } finally { if (sock != null) { sock.close() } } } } }.start() } val secret = if (isBarrier) { authHelper.secret } else { \"\" } // Close ServerSocket on task completion. serverSocket.foreach { server => context.addTaskCompletionListener[Unit](_ => server.close()) } val boundPort: Int = serverSocket.map(_.getLocalPort).getOrElse(0) if (boundPort == -1) { val message = \"ServerSocket failed to bind to Java side.\" logError(message) throw new SparkException(message) } else if (isBarrier) { logDebug(s\"Started ServerSocket on port $boundPort.\") } // Write out the TaskContextInfo dataOut.writeBoolean(isBarrier) dataOut.writeInt(boundPort) val secretBytes = secret.getBytes(UTF_8) dataOut.writeInt(secretBytes.length) dataOut.write(secretBytes, 0, secretBytes.length) dataOut.writeInt(context.stageId()) dataOut.writeInt(context.partitionId()) dataOut.writeInt(context.attemptNumber()) dataOut.writeLong(context.taskAttemptId()) dataOut.writeInt(context.cpus()) val resources = context.resources() dataOut.writeInt(resources.size) resources.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v.name, dataOut) dataOut.writeInt(v.addresses.size) v.addresses.foreach { case addr => PythonRDD.writeUTF(addr, dataOut) } } val localProps = context.getLocalProperties.asScala dataOut.writeInt(localProps.size) localProps.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v, dataOut) } // sparkFilesDir PythonRDD.writeUTF(SparkFiles.getRootDirectory(), dataOut) // Python includes (*.zip and *.egg files) dataOut.writeInt(pythonIncludes.size) for (include <- pythonIncludes) { PythonRDD.writeUTF(include, dataOut) } // Broadcast variables val oldBids = PythonRDD.getWorkerBroadcasts(worker) val newBids = broadcastVars.map(_.id).toSet // number of different broadcasts val toRemove = oldBids.diff(newBids) val addedBids = newBids.diff(oldBids) val cnt = toRemove.size + addedBids.size val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty dataOut.writeBoolean(needsDecryptionServer) dataOut.writeInt(cnt) def sendBidsToRemove(): Unit = { for (bid <- toRemove) { // remove the broadcast from worker dataOut.writeLong(-bid - 1) // bid >= 0 oldBids.remove(bid) } } if (needsDecryptionServer) { // if there is encryption, we setup a server which reads the encrypted files, and sends // the decrypted data to python val idsAndFiles = broadcastVars.flatMap { broadcast => if (!oldBids.contains(broadcast.id)) { Some((broadcast.id, broadcast.value.path)) } else { None } } val server = new EncryptedPythonBroadcastServer(env, idsAndFiles) dataOut.writeInt(server.port) logTrace(s\"broadcast decryption server setup on ${server.port}\") PythonRDD.writeUTF(server.secret, dataOut) sendBidsToRemove() idsAndFiles.foreach { case (id, _) => // send new broadcast dataOut.writeLong(id) oldBids.add(id) } dataOut.flush() logTrace(\"waiting for python to read decrypted broadcast data from server\") server.waitTillBroadcastDataSent() logTrace(\"done sending decrypted data to python\") } else { sendBidsToRemove() for (broadcast <- broadcastVars) { if (!oldBids.contains(broadcast.id)) { // send new broadcast dataOut.writeLong(broadcast.id) PythonRDD.writeUTF(broadcast.value.path, dataOut) oldBids.add(broadcast.id) } } } dataOut.flush() dataOut.writeInt(evalType) writeCommand(dataOut) writeIteratorToStream(dataOut) dataOut.writeInt(SpecialLengths.END_OF_STREAM) dataOut.flush() } catch { case t: Throwable if (NonFatal(t) || t.isInstanceOf[Exception]) => if (context.isCompleted || context.isInterrupted) { logDebug(\"Exception/NonFatal Error thrown after task completion (likely due to \" + \"cleanup)\", t) if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } else { // We must avoid throwing exceptions/NonFatals here, because the thread uncaught // exception handler will kill the whole executor (see // org.apache.spark.executor.Executor). _exception = t if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } } } /** * Gateway to call BarrierTaskContext methods.  def barrierAndServe(requestMethod: Int, sock: Socket, message: String = \"\"): Unit = { require( serverSocket.isDefined, \"No available ServerSocket to redirect the BarrierTaskContext method call.\" ) val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream)) try { val messages = requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => context.asInstanceOf[BarrierTaskContext].barrier() Array(BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => context.asInstanceOf[BarrierTaskContext].allGather(message) } out.writeInt(messages.length) messages.foreach(writeUTF(_, out)) } catch { case e: SparkException => writeUTF(e.getMessage, out) } finally { out.close() } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } } abstract class ReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext) extends Iterator[OUT] { private var nextObj: OUT = _ private var eos = false override def hasNext: Boolean = nextObj != null || { if (!eos) { nextObj = read() hasNext } else { false } } override def next(): OUT = { if (hasNext) { val obj = nextObj nextObj = null.asInstanceOf[OUT] obj } else { Iterator.empty.next() } } /** * Reads next object from the stream. * When the stream reaches end of data, needs to process the following sections, * and then returns null.  protected def read(): OUT protected def handleTimingData(): Unit = { // Timing data from worker val bootTime = stream.readLong() val initTime = stream.readLong() val finishTime = stream.readLong() val boot = bootTime - startTime val init = initTime - bootTime val finish = finishTime - initTime val total = finishTime - startTime logInfo(\"Times: total = %s, boot = %s, init = %s, finish = %s\".format(total, boot, init, finish)) val memoryBytesSpilled = stream.readLong() val diskBytesSpilled = stream.readLong() context.taskMetrics.incMemoryBytesSpilled(memoryBytesSpilled) context.taskMetrics.incDiskBytesSpilled(diskBytesSpilled) } protected def handlePythonException(): PythonException = { // Signals that an exception has been thrown in python val exLength = stream.readInt() val obj = new Array[Byte](exLength) stream.readFully(obj) new PythonException(new String(obj, StandardCharsets.UTF_8), writerThread.exception.getOrElse(null)) } protected def handleEndOfDataSection(): Unit = { // We've finished the data section of the output, but we can still // read some accumulator updates: val numAccumulatorUpdates = stream.readInt() (1 to numAccumulatorUpdates).foreach { _ => val updateLen = stream.readInt() val update = new Array[Byte](updateLen) stream.readFully(update) maybeAccumulator.foreach(_.add(update)) } // Check whether the worker is ready to be re-used. if (stream.readInt() == SpecialLengths.END_OF_STREAM) { if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) { env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker) } } eos = true } protected val handleException: PartialFunction[Throwable, OUT] = { case e: Exception if context.isInterrupted => logDebug(\"Exception thrown after task interruption\", e) throw new TaskKilledException(context.getKillReason().getOrElse(\"unknown reason\")) case e: Exception if writerThread.exception.isDefined => logError(\"Python worker exited unexpectedly (crashed)\", e) logError(\"This may have been caused by a prior exception:\", writerThread.exception.get) throw writerThread.exception.get case eof: EOFException if faultHandlerEnabled && pid.isDefined && JavaFiles.exists(BasePythonRunner.faultHandlerLogPath(pid.get)) => val path = BasePythonRunner.faultHandlerLogPath(pid.get) val error = String.join(\"\\n\", JavaFiles.readAllLines(path)) + \"\\n\" JavaFiles.deleteIfExists(path) throw new SparkException(s\"Python worker exited unexpectedly (crashed): $error\", eof) case eof: EOFException => throw new SparkException(\"Python worker exited unexpectedly (crashed)\", eof) } } /** * It is necessary to have a monitor thread for python workers if the user cancels with * interrupts disabled. In that case we will need to explicitly kill the worker, otherwise the * threads can block indefinitely.  class MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(s\"Worker Monitor for $pythonExec\") { /** How long to wait before killing the python worker if a task cannot be interrupted.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) private def monitorWorker(): Unit = { // Kill the worker if it is interrupted, checking until task completion. // TODO: This has a race condition if interruption occurs, as completed may still become true. while (!context.isInterrupted && !context.isCompleted) { Thread.sleep(2000) } if (!context.isCompleted) { Thread.sleep(taskKillTimeout) if (!context.isCompleted) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning(s\"Incomplete task $taskName interrupted: Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
            "## CLASS: org/apache/spark/sql/execution/python/PythonForeachWriter#\nclass PythonForeachWriter(func: PythonFunction, schema: StructType) extends ForeachWriter[UnsafeRow] { private lazy val context = TaskContext.get() private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer( context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length) private lazy val inputRowIterator = buffer.iterator private lazy val inputByteIterator = { EvaluatePython.registerPicklers() val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) } new SerDeUtil.AutoBatchedPickler(objIterator) } private lazy val pythonRunner = { PythonRunner(func) } private lazy val outputIterator = pythonRunner.compute(inputByteIterator, context.partitionId(), context) override def open(partitionId: Long, version: Long): Boolean = { outputIterator // initialize everything TaskContext.get.addTaskCompletionListener[Unit] { _ => buffer.close() } true } override def process(value: UnsafeRow): Unit = { buffer.add(value) } override def close(errorOrNull: Throwable): Unit = { buffer.allRowsAdded() if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one } } object PythonForeachWriter { /** * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeachWriter. * It is designed to be used with only 1 writer thread (i.e. JVM task thread) and only 1 reader * thread (i.e. PythonRunner writing thread that reads from the buffer and writes to the Python * worker stdin). Adds to the buffer are non-blocking, and reads through the buffer's iterator * are blocking, that is, it blocks until new data is available or all data has been added. * * Internally, it uses a [[HybridRowQueue]] to buffer the rows in a practically unlimited queue * across memory and local disk. However, HybridRowQueue is designed to be used only with * EvalPythonExec where the reader is always behind the writer, that is, the reader does not * try to read n+1 rows if the writer has only written n rows at any point of time. This * assumption is not true for PythonForeachWriter where rows may be added at a different rate as * they are consumed by the python worker. Hence, to maintain the invariant of the reader being * behind the writer while using HybridRowQueue, the buffer does the following * - Keeps a count of the rows in the HybridRowQueue * - Blocks the buffer's consuming iterator when the count is 0 so that the reader does not * try to read more rows than what has been written. * * The implementation of the blocking iterator (ReentrantLock, Condition, etc.) has been borrowed * from that of ArrayBlockingQueue.  class UnsafeRowBuffer(taskMemoryManager: TaskMemoryManager, tempDir: File, numFields: Int) extends Logging { private val queue = HybridRowQueue(taskMemoryManager, tempDir, numFields) private val lock = new ReentrantLock() private val unblockRemove = lock.newCondition() // All of these are guarded by `lock` private var count = 0L private var allAdded = false private var exception: Throwable = null val iterator = new NextIterator[UnsafeRow] { override protected def getNext(): UnsafeRow = { val row = remove() if (row == null) finished = true row } override protected def close(): Unit = { } } def add(row: UnsafeRow): Unit = withLock { assert(queue.add(row), s\"Failed to add row to HybridRowQueue while sending data to Python\" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count += 1 unblockRemove.signal() logTrace(s\"Added $row, $count left\") } private def remove(): UnsafeRow = withLock { while (count == 0 && !allAdded && exception == null) { unblockRemove.await(100, TimeUnit.MILLISECONDS) } // If there was any error in the adding thread, then rethrow it in the removing thread if (exception != null) throw exception if (count > 0) { val row = queue.remove() assert(row != null, \"HybridRowQueue.remove() returned null \" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count -= 1 logTrace(s\"Removed $row, $count left\") row } else { null } } def allRowsAdded(): Unit = withLock { allAdded = true unblockRemove.signal() } def close(): Unit = { queue.close() } private def withLock[T](f: => T): T = { lock.lockInterruptibly() try { f } catch { case e: Throwable => if (exception == null) exception = e throw e } finally { lock.unlock() } } } }",
            "## METHOD: org/apache/spark/api/python/PythonRunner.apply().\n* * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }\n",
            "## METHOD: org/apache/spark/api/python/BasePythonRunner#MonitorThread#run().\n} } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
            "## METHOD: org/apache/spark/api/python/PythonRDD#compute().\noverride def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = { val runner = PythonRunner(func) runner.compute(firstParent.iterator(split, context), split.index, context) } @transient protected lazy override val isBarrier_ : Boolean = isFromBarrier || dependencies.exists(_.rdd.isBarrier()) } /** * A wrapper for a Python function, contains all necessary context to run the function in Python * runner.  private[spark] case class PythonFunction( command: Seq[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) { def this( command: Array[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) = { this(command.toSeq, envVars, pythonIncludes, pythonExec, pythonVer, broadcastVars, accumulator) } } /** * A wrapper for chained Python functions (from bottom to top). * @param funcs  private[spark] case class ChainedPythonFunctions(funcs: Seq[PythonFunction]) /** Thrown for exceptions in user Python code.  private[spark] class PythonException(msg: String, cause: Throwable) extends RuntimeException(msg, cause) /** * Form an RDD[(Array[Byte], Array[Byte])] from key-value pairs returned from Python. * This is used by PySpark's shuffle operations.  private class PairwiseRDD(prev: RDD[Array[Byte]]) extends RDD[(Long, Array[Byte])](prev) { override def getPartitions: Array[Partition] = prev.partitions override val partitioner: Option[Partitioner] = prev.partitioner override def compute(split: Partition, context: TaskContext): Iterator[(Long, Array[Byte])] = prev.iterator(split, context).grouped(2).map { case Seq(a, b) => (Utils.deserializeLongValue(a), b) case x => throw new SparkException(\"PairwiseRDD: unexpected value: \" + x) } val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this) } private[spark] object PythonRDD extends Logging { // remember the broadcasts sent to each worker private val workerBroadcasts = new mutable.WeakHashMap[Socket, mutable.Set[Long]]() // Authentication helper used when serving iterator data. private lazy val authHelper = { val conf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf()) new SocketAuthHelper(conf) } def getWorkerBroadcasts(worker: Socket): mutable.Set[Long] = { synchronized { workerBroadcasts.getOrElseUpdate(worker, new mutable.HashSet[Long]()) } } /** * Return an RDD of values from an RDD of (Long, Array[Byte]), with preservePartitions=true * * This is useful for PySpark to have the partitioner after partitionBy()  def valueOfPair(pair: JavaPairRDD[Long, Array[Byte]]): JavaRDD[Array[Byte]] = { pair.rdd.mapPartitions(it => it.map(_._2), true) } /** * Adapter for calling SparkContext#runJob from Python. * * This method will serve an iterator of an array that contains all elements in the RDD * (effectively a collect()), but allows you to run on a certain subset of partitions, * or to enable local execution. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def runJob( sc: SparkContext, rdd: JavaRDD[Array[Byte]], partitions: JArrayList[Int]): Array[Any] = { type ByteArray = Array[Byte] type UnrolledPartition = Array[ByteArray] val allPartitions: Array[UnrolledPartition] = sc.runJob(rdd, (x: Iterator[ByteArray]) => x.toArray, partitions.asScala.toSeq) val flattenedPartition: UnrolledPartition = Array.concat(allPartitions: _*) serveIterator(flattenedPartition.iterator, s\"serve RDD ${rdd.id} with partitions ${partitions.asScala.mkString(\",\")}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def collectAndServe[T](rdd: RDD[T]): Array[Any] = { serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * This method is similar with `PythonRDD.collectAndServe`, but user can specify job group id, * job description, and interruptOnCancel option.  def collectAndServeWithJobGroup[T]( rdd: RDD[T], groupId: String, description: String, interruptOnCancel: Boolean): Array[Any] = { val sc = rdd.sparkContext sc.setJobGroup(groupId, description, interruptOnCancel) serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to create a local RDD iterator and serve it via socket. Partitions are * are collected as separate jobs, by order of index. Partition data is first requested by a * non-zero integer to start a collection job. The response is prefaced by an integer with 1 * meaning partition data will be served, 0 meaning the local iterator has been consumed, * and -1 meaning an error occurred during collection. This function is used by * pyspark.rdd._local_iterator_from_socket(). * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def toLocalIteratorAndServe[T](rdd: RDD[T], prefetchPartitions: Boolean = false): Array[Any] = { val handleFunc = (sock: Socket) => { val out = new DataOutputStream(sock.getOutputStream) val in = new DataInputStream(sock.getInputStream) Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Collects a partition on each iteration val collectPartitionIter = rdd.partitions.indices.iterator.map { i => var result: Array[Any] = null rdd.sparkContext.submitJob( rdd, (iter: Iterator[Any]) => iter.toArray, Seq(i), // The partition we are evaluating (_, res: Array[Any]) => result = res, result) } val prefetchIter = collectPartitionIter.buffered // Write data until iteration is complete, client stops iteration, or error occurs var complete = false while (!complete) { // Read request for data, value of zero will stop iteration or non-zero to continue if (in.readInt() == 0) { complete = true } else if (prefetchIter.hasNext) { // Client requested more data, attempt to collect the next partition val partitionFuture = prefetchIter.next() // Cause the next job to be submitted if prefetchPartitions is enabled. if (prefetchPartitions) { prefetchIter.headOption } val partitionArray = ThreadUtils.awaitResult(partitionFuture, Duration.Inf) // Send response there is a partition to read out.writeInt(1) // Write the next object and signal end of data for this iteration writeIteratorToStream(partitionArray.iterator, out) out.writeInt(SpecialLengths.END_OF_DATA_SECTION) out.flush() } else { // Send response there are no more partitions to read and close out.writeInt(0) complete = true } } })(catchBlock = { // Send response that an error occurred, original exception is re-thrown out.writeInt(-1) }, finallyBlock = { out.close() in.close() }) } val server = new SocketFuncServer(authHelper, \"serve toLocalIterator\", handleFunc) Array(server.port, server.secret, server) } def readRDDFromFile( sc: JavaSparkContext, filename: String, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromFile(sc, filename, parallelism) } def readRDDFromInputStream( sc: SparkContext, in: InputStream, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromInputStream(sc, in, parallelism) } def setupBroadcast(path: String): PythonBroadcast = { new PythonBroadcast(path) } def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream): Unit = { def write(obj: Any): Unit = obj match { case null => dataOut.writeInt(SpecialLengths.NULL) case arr: Array[Byte] => dataOut.writeInt(arr.length) dataOut.write(arr) case str: String => writeUTF(str, dataOut) case stream: PortableDataStream => write(stream.toArray()) case (key, value) => write(key) write(value) case other => throw new SparkException(\"Unexpected element type \" + other.getClass) } iter.foreach(write) } /** * Create an RDD from a path using [[org.apache.hadoop.mapred.SequenceFileInputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def sequenceFile[K, V]( sc: JavaSparkContext, path: String, keyClassMaybeNull: String, valueClassMaybeNull: String, keyConverterClass: String, valueConverterClass: String, minSplits: Int, batchSize: Int): JavaRDD[Array[Byte]] = { val keyClass = Option(keyClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val valueClass = Option(valueClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val rdd = sc.sc.sequenceFile[K, V](path, kc, vc, minSplits) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration())) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map that is * passed in from Python, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def newAPIHadoopRDDFromClassNames[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration): RDD[(K, V)] = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.newAPIHadoopFile[K, V, F](path.get, fc, kc, vc, conf) } else { sc.sc.newAPIHadoopRDD[K, V, F](conf, fc, kc, vc) } } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopFile[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map * that is passed in from Python, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopRDD[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def hadoopRDDFromClassNames[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration) = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.hadoopFile(path.get, fc, kc, vc) } else { sc.sc.hadoopRDD(new JobConf(conf), fc, kc, vc) } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(StandardCharsets.UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } /** * Create a socket server and a background thread to serve the data in `items`, * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it tries to serialize all the data in `items` * and send them into this connection. * * The thread will terminate after all the data are sent or any exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def serveIterator(items: Iterator[_], threadName: String): Array[Any] = { serveToStream(threadName) { out => writeIteratorToStream(items, new DataOutputStream(out)) } } /** * Create a socket server and background thread to execute the writeFunc * with the given OutputStream. * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it will execute the block of code and pass in * the socket output stream. * * The thread will terminate after the block of code is executed or any * exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  private[spark] def serveToStream( threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = { SocketAuthServer.serveToStream(threadName, authHelper)(writeFunc) } private def getMergedConf(confAsMap: java.util.HashMap[String, String], baseConf: Configuration): Configuration = { val conf = PythonHadoopUtil.mapToConf(confAsMap) PythonHadoopUtil.mergeConfs(baseConf, conf) } private def inferKeyValueTypes[K, V, KK, VV](rdd: RDD[(K, V)], keyConverterClass: String = null, valueConverterClass: String = null): (Class[_ <: KK], Class[_ <: VV]) = { // Peek at an element to figure out key/value types. Since Writables are not serializable, // we cannot call first() on the converted RDD. Instead, we call first() on the original RDD // and then convert locally. val (key, value) = rdd.first() val (kc, vc) = getKeyValueConverters[K, V, KK, VV]( keyConverterClass, valueConverterClass, new JavaToWritableConverter) (kc.convert(key).getClass, vc.convert(value).getClass) } private def getKeyValueTypes[K, V](keyClass: String, valueClass: String): Option[(Class[K], Class[V])] = { for { k <- Option(keyClass) v <- Option(valueClass) } yield (Utils.classForName(k), Utils.classForName(v)) } private def getKeyValueConverters[K, V, KK, VV]( keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[_, _]): (Converter[K, KK], Converter[V, VV]) = { val keyConverter = Converter.getInstance(Option(keyConverterClass), defaultConverter.asInstanceOf[Converter[K, KK]]) val valueConverter = Converter.getInstance(Option(valueConverterClass), defaultConverter.asInstanceOf[Converter[V, VV]]) (keyConverter, valueConverter) } /** * Convert an RDD of key-value pairs from internal types to serializable types suitable for * output, or vice versa.  private def convertRDD[K, V](rdd: RDD[(K, V)], keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[Any, Any]): RDD[(Any, Any)] = { val (kc, vc) = getKeyValueConverters[K, V, Any, Any](keyConverterClass, valueConverterClass, defaultConverter) PythonHadoopUtil.convertRDD(rdd, kc, vc) } /** * Output a Python RDD of key-value pairs as a Hadoop SequenceFile using the Writable types * we convert from the RDD's key and value types. Note that keys and values can't be * [[org.apache.hadoop.io.Writable]] types already, since Writables are not Java * `Serializable` and we can't peek at them. The `path` can be on any Hadoop file system.  def saveAsSequenceFile[C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, compressionCodecClass: String): Unit = { saveAsHadoopFile( pyRDD, batchSerialized, path, \"org.apache.hadoop.mapred.SequenceFileOutputFormat\", null, null, null, null, new java.util.HashMap(), compressionCodecClass) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using old Hadoop * `OutputFormat` in mapred package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsHadoopFile[F <: OutputFormat[_, _], C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], compressionCodecClass: String): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val codec = Option(compressionCodecClass).map(Utils.classForName(_).asInstanceOf[Class[C]]) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName[F](outputFormatClass) converted.saveAsHadoopFile(path, kc, vc, fc, new JobConf(mergedConf), codec = codec) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using new Hadoop * `OutputFormat` in mapreduce package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String]): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName(outputFormatClass).asInstanceOf[Class[F]] converted.saveAsNewAPIHadoopFile(path, kc, vc, fc, mergedConf) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using a Hadoop conf * converted from the passed-in `confAsMap`. The conf should set relevant output params ( * e.g., output path, output format, etc), in the same way as it would be configured for * a Hadoop MapReduce job. Both old and new Hadoop OutputFormat APIs are supported * (mapred vs. mapreduce). Keys/values are converted for output using either user specified * converters or, by default, [[org.apache.spark.api.python.JavaToWritableConverter]].  def saveAsHadoopDataset( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, confAsMap: java.util.HashMap[String, String], keyConverterClass: String, valueConverterClass: String, useNewAPI: Boolean): Unit = { val conf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized), keyConverterClass, valueConverterClass, new JavaToWritableConverter) if (useNewAPI) { converted.saveAsNewAPIHadoopDataset(conf) } else { converted.saveAsHadoopDataset(new JobConf(conf)) } } } private class BytesToString extends org.apache.spark.api.java.function.Function[Array[Byte], String] { override def call(arr: Array[Byte]) : String = new String(arr, StandardCharsets.UTF_8) } /** * Internal class that acts as an `AccumulatorV2` for Python accumulators. Inside, it * collects a list of pickled strings that we pass to Python through a socket.  private[spark] class PythonAccumulatorV2( @transient private val serverHost: String, private val serverPort: Int, private val secretToken: String) extends CollectionAccumulator[Array[Byte]] with Logging{ Utils.checkHost(serverHost) val bufferSize = SparkEnv.get.conf.get(BUFFER_SIZE) /** * We try to reuse a single Socket to transfer accumulator updates, as they are all added * by the DAGScheduler's single-threaded RpcEndpoint anyway.  @transient private var socket: Socket = _ private def openSocket(): Socket = synchronized { if (socket == null || socket.isClosed) { socket = new Socket(serverHost, serverPort) logInfo(s\"Connected to AccumulatorServer at host: $serverHost port: $serverPort\") // send the secret just for the initial authentication when opening a new connection socket.getOutputStream.write(secretToken.getBytes(StandardCharsets.UTF_8)) } socket } // Need to override so the types match with PythonFunction override def copyAndReset(): PythonAccumulatorV2 = { new PythonAccumulatorV2(serverHost, serverPort, secretToken) } override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit = synchronized { val otherPythonAccumulator = other.asInstanceOf[PythonAccumulatorV2] // This conditional isn't strictly speaking needed - merging only currently happens on the // driver program - but that isn't guaranteed so incase this changes. if (serverHost == null) { // We are on the worker super.merge(otherPythonAccumulator) } else { // This happens on the master, where we pass the updates to Python through a socket val socket = openSocket() val in = socket.getInputStream val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize)) val values = other.value out.writeInt(values.size) for (array <- values.asScala) { out.writeInt(array.length) out.write(array) } out.flush() // Wait for a byte from the Python side as an acknowledgement val byteRead = in.read() if (byteRead == -1) { throw new SparkException(\"EOF reached before Python server acknowledged\") } } } } private[spark] class PythonBroadcast(@transient var path: String) extends Serializable with Logging { // id of the Broadcast variable which wrapped this PythonBroadcast private var broadcastId: Long = _ private var encryptionServer: SocketAuthServer[Unit] = null private var decryptionServer: SocketAuthServer[Unit] = null /** * Read data from disks, then copy it to `out`  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { out.writeLong(broadcastId) val in = new FileInputStream(new File(path)) try { Utils.copyStream(in, out) } finally { in.close() } } /** * Write data into disk and map it to a broadcast block.  private def readObject(in: ObjectInputStream): Unit = { broadcastId = in.readLong() val blockId = BroadcastBlockId(broadcastId, \"python\") val blockManager = SparkEnv.get.blockManager val diskBlockManager = blockManager.diskBlockManager if (!diskBlockManager.containsBlock(blockId)) { Utils.tryOrIOException { val dir = new File(Utils.getLocalDir(SparkEnv.get.conf)) val file = File.createTempFile(\"broadcast\", \"\", dir) val out = new FileOutputStream(file) Utils.tryWithSafeFinally { val size = Utils.copyStream(in, out) val ct = implicitly[ClassTag[Object]] // SPARK-28486: map broadcast file to a broadcast block, so that it could be // cleared by unpersist/destroy rather than gc(previously). val blockStoreUpdater = blockManager. TempFileBasedBlockStoreUpdater(blockId, StorageLevel.DISK_ONLY, ct, file, size) blockStoreUpdater.save() } { out.close() } } } path = diskBlockManager.getFile(blockId).getAbsolutePath } def setBroadcastId(bid: Long): Unit = { this.broadcastId = bid } def setupEncryptionServer(): Array[Any] = { encryptionServer = new SocketAuthServer[Unit](\"broadcast-encrypt-server\") { override def handleConnection(sock: Socket): Unit = { val env = SparkEnv.get val in = sock.getInputStream() val abspath = new File(path).getAbsolutePath val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath)) DechunkedInputStream.dechunkAndCopyToOutput(in, out) } } Array(encryptionServer.port, encryptionServer.secret) } def setupDecryptionServer(): Array[Any] = { decryptionServer = new SocketAuthServer[Unit](\"broadcast-decrypt-server-for-driver\") { override def handleConnection(sock: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream())) Utils.tryWithSafeFinally { val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } out.flush() } { JavaUtils.closeQuietly(out) } } } Array(decryptionServer.port, decryptionServer.secret) } def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult() def waitTillDataReceived(): Unit = encryptionServer.getResult() } /** * The inverse of pyspark's ChunkedStream for sending data of unknown size. * * We might be serializing a really large object from python -- we don't want * python to buffer the whole thing in memory, nor can it write to a file, * so we don't know the length in advance. So python writes it in chunks, each chunk * preceded by a length, till we get a \"length\" of -1 which serves as EOF. * * Tested from python tests.  private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging { private val din = new DataInputStream(wrapped) private var remainingInChunk = din.readInt() override def read(): Int = { val into = new Array[Byte](1) val n = read(into, 0, 1) if (n == -1) { -1 } else { // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted // as an EOF val b = into(0) if (b < 0) { 256 + b } else { b } } } override def read(dest: Array[Byte], off: Int, len: Int): Int = { if (remainingInChunk == -1) { return -1 } var destSpace = len var destPos = off while (destSpace > 0 && remainingInChunk != -1) { val toCopy = math.min(remainingInChunk, destSpace) val read = din.read(dest, destPos, toCopy) destPos += read destSpace -= read remainingInChunk -= read if (remainingInChunk == 0) { remainingInChunk = din.readInt() } } assert(destSpace == 0 || remainingInChunk == -1) return destPos - off } override def close(): Unit = wrapped.close() } private[spark] object DechunkedInputStream { /** * Dechunks the input, copies to output, and closes both input and the output safely.  def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = { val dechunked = new DechunkedInputStream(chunked) Utils.tryWithSafeFinally { Utils.copyStream(dechunked, out) } { JavaUtils.closeQuietly(out) JavaUtils.closeQuietly(dechunked) } } } /** * Sends decrypted broadcast data to python worker. See [[PythonRunner]] for entire protocol.  private[spark] class EncryptedPythonBroadcastServer( val env: SparkEnv, val idsAndFiles: Seq[(Long, String)]) extends SocketAuthServer[Unit](\"broadcast-decrypt-server\") with Logging { override def handleConnection(socket: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream())) var socketIn: InputStream = null // send the broadcast id, then the decrypted data. We don't need to send the length, the // the python pickle module just needs a stream. Utils.tryWithSafeFinally { (idsAndFiles).foreach { case (id, path) => out.writeLong(id) val in = env.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } } logTrace(\"waiting for python to accept broadcast data over socket\") out.flush() socketIn = socket.getInputStream() socketIn.read() logTrace(\"done serving broadcast data\") } { JavaUtils.closeQuietly(socketIn) JavaUtils.closeQuietly(out) } } def waitTillBroadcastDataSent(): Unit = { getResult() } } /** * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python * over a socket. This is used in preference to writing data to a file when encryption is enabled.  private[spark] abstract class PythonRDDServer extends SocketAuthServer[JavaRDD[Array[Byte]]](\"pyspark-parallelize-server\") { def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = { val in = sock.getInputStream() val dechunkedInput: InputStream = new DechunkedInputStream(in) streamToRDD(dechunkedInput) } protected def streamToRDD(input: InputStream): RDD[Array[Byte]] } private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int) extends PythonRDDServer { override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = { PythonRDD.readRDDFromInputStream(sc, input, parallelism) } }"
          ],
          "tokens": 0,
          "more_info": "After getting context started checking files on it's own and relation_types is ANY, returned answer in .md file"
        },
        "without_mcp": {
          "answer": "# Spark PythonRunner: Objects, Classes, and Usages\n\nThis document describes the two distinct PythonRunner entities present in this Spark 3.3.0 source tree and all methods/usages that interact with them.\n\n- org.apache.spark.deploy.PythonRunner (a standalone launcher for Python applications used by spark-submit)\n- org.apache.spark.api.python.PythonRunner (an object with factory apply and a helper class to run Python functions/UDFs inside executors)\n\nIt also enumerates where and how these are used across the codebase.\n\n\n## 1) org.apache.spark.deploy.PythonRunner\n\nFile: core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n\nRole: Entry point used by SparkSubmit to launch Python applications. It starts a Py4J gateway, constructs PYTHONPATH (including pyFiles), and executes the target Python script as a subprocess with the proper environment variables.\n\n### Public API (object methods)\n\n- def main(args: Array[String]): Unit\n  - Args: \n    - args(0): pythonFile (path/URI to the Python script to run)\n    - args(1): pyFiles (comma-delimited list of additional Python files/archives)\n    - args(2..): any additional user arguments to pass to the Python script\n  - Behavior:\n    - Determines pythonExec using SparkConf and environment: spark.pyspark.driver.python, spark.pyspark.python, PYSPARK_DRIVER_PYTHON, PYSPARK_PYTHON; default \"python3\".\n    - Normalizes pythonFile and pyFiles via formatPath/formatPaths; resolves .py files into a temp directory via resolvePyFiles so that PYTHONPATH receives directories, not single .py files.\n    - Starts a Py4JServer to expose a JVM gateway for the Python process; publishes PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET.\n    - Builds PYTHONPATH with formattedPyFiles + Sparks python path + existing PYTHONPATH.\n    - Exports PYTHONUNBUFFERED=YES, propagates PYSPARK_PYTHON if set, and PYTHONHASHSEED if present.\n    - Optionally sets OMP_NUM_THREADS to spark.driver.cores when not explicitly set in yarn/mesos/k8s driver env, to avoid oversized OpenMP thread pools (SPARK-28843).\n    - Launches the Python process (redirecting stderr into stdout). Streams output to the JVM console via RedirectThread. Exits with SparkUserAppException(exitCode) on non-zero exit; always shuts down the gateway server.\n\n- def formatPath(path: String, testWindows: Boolean = false): String\n  - Purpose: Normalize a path/URI for inclusion in PYTHONPATH. Spark only supports local python files here.\n  - Behavior:\n    - Throws IllegalArgumentException if the path is non-local (Utils.nonLocalPaths nonEmpty) or malformed.\n    - Accepts schemes: file, local, or no scheme. Extracts uri.getPath where applicable.\n    - On Windows, strips leading \"/\" off drive-letter paths like \"/C:/...\".\n\n- def formatPaths(paths: String, testWindows: Boolean = false): Array[String]\n  - Purpose: Normalize a comma-delimited list of paths using formatPath.\n  - Behavior: Splits on comma, drops empties, maps formatPath over each entry.\n\n### Private helpers\n\n- private def resolvePyFiles(pyFiles: Array[String]): Array[String]\n  - Purpose: Ensure .py files are not added directly to PYTHONPATH (which expects directories). Copies readable .py files to a temp dir and returns that directory, deduplicated with other path entries.\n\n### Configuration and environment interactions\n\n- Reads: spark.pyspark.driver.python, spark.pyspark.python, spark.driver.cores, various driverEnv.OMP_NUM_THREADS keys.\n- Sets environment variables: PYTHONPATH, PYTHONUNBUFFERED, PYSPARK_GATEWAY_PORT, PYSPARK_GATEWAY_SECRET, optionally PYSPARK_PYTHON, PYTHONHASHSEED, OMP_NUM_THREADS.\n\n### Known usages across the codebase\n\n- org.apache.spark.deploy.SparkSubmit.scala\n  - args.mainClass = \"org.apache.spark.deploy.PythonRunner\" (line ~503)\n  - Adds \"--class org.apache.spark.deploy.PythonRunner\" in certain codepaths and formats pyFiles:\n    - PythonRunner.formatPaths(Utils.resolveURIs(pyFiles)).mkString(\",\")\n- org.apache.spark.deploy.RRunner.scala\n  - val rFile = PythonRunner.formatPath(args(0))  shares path normalization logic with R runner.\n- org.apache.spark.scheduler.DAGScheduler.scala\n  - Comment reference explaining pyspark resource propagation via PythonRunner.\n- resource-managers/yarn/.../ApplicationMaster.scala\n  - Comment reference for PySpark app run using PythonRunner.\n- Tests\n  - core/src/test/scala/.../PythonRunnerSuite.scala validates formatPath and formatPaths behavior including Windows paths.\n  - core/src/test/scala/.../SparkSubmitSuite.scala uses PythonRunner.formatPaths in expectations.\n\n\n## 2) org.apache.spark.api.python.PythonRunner\n\nFile: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala\n\nRole: Infrastructure for executing Python code in Spark tasks/executors. Consists of:\n- BasePythonRunner[IN, OUT]: abstract class implementing the Python worker protocol and I/O plumbing.\n- object PythonRunner: factory to wrap a single PythonFunction into the helper class.\n- class PythonRunner: concrete subclass of BasePythonRunner for NON_UDF mapPartitions-style functions, using raw byte streams.\n\nAlthough the BasePythonRunner is the core, this document focuses on the PythonRunner object and the PythonRunner class and how other parts of Spark use them.\n\n### object PythonRunner\n\n- Visibility: private[spark]\n- Members:\n  - val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]()\n    - Tracks running worker monitor threads by (workerSocket, taskAttemptId).\n  - def apply(func: PythonFunction): PythonRunner\n    - Usage: Convenience constructor to create a PythonRunner with a single chained function.\n    - Implementation: new PythonRunner(Seq(ChainedPythonFunctions(Seq(func))))\n\n### class PythonRunner(funcs: Seq[ChainedPythonFunctions])\n\n- Visibility: private[spark]\n- Extends: BasePythonRunner[Array[Byte], Array[Byte]](funcs, PythonEvalType.NON_UDF, Array(Array(0)))\n- Purpose: Executes normal (non-UDF) Python functions over partition data via the PySpark worker protocol.\n- Key overrides:\n  - protected override def newWriterThread(...): WriterThread\n    - Writes the command bytes for the function and then the input iterator:\n      - writeCommand: obtains the command from funcs.head.funcs.head.command, writes its length and bytes.\n      - writeIteratorToStream: uses PythonRDD.writeIteratorToStream to serialize input rows to the worker, then writes SpecialLengths.END_OF_DATA_SECTION.\n  - protected override def newReaderIterator(...): Iterator[Array[Byte]]\n    - Reads responses from the Python worker using a DataInputStream protocol:\n      - Positive length N: read N bytes as one result record.\n      - 0: empty byte array.\n      - SpecialLengths.TIMING_DATA: consume timing payload and continue.\n      - SpecialLengths.PYTHON_EXCEPTION_THROWN: decode and throw a Python-side exception in the JVM.\n      - SpecialLengths.END_OF_DATA_SECTION: handle section end and return null to signal iterator end.\n    - Propagates exceptions observed in the writer thread.\n\n- Relevant protocol constants (SpecialLengths): END_OF_DATA_SECTION, PYTHON_EXCEPTION_THROWN, TIMING_DATA, END_OF_STREAM, NULL, START_ARROW_STREAM.\n\n- Interactions inherited from BasePythonRunner (high level):\n  - Manages Python worker process acquisition/reuse, environment variables, auth sockets, buffer sizes.\n  - Supports barrier/allGather messaging for barrier execution mode.\n  - Can enable faulthandler logging and simplified tracebacks.\n  - Monitors writer thread to detect potential deadlocks; after a timeout, destroys the Python worker.\n\n### Configuration knobs affecting PythonRunner behavior\n\nMost are defined/consumed in BasePythonRunner but impact PythonRunner subclasses:\n- spark.buffer.size (BUFFER_SIZE): I/O buffer size for streams to workers.\n- spark.python.worker.reuse (PYTHON_WORKER_REUSE): reuse workers across tasks.\n- spark.python.auth.socket.timeout (PYTHON_AUTH_SOCKET_TIMEOUT): auth handshake timeout.\n- spark.python.worker.faulthandler.enabled (PYTHON_WORKER_FAULTHANLDER_ENABLED): whether to dump Python faulthandler logs.\n- Resource profile related local properties: executor cores and pyspark memory hints.\n\n\n## 3) Methods and places in Spark that use org.apache.spark.api.python.PythonRunner\n\nDirect constructions or usages of the object/class:\n\n- core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala\n  - val runner = PythonRunner(func)\n    - Constructs a PythonRunner via object.apply, wrapping a PythonFunction to run over an RDD partition.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonForeachWriter.scala\n  - private lazy val pythonRunner = PythonRunner(func)\n  - pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n    - Uses PythonRunner to compute side-effecting foreachBatch/foreach-like operations with serialized bytes.\n\nNote: Other SQL execution runners (ArrowPythonRunner, CoGroupedArrowPythonRunner, PythonUDFRunner) extend BasePythonRunner directly for Arrow or UDF-specific protocols and do not construct org.apache.spark.api.python.PythonRunner.\n\n\n## 4) Methods and places in Spark that use org.apache.spark.deploy.PythonRunner\n\n- core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\n  - Sets main class to org.apache.spark.deploy.PythonRunner for Python apps\n  - Formats/normalizes pyFiles via PythonRunner.formatPaths when constructing child args and options.\n\n- core/src/main/scala/org/apache/spark/deploy/RRunner.scala\n  - Uses PythonRunner.formatPath to normalize the primary R file path.\n\n- Tests referencing behavior\n  - core/src/test/scala/org/apache/spark/deploy/PythonRunnerSuite.scala covers formatPath and formatPaths edge cases, including Windows drive letters and malformed paths.\n  - core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala validates formatting and argument propagation using PythonRunner.formatPaths.\n\n\n## 5) Summary of the execution flow\n\n- Submission time (driver): SparkSubmit launches org.apache.spark.deploy.PythonRunner main(), which:\n  1. Determines Python executable.\n  2. Normalizes python script path and pyFiles; ensures .py files are placed in a temp dir for PYTHONPATH.\n  3. Starts Py4J gateway and exports connection info as environment variables.\n  4. Builds PYTHONPATH and launches the Python process with combined environment.\n  5. Redirects and mirrors Python stdout/stderr, propagating non-zero exit code as SparkUserAppException.\n\n- Executor time (task): RDD/SQL components create a org.apache.spark.api.python.PythonRunner (or Arrow/UDR-specific BasePythonRunner subclass), which:\n  1. Creates a writer thread to send command + input iterator to the worker process.\n  2. Iterates a reader that decodes results and exceptions via a binary protocol.\n  3. Manages worker lifecycle and monitors for deadlocks, timing data, and error propagation.\n\n\n## 6) Quick reference of key signatures\n\n- deploy.PythonRunner\n  - def main(args: Array[String]): Unit\n  - def formatPath(path: String, testWindows: Boolean = false): String\n  - def formatPaths(paths: String, testWindows: Boolean = false): Array[String]\n\n- api.python.PythonRunner object\n  - def apply(func: PythonFunction): PythonRunner\n\n- api.python.PythonRunner class\n  - new PythonRunner(funcs: Seq[ChainedPythonFunctions])\n  - Overrides\n    - def newWriterThread(...): WriterThread\n    - def newReaderIterator(...): Iterator[Array[Byte]]\n\n\n## 7) Code references index (by file)\n\n- core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala  implementation of deploy.PythonRunner\n- core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala  implementation of BasePythonRunner, api.python.PythonRunner object and class\n- core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala  constructs PythonRunner(func)\n- sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonForeachWriter.scala  constructs and uses PythonRunner(func)\n- core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala  refers to deploy.PythonRunner and calls formatPaths\n- core/src/main/scala/org/apache/spark/deploy/RRunner.scala  calls deploy.PythonRunner.formatPath\n- Tests: PythonRunnerSuite.scala, SparkSubmitSuite.scala (under core/src/test/scala/...)\n\n\n## 8) Edge cases and caveats\n\n- Path normalization rejects non-local schemes for deploy.PythonRunner; only file/local paths are supported at submission time.\n- On Windows, ensure drive-letter paths are not prefixed with a slash for Python to accept them.\n- .py files should not be placed directly on PYTHONPATH; the parent directory must be used. Spark handles this by copying .py files to a temp dir when building PYTHONPATH.\n- Worker reuse and faulthandler settings can materially affect stability and diagnostics; ensure configs are set appropriately for your environment.\n- The api.python.PythonRunner class uses raw byte records; schema-aware/Arrow-based processors use specialized runners extending BasePythonRunner rather than this class.\n",
          "answer_time": 90,
          "tokens": 0,
          "more_info": "Added `return full answer in .md file` at the end of question"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q003",
      "question": "What could be improved in class KafkaWrite?",
      "category": "definition",
      "ground_truth_contexts": [
        "case class KafkaWrite( topic: Option[String], producerParams: ju.Map[String, Object], schema: StructType) extends Write { override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
        "override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
        "override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
        "override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }"
      ],
      "key_entities": [
        "org/apache/spark/sql/kafka010/KafkaWrite#",
        "org/apache/spark/sql/kafka010/KafkaWrite#toBatch().",
        "org/apache/spark/sql/kafka010/KafkaWrite#toStreaming().",
        "org/apache/spark/sql/kafka010/KafkaWrite#description()."
      ],
      "claude_stats": {
        "answer": "Based on the code analysis of the `KafkaWrite` class, here are several potential improvements:\n\n**1. Schema Validation Timing**\nThe `schema` is validated using `assert` statements in both `toBatch` and `toStreaming` methods. This is problematic because assertions can be disabled at runtime with the `-da` flag. It would be better to validate the schema in the constructor or use `require()` which always throws an exception.\n\n```scala\ncase class KafkaWrite(\n  topic: Option[String],\n  producerParams: ju.Map[String, Object],\n  schema: StructType\n) extends Write {\n  require(schema != null, \"Schema cannot be null\")\n  // ...\n}\n```\n\n**2. Immutability of Producer Parameters**\nThe `producerParams` is a mutable Java Map (`ju.Map[String, Object]`). This could lead to unexpected behavior if the map is modified externally after the `KafkaWrite` instance is created. Consider creating a defensive copy or converting it to an immutable Scala collection.\n\n**3. Description Method**\nThe `description()` method returns a generic \"Kafka\" string. This could be more informative by including details like the topic name (if specified) or other configuration details to aid in debugging and monitoring.\n\n**4. Code Duplication**\nBoth `toBatch` and `toStreaming` methods have identical validation logic and pass the same parameters to their respective write implementations. This duplication could be reduced.\n\n**5. Type Safety**\nThe `producerParams` uses `Object` as the value type, which sacrifices type safety. While this might be necessary for compatibility with Kafka's producer configuration, it could benefit from additional validation.\n\n",
        "prompt_time": 1.466,
        "answer_time": 20,
        "tokens": 0,
        "context_tokens": 324,
        "used_context": [
          "## CLASS: org/apache/spark/sql/kafka010/KafkaWrite#\ncase class KafkaWrite( topic: Option[String], producerParams: ju.Map[String, Object], schema: StructType) extends Write { override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
          "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toBatch().\noverride def toBatch: BatchWrite;\noverride def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
          "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toStreaming().\noverride def toStreaming: StreamingWrite;\noverride def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
          "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#description().\noverride def description(): String;\noverride def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "Improvements to KafkaWrite include better validation, immutability, and code clarity. A minimal refactor example was provided. External API and behavior remain unchanged.",
          "prompt_time": 0.131,
          "answer_time": 136,
          "context_tokens": 238,
          "used_context": [
            "## CLASS: org/apache/spark/sql/kafka010/KafkaWrite#\ncase class KafkaWrite( topic: Option[String], producerParams: ju.Map[String, Object], schema: StructType) extends Write { override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
            "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toBatch().\noverride def toBatch: BatchWrite;\noverride def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
            "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toStreaming().\noverride def toStreaming: StreamingWrite;\noverride def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }"
          ],
          "tokens": 0,
          "more_info": "Used same query 2 times and very poor answer, edited KafkaWrite.scala on it's own after being asked for full solution"
        },
        "without_mcp": {
          "answer": "# KafkaWrite class  Review and Improvement Suggestions\n\nThis document reviews the current implementation of `org.apache.spark.sql.kafka010.KafkaWrite` and proposes pragmatic, lowrisk improvements. The intent is to enhance correctness, safety, observability, and maintainability without disrupting Sparks DataSource V2 Kafka connector design.\n\nFile under review:\n- external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaWrite.scala\n\nCurrent (abridged) implementation characteristics:\n- Case class with three fields: `topic: Option[String]`, `producerParams: ju.Map[String, Object]`, `schema: StructType` implementing `Write`.\n- Eager parameter validation via `require(...)`.\n- Defensive copy and unmodifiable view around `producerParams`.\n- `description()` returns `Kafka(topic=...)` if topic is specified, else `Kafka`.\n- `toBatch` and `toStreaming` delegate to `KafkaBatchWrite` and `KafkaStreamingWrite` respectively.\n\nOverall, the class is lean and already applies good practices (null checks and defensive copy). The following suggestions are incremental and focus on edge cases and longterm maintainability.\n\n---\n\n## 1) Validation completeness and error messaging\n\n- Topic validation\n  - Today: `require(topic.forall(_.nonEmpty))` prevents empty string but allows whitespace and otherwise invalid topic names.\n  - Improve by trimming and rejecting blank strings: `_.trim.nonEmpty`.\n  - Optionally align with Kafkas topic name constraints (length 1249, allowed characters `[a-zA-Z0-9._-]`, no `..` or `.` only, etc.), with clear error messages. Keep it optin if strict enforcement risks compatibility.\n\n- Producer parameters validation\n  - Validate presence of essential config such as `bootstrap.servers` early; fail fast with actionable message.\n  - Optionally warn or reject unsupported keys or deprecated aliases (if there is a central validator in `KafkaSourceProvider`/`KafkaWriter`). Centralize validation so the same rules apply across batch and streaming.\n\n- Schema validation messaging\n  - The current `require(schema != null)` is correct; consider tailoring the message if downstream expects specific fields (e.g., value/key/topic/partition) to reduce roundtrip debugging. Actual field validation likely happens later, but a short hint improves UX.\n\n- Consistency of exceptions\n  - Use `IllegalArgumentException` (produced by `require`) consistently for precondition violations. Ensure messages include the offending input value when safe.\n\n## 2) Immutability and exposure of state\n\n- Defensive copy is in place; this is great.\n- Consider masking sensitive producer parameters in any logging or description (e.g., `sasl.jaas.config`, `ssl.key.password`, `sasl.password`). This is more relevant where parameters are logged, but adding a small helper here (or in a shared util) avoids accidental leakage when building descriptions or debugging.\n\n## 3) Description/observability improvements\n\n- `description()` currently hides whether batch or streaming is intended; thats fine because `Write` is a neutral interface. However, adding topic trimming in description and guarding against long topic values can help readability.\n- Consider adding a compact, redacted snapshot of relevant, nonsensitive producer settings in debug logs of the builders (`KafkaBatchWrite`/`KafkaStreamingWrite`). Keep `KafkaWrite` description concise.\n\n## 4) API surface and type choices\n\n- Java map in Scala API\n  - `producerParams: ju.Map[String, Object]` mirrors Kafka client expectations, which is reasonable, but using a Scala `Map[String, AnyRef]` at the boundary and converting to Java internally would:\n    - Improve type safety on the Scala side.\n    - Reduce accidental mutation downstream.\n  - This change has compatibility implications (binary/source). If changing the constructor is not feasible, consider adding an auxiliary constructor or a companion `apply`/builder that accepts a Scala map.\n\n- Final/sealed concerns\n  - Case classes in Scala 2 are not `final` by default; given this is a simple value holder, extension is not expected. Marking the class `final` could prevent accidental subclassing, but doing so can break binary compatibility. Weigh this against Sparks compatibility policy before changing.\n\n## 5) Serialization and stability considerations\n\n- While `Write` is typically used on the driver to produce permode write plans, some objects can be serialized across JVM boundaries. Ensure everything remains Javaserializable or Kryofriendly. Case classes are serializable, `StructType` is serializable, and `java.util.Map` is serializable if its values are; current usage is likely safe.\n- Avoid capturing large or nonserializable state in future extensions.\n\n## 6) Documentation improvements\n\n- Add Scaladoc explaining:\n  - What `KafkaWrite` represents in the DataSource V2 lifecycle.\n  - The meaning and expected contents of `producerParams` (with link to Kafka Producer configs), and treatment of sensitive configs.\n  - How `topic` interacts with perrow topic expressions (i.e., if both are present, the row value overrides or is combineddocument actual behavior to reduce confusion).\n\n## 7) Testing enhancements\n\n- Unit tests around construction edge cases:\n  - `topic = Some(\"\")` (reject).\n  - `topic = Some(\"  \")` (if trim check is added, reject).\n  - `topic = None` with and without topic in rows (integration tested in writer).\n  - Missing `bootstrap.servers` (if validation is added).\n  - Producer params immutability: external mutations do not affect internal `params`.\n\n- Property test for redaction (when logging helpers are added): ensure sensitive keys are masked.\n\n## 8) Defensive programming against accidental mutation\n\n- Although `params` is an unmodifiable map wrapper, note that values in the map can still be mutable (e.g., `List`, arrays). If Spark or Kafka code never mutates values, this is fine. Otherwise, consider deep copying for known mutable value types when they appear (rarely necessary for Kafka configs).\n\n## 9) Backward/forward compatibility strategy\n\n- Any stricter validation (topic/required keys) should be guarded by compatibility considerations:\n  - Start with warnings, promote to errors in a major version, or gate behind a configuration flag (e.g., `spark.sql.kafka.enforceStrictTopicValidation`).\n  - Document deprecations and migration paths.\n\n## 10) Small code quality nits\n\n- Use `Objects.requireNonNull` equivalent? Current `require` is idiomatic Scala and fine.\n- Minor microallocation: `description()` creates a `String`; its trivial and not performance sensitive.\n- Keep the constructor lightweight to avoid overhead when planning; heavy validation of configs can be deferred to the first use point if needed for performance.\n\n---\n\n## Suggested minimal, lowrisk patch (illustrative)\n\n- Trim topic and improve message:\n\n```scala\nrequire(topic != null, \"topic Option must not be null; use None when absent\")\nrequire(topic.forall(t => t != null && t.trim.nonEmpty),\n  s\"topic, if provided, must be non-blank\")\n```\n\n- Validate presence of `bootstrap.servers` early (optional):\n\n```scala\nprivate def hasBootstrap(params: ju.Map[String, Object]): Boolean =\n  params.containsKey(\"bootstrap.servers\") &&\n    Option(params.get(\"bootstrap.servers\")).exists(_.toString.trim.nonEmpty)\n\nrequire(hasBootstrap(producerParams),\n  \"Kafka producer parameter 'bootstrap.servers' must be defined and non-empty\")\n```\n\n- Add Scaladoc to the class explaining role and parameters.\n\nThese can be adopted incrementally; full stricter Kafka topic checks and redaction utilities can be added in shared Kafka utils to keep `KafkaWrite` minimal.\n\n---\n\n## Conclusion\n\n`KafkaWrite` is already concise and robust for its role. The improvements above mainly enhance validation clarity, reduce footguns around whitespace topics and missing essentials, improve documentation and test coverage, and guard against accidental leakage of sensitive configs in logs. Changes should be weighed against Sparks compatibility guarantees; starting with documentation and tests plus nonbreaking validations is recommended.\n",
          "answer_time": 58,
          "tokens": 0
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q004",
      "question": "What are 5 classes with most lines of code?",
      "category": "top",
      "ground_truth_contexts": [
        "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
      ],
      "key_entities": [
        "functions",
        "astbuilder",
        "dataset",
        "sqlconf",
        "analyzer"
      ],
      "claude_stats": {
        "answer": "Here are the 5 classes with the most lines of code:\n\n1. **astbuilder** - 4,408 lines\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala`\n\n2. **dataset** - 3,683 lines\n   - Location: `sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala`\n\n3. **analyzer** - 3,483 lines\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala`\n\n4. **sparkcontext** - 2,534 lines\n   - Location: `core/src/main/scala/org/apache/spark/SparkContext.scala`\n\n5. **dagscheduler** - 2,448 lines\n   - Location: `core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala`\n\nThese classes are among the largest in the codebase, with `astbuilder` being the largest at over 4,400 lines of code. This suggests they handle complex functionality - `astbuilder` likely deals with parsing and building abstract syntax trees, `dataset` provides the main Dataset API, and `sparkcontext` serves as the entry point for Spark functionality.\n\n",
        "prompt_time": 0.649,
        "answer_time": 10,
        "tokens": 0,
        "context_tokens": 154,
        "used_context": [
          "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# Top 5 Classes by Lines of Code (LOC)\n\nThis document records the top 5 classes in Apache Spark 3.3.0 with the most lines of code, as identified during analysis on 2025-12-27 10:50 (local time).\n\nSource project root: projects/spark-3.3.0\n\n## Results\n\n1. astbuilder  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala (LOC: 4408)\n2. dataset  sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala (LOC: 3683)\n3. analyzer  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala (LOC: 3483)\n4. sparkcontext  core/src/main/scala/org/apache/spark/SparkContext.scala (LOC: 2534)\n5. dagscheduler  core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala (LOC: 2448)\n\n## Notes\n\n- These classes rank highest by sheer lines of code, indicating central components:\n  - Catalyst SQL parsing (AstBuilder) and analysis (Analyzer)\n  - Dataset API surface (Dataset)\n  - Core scheduling and context management (DAGScheduler, SparkContext)\n- If deeper analysis is needed, consider starting with AstBuilder to summarize responsibilities, complexity hotspots, and potential refactoring candidates.\n\n.",
          "prompt_time": 0.577,
          "answer_time": 100,
          "context_tokens": 238,
          "used_context": [
            "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
          ],
          "tokens": 0,
          "more_info": "After asking for full answer returned better list in .md file with all the information returned by Server MCP"
        },
        "without_mcp": {
          "answer": "",
          "answer_time": 700,
          "tokens": 0,
          "more_info": "Timeout on powershell script, no answer"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q005",
      "question": "Describe 3 most important classes in project.",
      "category": "top",
      "ground_truth_contexts": [
        "*/ abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent */ def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition. */ @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }` */ protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it. */ protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences. */ protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned. */ @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD. */ def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext). */ val id: Int = sc.newRddId() /** A friendly name for this RDD */ @transient var name: String = _ /** Assign a name to this RDD */ def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one */ private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception. */ def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD. */ def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */ def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver). */ private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed */ private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing. */ final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD. */ @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed. */ final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD. */ final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned. */ private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing. */ private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached. */ private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body. */ private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate. */ def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable. */ def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition. */ val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]]. */ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array */ def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement. */ private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function. */ def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. */ def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD */ def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD */ def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array. */ def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */ def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD */ def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning. */ private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). */ def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. */ def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD. */ def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD. */ def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first. */ def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`. */ def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator. */ def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]] */ def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions */ def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions */ def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2) */ def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor */ def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD. */ def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map. */ def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped. */ def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. */ def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`. */ def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD. */ def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD * */ def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD * */ def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. */ def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements. */ def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements. */ def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects. */ def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`. */ def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition */ private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. */ def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used. */ def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. */ def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing. */ private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing. */ private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized. */ private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally. */ def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned */ @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents. */ def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a> */ @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD. */ @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified */ @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`). */ @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations. */ @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD */ protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T] */ protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on. */ def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs. */ private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions. */ private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example. */ protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging. */ def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier. */ private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level. */ // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`. */ object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE. */ private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
        "@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema). */ private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. */ private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). */ private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0 */ // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0 */ def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0 */ @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0 */ def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0 */ def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0 */ def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0 */ def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0 */ def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0 */ def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0 */ def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0 */ def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0 */ def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0 */ def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0 */ def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem */ private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0 */ // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0 */ def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0 */ def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0 */ def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0 */ // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0 */ def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0 */ def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0 */ def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0 */ @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0 */ def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0 */ def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0 */ def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0 */ def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface. */ protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0 */ def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0 */ @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0 */ @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0 */ def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0 */ def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0 */ def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0 */ def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0 */ def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0 */ def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0 */ def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. */ private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0 */ def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names. */ private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata. */ private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata. */ private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0 */ def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0 */ def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0 */ def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0 */ def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0 */ @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0 */ @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0 */ def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0 */ def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0 */ def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0 */ def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition. */ private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers. */ private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches. */ private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0 */ def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0 */ def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0 */ def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0 */ def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0 */ def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0 */ def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0 */ def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0 */ def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0 */ def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0 */ def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0 */ def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0 */ def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0 */ lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0 */ @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0 */ @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0 */ def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0 */ @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0 */ def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0 */ def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0 */ def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0 */ def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0 */ def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0 */ def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0 */ @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0 */ @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark. */ private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD. */ private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`. */ private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side. */ private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution. */ private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset. */ private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception. */ private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan. */ private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame. */ @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset. */ @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset. */ @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches. */ private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
        "@Stable class Column( val name: String, @Nullable val description: String, val dataType: String, val nullable: Boolean, val isPartition: Boolean, val isBucket: Boolean) extends DefinedByConstructorParams { override def toString: String = { \"Column[\" + s\"name='$name', \" + Option(description).map { d => s\"description='$d', \" }.getOrElse(\"\") + s\"dataType='$dataType', \" + s\"nullable='$nullable', \" + s\"isPartition='$isPartition', \" + s\"isBucket='$isBucket']\" } } /** * A user-defined function in Spark, as returned by `listFunctions` method in [[Catalog]]. * * @param name name of the function. * @param database name of the database the function belongs to. * @param description description of the function; description can be null. * @param className the fully qualified class name of the function. * @param isTemporary whether the function is a temporary function or not. * @since 2.0.0 */ @Stable class Function( val name: String, @Nullable val database: String, @Nullable val description: String, val className: String, val isTemporary: Boolean) extends DefinedByConstructorParams { override def toString: String = { \"Function[\" + s\"name='$name', \" + Option(database).map { d => s\"database='$d', \" }.getOrElse(\"\") + Option(description).map { d => s\"description='$d', \" }.getOrElse(\"\") + s\"className='$className', \" + s\"isTemporary='$isTemporary']\" } }"
      ],
      "key_entities": [
        "org/apache/spark/rdd/RDD#",
        "org/apache/spark/sql/Dataset#",
        "org/apache/spark/sql/catalog/Column#"
      ],
      "claude_stats": {
        "answer": "Based on the information retrieved, here are the **3 most important classes** in the project:\n\n## 1. **RDD** (Resilient Distributed Dataset)\n**Location:** `org/apache/spark/rdd/RDD#`\n\nRDD is the fundamental data structure in Spark. It represents an immutable, distributed collection of objects that can be processed in parallel. Key features include:\n\n- **Fault tolerance** through lineage tracking - if a partition is lost, it can be recomputed from the original data\n- **Lazy evaluation** - transformations are not computed until an action is called\n- **In-memory computing** - data can be cached in memory for faster iterative processing\n- **Partitioning** - data is automatically distributed across the cluster\n\nThe class provides numerous transformation operations (`map`, `filter`, `flatMap`, `groupBy`, etc.) and action operations (`collect`, `count`, `reduce`, `take`, etc.). It's the core abstraction that makes Spark's distributed computing model work.\n\n## 2. **Dataset**\n**Location:** `org/apache/spark/sql/Dataset#`\n\nDataset is Spark's primary structured data API, providing a strongly-typed, object-oriented interface for working with structured and semi-structured data. Key features include:\n\n- **Type safety** - compile-time type checking with encoders\n- **Optimized execution** - uses Catalyst query optimizer and Tungsten execution engine\n- **SQL integration** - can be queried using SQL or DataFrame API\n- **Unified batch and streaming** - same API works for both processing modes\n- **Rich transformations** - operations like `select`, `filter`, `join`, `groupBy`, `agg`\n\nDataset builds on top of RDD but provides higher-level abstractions and automatic optimization. When working with Row objects, it's called a DataFrame (which is an alias for `Dataset[Row]`).\n\n## 3. **SparkContext**\n**Location:** `org/apache/spark/SparkContext#`\n\nSparkContext is the main entry point for Spark functionality and represents the connection to a Spark cluster. Key responsibilities include:\n\n- **Cluster connection** - establishes connection to the cluster manager (YARN, Mesos, Kubernetes, or standalone)\n- **RDD creation** - provides methods to create RDDs from data sources (`parallelize`, `textFile`, `hadoopFile`)\n- **Resource management** - manages executors, memory, and scheduling\n- **Job execution** - coordinates job submission and task distribution\n- **Configuration** - holds Spark configuration and environment settings\n- **Broadcasting and accumulators** - provides mechanisms for sharing data and collecting metrics\n\nOnly one SparkContext can be active per JVM, and it must be stopped properly to release cluster resources. It's the foundation that enables all distributed computation in Spark.\n\n---\n\nThese three classes form the core of Apache Spark's architecture: **SparkContext** manages the execution environment, **RDD** provides the low-level distributed data abstraction, and **Dataset** offers the high-level structured data API with query optimization.",
        "prompt_time": 1.335,
        "answer_time": 28,
        "tokens": 0,
        "context_tokens": 74560,
        "used_context": [
          "## CLASS: org/apache/spark/rdd/RDD#\n abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent  def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition.  @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`  protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it.  protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences.  protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned.  @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD.  def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext).  val id: Int = sc.newRddId() /** A friendly name for this RDD  @transient var name: String = _ /** Assign a name to this RDD  def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception.  def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD.  def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set.  def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver).  private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not.  final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing.  final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not.  final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD.  @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed.  final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD.  final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned.  private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD.  def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results.  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate.  def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable.  def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition.  val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]].  def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array  def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement.  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function.  def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally.  def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD  def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array.  def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`.  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD  def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning.  private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other).  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition.  def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD.  def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD.  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first.  def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`.  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us.  def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator.  def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]]  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions  def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2)  def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor  def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD.  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map.  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped.  def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017.  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`.  def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD.  def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD *  def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD *  def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition.  def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements.  def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements.  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects.  def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`.  def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition  private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.  def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.  def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally.  def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing.  private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing.  private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized.  private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally.  def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned  @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents.  def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a>  @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD.  @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified  @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`).  @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations.  @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD  protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T]  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on.  def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs.  private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions.  private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.  protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging.  def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier.  private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level.  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.  object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.  private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
          "## CLASS: org/apache/spark/sql/Dataset#\n@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema).  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right.  private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value).  private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0  // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0  @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0  def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0  def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0  def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0  def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0  def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0  def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0  def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0  def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0  def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0  def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0  // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0  def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0  def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0  // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0  def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0  @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0  def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0  def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0  def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface.  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0  @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0  @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0  def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0  def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0  def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0  def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0  def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling.  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names.  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata.  private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata.  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0  def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0  def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0  def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0  def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0  @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0  @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0  def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0  def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0  def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition.  private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers.  private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches.  private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0  def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0  def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0  def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0  def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0  def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0  def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0  def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0  def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0  def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0  lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0  @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0  @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0  def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0  @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0  def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0  def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0  def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0  def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0  def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0  @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0  @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark.  private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD.  private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`.  private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side.  private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution.  private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset.  private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception.  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan.  private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame.  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset.  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset.  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches.  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
          "## CLASS: org/apache/spark/SparkContext#\n* this config overrides the default configs as well as system properties.  class SparkContext(config: SparkConf) extends Logging { // The call site where this SparkContext was constructed. private val creationSite: CallSite = Utils.getCallSite() if (!config.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { // In order to prevent SparkContext from being created in executors. SparkContext.assertOnDriver() } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having started construction. // NOTE: this must be placed at the beginning of the SparkContext constructor. SparkContext.markPartiallyConstructed(this) val startTime = System.currentTimeMillis() private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false) private[spark] def assertNotStopped(): Unit = { if (stopped.get()) { val activeContext = SparkContext.activeContext.get() val activeCreationSite = if (activeContext == null) { \"(No active SparkContext.)\" } else { activeContext.creationSite.longForm } throw new IllegalStateException( s\"\"\"Cannot call methods on a stopped SparkContext. |This stopped SparkContext was created at: | |${creationSite.longForm} | |The currently active SparkContext was created at: | |$activeCreationSite \"\"\".stripMargin) } } /** * Create a SparkContext that loads settings from system properties (for instance, when * launching with ./bin/spark-submit).  def this() = this(new SparkConf()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters  def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(conf, master, appName)) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs. * @param environment Environment variables to set on worker nodes.  def this( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment)) } // The following constructors are required when Java code accesses SparkContext directly. // Please see SI-4278 /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI.  private[spark] def this(master: String, appName: String) = this(master, appName, null, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes.  private[spark] def this(master: String, appName: String, sparkHome: String) = this(master, appName, sparkHome, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs.  private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) = this(master, appName, sparkHome, jars, Map()) // log out Spark Version in Spark driver log logInfo(s\"Running Spark version $SPARK_VERSION\") /* ------------------------------------------------------------------------------------- * | Private variables. These variables keep the internal state of the context, and are | | not accessible by the outside world. They're mutable since we want to initialize all | | of them to some neutral value ahead of time, so that calling \"stop()\" while the | | constructor is still running is safe. | * -------------------------------------------------------------------------------------  private var _conf: SparkConf = _ private var _eventLogDir: Option[URI] = None private var _eventLogCodec: Option[String] = None private var _listenerBus: LiveListenerBus = _ private var _env: SparkEnv = _ private var _statusTracker: SparkStatusTracker = _ private var _progressBar: Option[ConsoleProgressBar] = None private var _ui: Option[SparkUI] = None private var _hadoopConfiguration: Configuration = _ private var _executorMemory: Int = _ private var _schedulerBackend: SchedulerBackend = _ private var _taskScheduler: TaskScheduler = _ private var _heartbeatReceiver: RpcEndpointRef = _ @volatile private var _dagScheduler: DAGScheduler = _ private var _applicationId: String = _ private var _applicationAttemptId: Option[String] = None private var _eventLogger: Option[EventLoggingListener] = None private var _driverLogger: Option[DriverLogger] = None private var _executorAllocationManager: Option[ExecutorAllocationManager] = None private var _cleaner: Option[ContextCleaner] = None private var _listenerBusStarted: Boolean = false private var _jars: Seq[String] = _ private var _files: Seq[String] = _ private var _archives: Seq[String] = _ private var _shutdownHookRef: AnyRef = _ private var _statusStore: AppStatusStore = _ private var _heartbeater: Heartbeater = _ private var _resources: immutable.Map[String, ResourceInformation] = _ private var _shuffleDriverComponents: ShuffleDriverComponents = _ private var _plugins: Option[PluginContainer] = None private var _resourceProfileManager: ResourceProfileManager = _ /* ------------------------------------------------------------------------------------- * | Accessors and public fields. These provide access to the internal state of the | | context. | * -------------------------------------------------------------------------------------  private[spark] def conf: SparkConf = _conf /** * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be * changed at runtime.  def getConf: SparkConf = conf.clone() def resources: Map[String, ResourceInformation] = _resources def jars: Seq[String] = _jars def files: Seq[String] = _files def archives: Seq[String] = _archives def master: String = _conf.get(\"spark.master\") def deployMode: String = _conf.get(SUBMIT_DEPLOY_MODE) def appName: String = _conf.get(\"spark.app.name\") private[spark] def isEventLogEnabled: Boolean = _conf.get(EVENT_LOG_ENABLED) private[spark] def eventLogDir: Option[URI] = _eventLogDir private[spark] def eventLogCodec: Option[String] = _eventLogCodec def isLocal: Boolean = Utils.isLocalMaster(_conf) /** * @return true if context is stopped or in the midst of stopping.  def isStopped: Boolean = stopped.get() private[spark] def statusStore: AppStatusStore = _statusStore // An asynchronous listener bus for Spark events private[spark] def listenerBus: LiveListenerBus = _listenerBus // This function allows components created by SparkEnv to be mocked in unit tests: private[spark] def createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv = { SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf)) } private[spark] def env: SparkEnv = _env // Used to store a URL for each static file/jar together with the file's local timestamp private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedArchives = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala // Keeps track of all persisted RDDs private[spark] val persistentRdds = { val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]() map.asScala } def statusTracker: SparkStatusTracker = _statusTracker private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar private[spark] def ui: Option[SparkUI] = _ui def uiWebUrl: Option[String] = _ui.map(_.webUrl) /** * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse. * * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you * plan to set some global configurations for all Hadoop RDDs.  def hadoopConfiguration: Configuration = _hadoopConfiguration private[spark] def executorMemory: Int = _executorMemory // Environment variables to pass to our executors. private[spark] val executorEnvs = HashMap[String, String]() // Set SPARK_USER for user who is running SparkContext. val sparkUser = Utils.getCurrentUserName() private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend private[spark] def taskScheduler: TaskScheduler = _taskScheduler private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = { _taskScheduler = ts } private[spark] def dagScheduler: DAGScheduler = _dagScheduler private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = { _dagScheduler = ds } private[spark] def shuffleDriverComponents: ShuffleDriverComponents = _shuffleDriverComponents /** * A unique identifier for the Spark application. * Its format depends on the scheduler implementation. * (i.e. * in case of local spark app something like 'local-1433865536131' * in case of YARN something like 'application_1433865536131_34483' * in case of MESOS something like 'driver-20170926223339-0001' * )  def applicationId: String = _applicationId def applicationAttemptId: Option[String] = _applicationAttemptId private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] = _executorAllocationManager private[spark] def resourceProfileManager: ResourceProfileManager = _resourceProfileManager private[spark] def cleaner: Option[ContextCleaner] = _cleaner private[spark] var checkpointDir: Option[String] = None // Thread Local variable that can be used by users to pass information down the stack protected[spark] val localProperties = new InheritableThreadLocal[Properties] { override def childValue(parent: Properties): Properties = { // Note: make a clone such that changes in the parent properties aren't reflected in // the those of the children threads, which has confusing semantics (SPARK-10563). Utils.cloneProperties(parent) } override protected def initialValue(): Properties = new Properties() } /* ------------------------------------------------------------------------------------- * | Initialization. This code initializes the context in a manner that is exception-safe. | | All internal fields holding state are initialized here, and any error prompts the | | stop() method to be called. | * -------------------------------------------------------------------------------------  private def warnSparkMem(value: String): String = { logWarning(\"Using SPARK_MEM to set amount of memory to use per executor process is \" + \"deprecated, please use spark.executor.memory instead.\") value } /** Control our logLevel. This overrides any user-defined log settings. * @param logLevel The desired log level as a string. * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN  def setLogLevel(logLevel: String): Unit = { // let's allow lowercase or mixed case too val upperCased = logLevel.toUpperCase(Locale.ROOT) require(SparkContext.VALID_LOG_LEVELS.contains(upperCased), s\"Supplied level $logLevel did not match one of:\" + s\" ${SparkContext.VALID_LOG_LEVELS.mkString(\",\")}\") Utils.setLogLevel(Level.toLevel(upperCased)) } try { _conf = config.clone() _conf.validateSettings() _conf.set(\"spark.app.startTime\", startTime.toString) if (!_conf.contains(\"spark.master\")) { throw new SparkException(\"A master URL must be set in your configuration\") } if (!_conf.contains(\"spark.app.name\")) { throw new SparkException(\"An application name must be set in your configuration\") } // This should be set as early as possible. SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf) SparkContext.supplementJavaModuleOptions(_conf) _driverLogger = DriverLogger(_conf) val resourcesFileOpt = conf.get(DRIVER_RESOURCES_FILE) _resources = getOrDiscoverAllResources(_conf, SPARK_DRIVER_PREFIX, resourcesFileOpt) logResourceInfo(SPARK_DRIVER_PREFIX, _resources) // log out spark.app.name in the Spark driver logs logInfo(s\"Submitted application: $appName\") // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster if (master == \"yarn\" && deployMode == \"cluster\" && !_conf.contains(\"spark.yarn.app.id\")) { throw new SparkException(\"Detected yarn cluster mode, but isn't running on a cluster. \" + \"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.\") } if (_conf.getBoolean(\"spark.logConf\", false)) { logInfo(\"Spark configuration:\\n\" + _conf.toDebugString) } // Set Spark driver host and port system properties. This explicitly sets the configuration // instead of relying on the default value of the config constant. _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS)) _conf.setIfMissing(DRIVER_PORT, 0) _conf.set(EXECUTOR_ID, SparkContext.DRIVER_IDENTIFIER) _jars = Utils.getUserJars(_conf) _files = _conf.getOption(FILES.key).map(_.split(\",\")).map(_.filter(_.nonEmpty)) .toSeq.flatten _archives = _conf.getOption(ARCHIVES.key).map(Utils.stringToSeq).toSeq.flatten _eventLogDir = if (isEventLogEnabled) { val unresolvedDir = conf.get(EVENT_LOG_DIR).stripSuffix(\"/\") Some(Utils.resolveURI(unresolvedDir)) } else { None } _eventLogCodec = { val compress = _conf.get(EVENT_LOG_COMPRESS) if (compress && isEventLogEnabled) { Some(_conf.get(EVENT_LOG_COMPRESSION_CODEC)).map(CompressionCodec.getShortName) } else { None } } _listenerBus = new LiveListenerBus(_conf) _resourceProfileManager = new ResourceProfileManager(_conf, _listenerBus) // Initialize the app status store and listener before SparkEnv is created so that it gets // all events. val appStatusSource = AppStatusSource.createSource(conf) _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource) listenerBus.addToStatusQueue(_statusStore.listener.get) // Create the Spark execution environment (cache, map output tracker, etc) _env = createSparkEnv(_conf, isLocal, listenerBus) SparkEnv.set(_env) // If running the REPL, register the repl's output dir with the file server. _conf.getOption(\"spark.repl.class.outputDir\").foreach { path => val replUri = _env.rpcEnv.fileServer.addDirectory(\"/classes\", new File(path)) _conf.set(\"spark.repl.class.uri\", replUri) } _statusTracker = new SparkStatusTracker(this, _statusStore) _progressBar = if (_conf.get(UI_SHOW_CONSOLE_PROGRESS)) { Some(new ConsoleProgressBar(this)) } else { None } _ui = if (conf.get(UI_ENABLED)) { Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, \"\", startTime)) } else { // For tests, do not enable the UI None } // Bind the UI before starting the task scheduler to communicate // the bound port to the cluster manager properly _ui.foreach(_.bind()) _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf) // Performance optimization: this dummy call to .size() triggers eager evaluation of // Configuration's internal `properties` field, guaranteeing that it will be computed and // cached before SessionState.newHadoopConf() uses `sc.hadoopConfiguration` to create // a new per-session Configuration. If `properties` has not been computed by that time // then each newly-created Configuration will perform its own expensive IO and XML // parsing to load configuration defaults and populate its own properties. By ensuring // that we've pre-computed the parent's properties, the child Configuration will simply // clone the parent's properties. _hadoopConfiguration.size() // Add each JAR given through the constructor if (jars != null) { jars.foreach(jar => addJar(jar, true)) if (addedJars.nonEmpty) { _conf.set(\"spark.app.initial.jar.urls\", addedJars.keys.toSeq.mkString(\",\")) } } if (files != null) { files.foreach(file => addFile(file, false, true)) if (addedFiles.nonEmpty) { _conf.set(\"spark.app.initial.file.urls\", addedFiles.keys.toSeq.mkString(\",\")) } } if (archives != null) { archives.foreach(file => addFile(file, false, true, isArchive = true)) if (addedArchives.nonEmpty) { _conf.set(\"spark.app.initial.archive.urls\", addedArchives.keys.toSeq.mkString(\",\")) } } _executorMemory = _conf.getOption(EXECUTOR_MEMORY.key) .orElse(Option(System.getenv(\"SPARK_EXECUTOR_MEMORY\"))) .orElse(Option(System.getenv(\"SPARK_MEM\")) .map(warnSparkMem)) .map(Utils.memoryStringToMb) .getOrElse(1024) // Convert java options to env vars as a work around // since we can't set env vars directly in sbt. for { (envKey, propKey) <- Seq((\"SPARK_TESTING\", IS_TESTING.key)) value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} { executorEnvs(envKey) = value } Option(System.getenv(\"SPARK_PREPEND_CLASSES\")).foreach { v => executorEnvs(\"SPARK_PREPEND_CLASSES\") = v } // The Mesos scheduler backend relies on this environment variable to set executor memory. // TODO: Set this only in the Mesos scheduler. executorEnvs(\"SPARK_EXECUTOR_MEMORY\") = executorMemory + \"m\" executorEnvs ++= _conf.getExecutorEnv executorEnvs(\"SPARK_USER\") = sparkUser _shuffleDriverComponents = ShuffleDataIOUtils.loadShuffleDataIO(config).driver() _shuffleDriverComponents.initializeApplication().asScala.foreach { case (k, v) => _conf.set(ShuffleDataIOUtils.SHUFFLE_SPARK_CONF_PREFIX + k, v) } // We need to register \"HeartbeatReceiver\" before \"createTaskScheduler\" because Executor will // retrieve \"HeartbeatReceiver\" in the constructor. (SPARK-6640) _heartbeatReceiver = env.rpcEnv.setupEndpoint( HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this)) // Initialize any plugins before the task scheduler is initialized. _plugins = PluginContainer(this, _resources.asJava) // Create and start the scheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master) _schedulerBackend = sched _taskScheduler = ts _dagScheduler = new DAGScheduler(this) _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet) val _executorMetricsSource = if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) { Some(new ExecutorMetricsSource) } else { None } // create and start the heartbeater for collecting memory metrics _heartbeater = new Heartbeater( () => SparkContext.this.reportHeartBeat(_executorMetricsSource), \"driver-heartbeater\", conf.get(EXECUTOR_HEARTBEAT_INTERVAL)) _heartbeater.start() // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's // constructor _taskScheduler.start() _applicationId = _taskScheduler.applicationId() _applicationAttemptId = _taskScheduler.applicationAttemptId() _conf.set(\"spark.app.id\", _applicationId) _applicationAttemptId.foreach { attemptId => _conf.set(APP_ATTEMPT_ID, attemptId) _env.blockManager.blockStoreClient.setAppAttemptId(attemptId) } if (_conf.get(UI_REVERSE_PROXY)) { val proxyUrl = _conf.get(UI_REVERSE_PROXY_URL.key, \"\").stripSuffix(\"/\") + \"/proxy/\" + _applicationId System.setProperty(\"spark.ui.proxyBase\", proxyUrl) } _ui.foreach(_.setAppId(_applicationId)) _env.blockManager.initialize(_applicationId) FallbackStorage.registerBlockManagerIfNeeded(_env.blockManager.master, _conf) // The metrics system for Driver need to be set spark.app.id to app ID. // So it should start after we get app ID from the task scheduler and set spark.app.id. _env.metricsSystem.start(_conf.get(METRICS_STATIC_SOURCES_ENABLED)) _eventLogger = if (isEventLogEnabled) { val logger = new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get, _conf, _hadoopConfiguration) logger.start() listenerBus.addToEventLogQueue(logger) Some(logger) } else { None } _cleaner = if (_conf.get(CLEANER_REFERENCE_TRACKING)) { Some(new ContextCleaner(this, _shuffleDriverComponents)) } else { None } _cleaner.foreach(_.start()) val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) { schedulerBackend match { case b: ExecutorAllocationClient => Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ => None } } else { None } _executorAllocationManager.foreach(_.start()) setupAndStartListenerBus() postEnvironmentUpdate() postApplicationStart() // After application started, attach handlers to started server and start handler. _ui.foreach(_.attachAllHandler()) // Attach the driver metrics servlet handler to the web ui after the metrics system is started. _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler))) // Make sure the context is stopped if the user forgets about it. This avoids leaving // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM // is killed, though. logDebug(\"Adding shutdown hook\") // force eager creation of logger _shutdownHookRef = ShutdownHookManager.addShutdownHook( ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () => logInfo(\"Invoking stop() from shutdown hook\") try { stop() } catch { case e: Throwable => logWarning(\"Ignoring Exception while stopping SparkContext from shutdown hook\", e) } } // Post init _taskScheduler.postStartHook() if (isLocal) { _env.metricsSystem.registerSource(Executor.executorSourceLocalModeOnly) } _env.metricsSystem.registerSource(_dagScheduler.metricsSource) _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager)) _env.metricsSystem.registerSource(new JVMCPUSource()) _executorMetricsSource.foreach(_.register(_env.metricsSystem)) _executorAllocationManager.foreach { e => _env.metricsSystem.registerSource(e.executorAllocationManagerSource) } appStatusSource.foreach(_env.metricsSystem.registerSource(_)) _plugins.foreach(_.registerMetrics(applicationId)) } catch { case NonFatal(e) => logError(\"Error initializing SparkContext.\", e) try { stop() } catch { case NonFatal(inner) => logError(\"Error stopping SparkContext after init error.\", inner) } finally { throw e } } /** * Called by the web UI to obtain executor thread dumps. This method may be expensive. * Logs an error and returns None if we failed to obtain a thread dump, which could occur due * to an executor being dead or unresponsive or due to network issues while sending the thread * dump message back to the driver.  private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = { try { if (executorId == SparkContext.DRIVER_IDENTIFIER) { Some(Utils.getThreadDump()) } else { env.blockManager.master.getExecutorEndpointRef(executorId) match { case Some(endpointRef) => Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump)) case None => logWarning(s\"Executor $executorId might already have stopped and \" + \"can not request thread dump from it.\") None } } } catch { case e: Exception => logError(s\"Exception getting thread dump from executor $executorId\", e) None } } private[spark] def getLocalProperties: Properties = localProperties.get() private[spark] def setLocalProperties(props: Properties): Unit = { localProperties.set(props) } /** * Set a local property that affects jobs submitted from this thread, such as the Spark fair * scheduler pool. User-defined properties may also be set here. These properties are propagated * through to worker tasks and can be accessed there via * [[org.apache.spark.TaskContext#getLocalProperty]]. * * These properties are inherited by child threads spawned from this thread. This * may have unexpected consequences when working with thread pools. The standard java * implementation of thread pools have worker threads spawn other worker threads. * As a result, local properties may propagate unpredictably.  def setLocalProperty(key: String, value: String): Unit = { if (value == null) { localProperties.get.remove(key) } else { localProperties.get.setProperty(key, value) } } /** * Get a local property set in this thread, or null if it is missing. See * `org.apache.spark.SparkContext.setLocalProperty`.  def getLocalProperty(key: String): String = Option(localProperties.get).map(_.getProperty(key)).orNull /** Set a human readable description of the current job.  def setJobDescription(value: String): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value) } /** * Assigns a group ID to all the jobs started by this thread until the group ID is set to a * different value or cleared. * * Often, a unit of execution in an application consists of multiple Spark actions or jobs. * Application programmers can use this method to group all those jobs together and give a * group description. Once set, the Spark web UI will associate such jobs with this group. * * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all * running jobs in this group. For example, * {{{ * // In the main thread: * sc.setJobGroup(\"some_job_to_cancel\", \"some job description\") * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count() * * // In a separate thread: * sc.cancelJobGroup(\"some_job_to_cancel\") * }}} * * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()` * being called on the job's executor threads. This is useful to help ensure that the tasks * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS * may respond to Thread.interrupt() by marking nodes as dead.  def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId) // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup // APIs to also take advantage of this property (e.g., internal job failures or canceling from // JobProgressTab UI) on a per-job basis. setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString) } /** Clear the current thread's job group ID and its description.  def clearJobGroup(): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null) setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null) } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * @note Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body) // Methods for creating RDDs /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by * `step` every element. * * @note if we need to cache this RDD, we should make sure each partition does not exceed limit. * * @param start the start value. * @param end the end value. * @param step the incremental step * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed range  def range( start: Long, end: Long, step: Long = 1, numSlices: Int = defaultParallelism): RDD[Long] = withScope { assertNotStopped() // when step is 0, range will run infinitely require(step != 0, \"step cannot be 0\") val numElements: BigInt = { val safeStart = BigInt(start) val safeEnd = BigInt(end) if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) { (safeEnd - safeStart) / step } else { // the remainder has the same sign with range, could add 1 more (safeEnd - safeStart) / step + 1 } } parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) => val partitionStart = (i * numElements) / numSlices * step + start val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start def getSafeMargin(bi: BigInt): Long = if (bi.isValidLong) { bi.toLong } else if (bi > 0) { Long.MaxValue } else { Long.MinValue } val safePartitionStart = getSafeMargin(partitionStart) val safePartitionEnd = getSafeMargin(partitionEnd) new Iterator[Long] { private[this] var number: Long = safePartitionStart private[this] var overflow: Boolean = false override def hasNext = if (!overflow) { if (step > 0) { number < safePartitionEnd } else { number > safePartitionEnd } } else false override def next() = { val ret = number number += step if (number < ret ^ step < 0) { // we have Long.MaxValue + Long.MaxValue < Long.MaxValue // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step // back, we are pretty sure that we have an overflow. overflow = true } ret } } } } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * The text files must be encoded as UTF-8. * * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file  def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair => pair._2.toString).setName(path) } /** * Read a directory of text files from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI. Each file is read as a single record and returned in a * key-value pair, where the key is the path of each file, the value is the content of each file. * The text files must be encoded as UTF-8. * * <p> For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do `val rdd = sparkContext.wholeTextFile(\"hdfs://a-hdfs-path\")`, * * <p> then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred, large file is also allowable, but may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and the corresponding file content  def wholeTextFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new WholeTextFileRDD( this, classOf[WholeTextFileInputFormat], classOf[Text], classOf[Text], updateConf, minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path) } /** * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file * (useful for binary data) * * For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do * `val rdd = sparkContext.binaryFiles(\"hdfs://a-hdfs-path\")`, * * then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred; very large files may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and corresponding file content  def binaryFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new BinaryFileRDD( this, classOf[StreamInputFormat], classOf[String], classOf[PortableDataStream], updateConf, minPartitions).setName(path) } /** * Load data from a flat binary file, assuming the length of each record is constant. * * @note We ensure that the byte array for each record in the resulting RDD * has the provided record length. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param recordLength The length at which to split the records * @param conf Configuration for setting up the dataset. * * @return An RDD of data with values, represented as byte arrays  def binaryRecords( path: String, recordLength: Int, conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope { assertNotStopped() conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path, classOf[FixedLengthBinaryInputFormat], classOf[LongWritable], classOf[BytesWritable], conf = conf) br.map { case (k, v) => val bytes = v.copyBytes() assert(bytes.length == recordLength, \"Byte array does not have correct length\") bytes } } /** * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable), * using the older MapReduce API (`org.apache.hadoop.mapred`). * * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions Minimum number of Hadoop Splits to generate. * @return RDD of tuples of key and corresponding value * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def hadoopRDD[K, V]( conf: JobConf, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf before broadcasting it. SparkHadoopUtil.get.addCredentials(conf) new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions) } /** Get an RDD for a Hadoop file with an arbitrary InputFormat * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KiB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]] (path: String, minPartitions: Int) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile(path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], minPartitions) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths as * a list of inputs * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile[K, V, F](path, defaultMinPartitions) } /** * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys, * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user * don't need to pass them directly. Instead, callers can just write, for example: * ``` * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * ``` * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]] (path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { newAPIHadoopFile( path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]]) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * @param conf Hadoop configuration * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( path: String, fClass: Class[F], kClass: Class[K], vClass: Class[V], conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // The call to NewHadoopJob automatically adds security credentials to conf, // so we don't need to explicitly add them ourselves val job = NewHadoopJob.getInstance(conf) // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( conf: Configuration = hadoopConfiguration, fClass: Class[F], kClass: Class[K], vClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf. Required to access secure HDFS. val jconf = new JobConf(conf) SparkHadoopUtil.get.addCredentials(jconf) new NewHadoopRDD(this, fClass, kClass, vClass, jconf) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V], minPartitions: Int ): RDD[(K, V)] = withScope { assertNotStopped() val inputFormatClass = classOf[SequenceFileInputFormat[K, V]] hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V]( path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() sequenceFile(path, keyClass, valueClass, defaultMinPartitions) } /** * Version of sequenceFile() for types implicitly convertible to Writables through a * WritableConverter. For example, to access a SequenceFile where the keys are Text and the * values are IntWritable, you could simply write * {{{ * sparkContext.sequenceFile[String, Int](path, ...) * }}} * * WritableConverters are provided in a somewhat strange way (by an implicit function) to support * both subclasses of Writable and types for which we define a converter (e.g. Int to * IntWritable). The most natural thing would've been to have implicit objects for the * converters, but then we couldn't have an object for every subclass of Writable (you can't * have a parameterized singleton object). We use functions instead to create a new converter * for the appropriate type. In addition, we pass the converter a ClassTag of its type to * allow it to figure out the Writable class to use in the subclass case. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V] (path: String, minPartitions: Int = defaultMinPartitions) (implicit km: ClassTag[K], vm: ClassTag[V], kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = { withScope { assertNotStopped() val kc = clean(kcf)() val vc = clean(vcf)() val format = classOf[SequenceFileInputFormat[Writable, Writable]] val writables = hadoopFile(path, format, kc.writableClass(km).asInstanceOf[Class[Writable]], vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions) writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) } } } /** * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and * BytesWritable values that contain a serialized partition. This is still an experimental * storage format and may not be supported exactly as is in future Spark releases. It will also * be pretty slow if you use the default serializer (Java serialization), * though the nice thing about it is that there's very little effort required to save arbitrary * objects. * * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD representing deserialized data from the file(s)  def objectFile[T: ClassTag]( path: String, minPartitions: Int = defaultMinPartitions): RDD[T] = withScope { assertNotStopped() sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions) .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader)) } protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope { new ReliableCheckpointRDD[T](this, path) } /** Build the union of a list of RDDs.  def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } /** Build the union of a list of RDDs passed as variable-length arguments.  def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } /** Get an RDD that has no partitions or elements.  def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this) // Methods for creating shared variables /** * Register the given accumulator. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _]): Unit = { acc.register(this) } /** * Register the given accumulator with given name. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _], name: String): Unit = { acc.register(this, name = Option(name)) } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } /** * Broadcast a read-only variable to the cluster, returning a * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. * The variable will be sent to each cluster only once. * * @param value value to broadcast to the Spark nodes * @return `Broadcast` object, a read-only variable cached on each machine  def broadcast[T: ClassTag](value: T): Broadcast[T] = { assertNotStopped() require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\") val bc = env.broadcastManager.newBroadcast[T](value, isLocal) val callSite = getCallSite logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm) cleaner.foreach(_.registerBroadcastForCleanup(bc)) bc } /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String): Unit = { addFile(path, false, false) } /** * Returns a list of file paths that are added to resources.  def listFiles(): Seq[String] = addedFiles.keySet.toSeq /** * :: Experimental :: * Add an archive to be downloaded and unpacked with this Spark job on every node. * * If an archive is added during execution, it will not be available until the next TaskSet * starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(paths-to-files)` to find its download/unpacked location. * The given path should be one of .zip, .tar, .tar.gz, .tgz and .jar. * * @note A path can be added only once. Subsequent additions of the same path are ignored. * * @since 3.1.0  @Experimental def addArchive(path: String): Unit = { addFile(path, false, false, isArchive = true) } /** * :: Experimental :: * Returns a list of archive paths that are added to resources. * * @since 3.1.0  @Experimental def listArchives(): Seq[String] = addedArchives.keySet.toSeq /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * @param recursive if true, a directory can be given in `path`. Currently directories are * only supported for Hadoop-supported filesystems. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution.  @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus.  @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently.  private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager.  @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received.  @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received.  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running.  def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching.  def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc.  @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful.  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler  @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists  @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode  def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage  private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources.  def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead.  private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext.  def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None.  private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs.  def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version.  private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout)  @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready  def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future.  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information.  def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running.  def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed.  def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster)  def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD).  def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718  def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID  private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered).  private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event  private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event  private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready  private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver.  private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features.  object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction.  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors.  private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call)  def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call)  def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any.  private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started.  private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts.  private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details).  private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found  def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would.  private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler.  private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled`  private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`.  private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string.  private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).  private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`.  private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Summary: Top 3 most important classes in this project\n\nGenerated from repository code graph analysis on 2025-12-27 10:33 (local time).\n\nBelow are the three most important classes identified by the repositorys code graph analysis (combined metric), along with precise descriptions based on the code provided by the context tool.\n\n### 1) org.apache.spark.rdd.RDD[T]\n- What it is: The abstract foundational data structure of Sparks core, representing a resilient, partitioned dataset with a lineage of dependencies. It is Serializable and mixes in Logging.\n- Key constructor and fields:\n  - Primary constructor: RDD[T](@transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]])\n  - One-parent constructor: def this(@transient oneParent: RDD[_])\n  - Access to Spark conf via private[spark] def conf = sc.conf\n  - Validates at runtime that nested RDDs are not used (warns via logWarning).\n- Contracts for subclasses:\n  - @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T]\n  - protected def getPartitions: Array[Partition]\n  - protected def getDependencies: Seq[Dependency[_]] = deps\n  - protected def getPreferredLocations(split: Partition): Seq[String] = Nil\n- Purpose and importance:\n  - Defines the core execution and partitioning model of Spark, specifying how data is computed per Partition and how RDDs depend on each other. All transformations/actions ultimately operate on RDDs, making this class central to Sparks computation model.\n\n### 2) org.apache.spark.sql.Dataset[T]\n- What it is: The primary, strongly-typed logical dataset in Spark SQL, coupling logical plans with encoders to produce typed user-facing APIs.\n- Key constructor and fields:\n  - @Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T])\n  - Alternate constructors from SparkSession/SQLContext and LogicalPlan.\n  - @transient lazy val sparkSession: SparkSession derived from queryExecution; throws if invoked off-driver or when missing.\n  - A globally unique id for the Dataset used in self-join ambiguity tagging.\n  - queryExecution.assertAnalyzed() is invoked to ensure analysis is complete.\n  - private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)\n  - private lazy val resolvedEnc computed against logicalPlan.output and session state.\n  - logicalPlan is obtained from queryExecution.commandExecuted; conditionally tags plan with DATASET_ID_TAG when FAIL_AMBIGUOUS_SELF_JOIN_ENABLED is on.\n- Purpose and importance:\n  - Binds the SQL Catalyst planning/execution (QueryExecution, LogicalPlan) to user-level typed operations via Encoder/ExpressionEncoder. It is the core abstraction for typed structured queries, transformations, and actions in Spark SQL, bridging the optimizer with user APIs.\n\n### 3) org.apache.spark.SparkContext\n- What it is: The entry point for Spark Core, responsible for driver-side lifecycle, configuration, and job submission.\n- Key constructor and fields:\n  - class SparkContext(config: SparkConf) extends Logging\n  - Guards: prevents creation on executors (EXECUTOR_ALLOW_SPARK_CONTEXT), and enforces a single active context via SparkContext.markPartiallyConstructed and activeContext checks.\n  - Tracks creationSite to aid diagnostics; asserts not stopped via assertNotStopped with detailed error messages.\n  - Convenience constructors: def this() = this(new SparkConf()); def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(...)).\n  - Lifecycle state: private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false)\n- Purpose and importance:\n  - Central orchestration object that wires user applications to clusters, manages configuration, and maintains the active Spark client context. Nearly all RDD-based operations originate from or pass through SparkContext.\n\n### Why these three are most important\n- RDD defines the core distributed data model and computation contract in Spark Core.\n- Dataset is the typed, optimized, SQL-integrated abstraction that most higher-level structured workloads build upon.\n- SparkContext is the lifecycle and entry point to cluster interaction, guarding correct driver behavior and configuration.\n\n",
          "prompt_time": 0.906,
          "answer_time": 74,
          "context_tokens": 74560,
          "used_context": [
            "## CLASS: org/apache/spark/rdd/RDD#\n abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent  def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition.  @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`  protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it.  protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences.  protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned.  @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD.  def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext).  val id: Int = sc.newRddId() /** A friendly name for this RDD  @transient var name: String = _ /** Assign a name to this RDD  def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception.  def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD.  def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set.  def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver).  private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not.  final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing.  final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not.  final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD.  @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed.  final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD.  final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned.  private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD.  def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results.  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate.  def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable.  def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition.  val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]].  def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array  def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement.  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function.  def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally.  def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD  def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array.  def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`.  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD  def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning.  private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other).  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition.  def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD.  def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD.  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first.  def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`.  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us.  def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator.  def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]]  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions  def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2)  def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor  def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD.  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map.  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped.  def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017.  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`.  def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD.  def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD *  def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD *  def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition.  def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements.  def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements.  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects.  def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`.  def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition  private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.  def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.  def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally.  def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing.  private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing.  private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized.  private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally.  def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned  @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents.  def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a>  @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD.  @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified  @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`).  @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations.  @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD  protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T]  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on.  def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs.  private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions.  private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.  protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging.  def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier.  private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level.  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.  object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.  private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
            "## CLASS: org/apache/spark/sql/Dataset#\n@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema).  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right.  private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value).  private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0  // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0  @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0  def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0  def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0  def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0  def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0  def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0  def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0  def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0  def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0  def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0  def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0  // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0  def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0  def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0  // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0  def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0  @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0  def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0  def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0  def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface.  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0  @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0  @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0  def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0  def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0  def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0  def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0  def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling.  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names.  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata.  private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata.  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0  def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0  def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0  def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0  def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0  @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0  @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0  def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0  def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0  def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition.  private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers.  private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches.  private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0  def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0  def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0  def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0  def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0  def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0  def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0  def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0  def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0  def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0  lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0  @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0  @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0  def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0  @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0  def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0  def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0  def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0  def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0  def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0  @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0  @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark.  private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD.  private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`.  private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side.  private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution.  private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset.  private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception.  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan.  private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame.  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset.  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset.  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches.  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
            "## CLASS: org/apache/spark/SparkContext#\n* this config overrides the default configs as well as system properties.  class SparkContext(config: SparkConf) extends Logging { // The call site where this SparkContext was constructed. private val creationSite: CallSite = Utils.getCallSite() if (!config.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { // In order to prevent SparkContext from being created in executors. SparkContext.assertOnDriver() } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having started construction. // NOTE: this must be placed at the beginning of the SparkContext constructor. SparkContext.markPartiallyConstructed(this) val startTime = System.currentTimeMillis() private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false) private[spark] def assertNotStopped(): Unit = { if (stopped.get()) { val activeContext = SparkContext.activeContext.get() val activeCreationSite = if (activeContext == null) { \"(No active SparkContext.)\" } else { activeContext.creationSite.longForm } throw new IllegalStateException( s\"\"\"Cannot call methods on a stopped SparkContext. |This stopped SparkContext was created at: | |${creationSite.longForm} | |The currently active SparkContext was created at: | |$activeCreationSite \"\"\".stripMargin) } } /** * Create a SparkContext that loads settings from system properties (for instance, when * launching with ./bin/spark-submit).  def this() = this(new SparkConf()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters  def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(conf, master, appName)) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs. * @param environment Environment variables to set on worker nodes.  def this( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment)) } // The following constructors are required when Java code accesses SparkContext directly. // Please see SI-4278 /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI.  private[spark] def this(master: String, appName: String) = this(master, appName, null, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes.  private[spark] def this(master: String, appName: String, sparkHome: String) = this(master, appName, sparkHome, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs.  private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) = this(master, appName, sparkHome, jars, Map()) // log out Spark Version in Spark driver log logInfo(s\"Running Spark version $SPARK_VERSION\") /* ------------------------------------------------------------------------------------- * | Private variables. These variables keep the internal state of the context, and are | | not accessible by the outside world. They're mutable since we want to initialize all | | of them to some neutral value ahead of time, so that calling \"stop()\" while the | | constructor is still running is safe. | * -------------------------------------------------------------------------------------  private var _conf: SparkConf = _ private var _eventLogDir: Option[URI] = None private var _eventLogCodec: Option[String] = None private var _listenerBus: LiveListenerBus = _ private var _env: SparkEnv = _ private var _statusTracker: SparkStatusTracker = _ private var _progressBar: Option[ConsoleProgressBar] = None private var _ui: Option[SparkUI] = None private var _hadoopConfiguration: Configuration = _ private var _executorMemory: Int = _ private var _schedulerBackend: SchedulerBackend = _ private var _taskScheduler: TaskScheduler = _ private var _heartbeatReceiver: RpcEndpointRef = _ @volatile private var _dagScheduler: DAGScheduler = _ private var _applicationId: String = _ private var _applicationAttemptId: Option[String] = None private var _eventLogger: Option[EventLoggingListener] = None private var _driverLogger: Option[DriverLogger] = None private var _executorAllocationManager: Option[ExecutorAllocationManager] = None private var _cleaner: Option[ContextCleaner] = None private var _listenerBusStarted: Boolean = false private var _jars: Seq[String] = _ private var _files: Seq[String] = _ private var _archives: Seq[String] = _ private var _shutdownHookRef: AnyRef = _ private var _statusStore: AppStatusStore = _ private var _heartbeater: Heartbeater = _ private var _resources: immutable.Map[String, ResourceInformation] = _ private var _shuffleDriverComponents: ShuffleDriverComponents = _ private var _plugins: Option[PluginContainer] = None private var _resourceProfileManager: ResourceProfileManager = _ /* ------------------------------------------------------------------------------------- * | Accessors and public fields. These provide access to the internal state of the | | context. | * -------------------------------------------------------------------------------------  private[spark] def conf: SparkConf = _conf /** * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be * changed at runtime.  def getConf: SparkConf = conf.clone() def resources: Map[String, ResourceInformation] = _resources def jars: Seq[String] = _jars def files: Seq[String] = _files def archives: Seq[String] = _archives def master: String = _conf.get(\"spark.master\") def deployMode: String = _conf.get(SUBMIT_DEPLOY_MODE) def appName: String = _conf.get(\"spark.app.name\") private[spark] def isEventLogEnabled: Boolean = _conf.get(EVENT_LOG_ENABLED) private[spark] def eventLogDir: Option[URI] = _eventLogDir private[spark] def eventLogCodec: Option[String] = _eventLogCodec def isLocal: Boolean = Utils.isLocalMaster(_conf) /** * @return true if context is stopped or in the midst of stopping.  def isStopped: Boolean = stopped.get() private[spark] def statusStore: AppStatusStore = _statusStore // An asynchronous listener bus for Spark events private[spark] def listenerBus: LiveListenerBus = _listenerBus // This function allows components created by SparkEnv to be mocked in unit tests: private[spark] def createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv = { SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf)) } private[spark] def env: SparkEnv = _env // Used to store a URL for each static file/jar together with the file's local timestamp private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedArchives = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala // Keeps track of all persisted RDDs private[spark] val persistentRdds = { val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]() map.asScala } def statusTracker: SparkStatusTracker = _statusTracker private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar private[spark] def ui: Option[SparkUI] = _ui def uiWebUrl: Option[String] = _ui.map(_.webUrl) /** * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse. * * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you * plan to set some global configurations for all Hadoop RDDs.  def hadoopConfiguration: Configuration = _hadoopConfiguration private[spark] def executorMemory: Int = _executorMemory // Environment variables to pass to our executors. private[spark] val executorEnvs = HashMap[String, String]() // Set SPARK_USER for user who is running SparkContext. val sparkUser = Utils.getCurrentUserName() private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend private[spark] def taskScheduler: TaskScheduler = _taskScheduler private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = { _taskScheduler = ts } private[spark] def dagScheduler: DAGScheduler = _dagScheduler private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = { _dagScheduler = ds } private[spark] def shuffleDriverComponents: ShuffleDriverComponents = _shuffleDriverComponents /** * A unique identifier for the Spark application. * Its format depends on the scheduler implementation. * (i.e. * in case of local spark app something like 'local-1433865536131' * in case of YARN something like 'application_1433865536131_34483' * in case of MESOS something like 'driver-20170926223339-0001' * )  def applicationId: String = _applicationId def applicationAttemptId: Option[String] = _applicationAttemptId private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] = _executorAllocationManager private[spark] def resourceProfileManager: ResourceProfileManager = _resourceProfileManager private[spark] def cleaner: Option[ContextCleaner] = _cleaner private[spark] var checkpointDir: Option[String] = None // Thread Local variable that can be used by users to pass information down the stack protected[spark] val localProperties = new InheritableThreadLocal[Properties] { override def childValue(parent: Properties): Properties = { // Note: make a clone such that changes in the parent properties aren't reflected in // the those of the children threads, which has confusing semantics (SPARK-10563). Utils.cloneProperties(parent) } override protected def initialValue(): Properties = new Properties() } /* ------------------------------------------------------------------------------------- * | Initialization. This code initializes the context in a manner that is exception-safe. | | All internal fields holding state are initialized here, and any error prompts the | | stop() method to be called. | * -------------------------------------------------------------------------------------  private def warnSparkMem(value: String): String = { logWarning(\"Using SPARK_MEM to set amount of memory to use per executor process is \" + \"deprecated, please use spark.executor.memory instead.\") value } /** Control our logLevel. This overrides any user-defined log settings. * @param logLevel The desired log level as a string. * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN  def setLogLevel(logLevel: String): Unit = { // let's allow lowercase or mixed case too val upperCased = logLevel.toUpperCase(Locale.ROOT) require(SparkContext.VALID_LOG_LEVELS.contains(upperCased), s\"Supplied level $logLevel did not match one of:\" + s\" ${SparkContext.VALID_LOG_LEVELS.mkString(\",\")}\") Utils.setLogLevel(Level.toLevel(upperCased)) } try { _conf = config.clone() _conf.validateSettings() _conf.set(\"spark.app.startTime\", startTime.toString) if (!_conf.contains(\"spark.master\")) { throw new SparkException(\"A master URL must be set in your configuration\") } if (!_conf.contains(\"spark.app.name\")) { throw new SparkException(\"An application name must be set in your configuration\") } // This should be set as early as possible. SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf) SparkContext.supplementJavaModuleOptions(_conf) _driverLogger = DriverLogger(_conf) val resourcesFileOpt = conf.get(DRIVER_RESOURCES_FILE) _resources = getOrDiscoverAllResources(_conf, SPARK_DRIVER_PREFIX, resourcesFileOpt) logResourceInfo(SPARK_DRIVER_PREFIX, _resources) // log out spark.app.name in the Spark driver logs logInfo(s\"Submitted application: $appName\") // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster if (master == \"yarn\" && deployMode == \"cluster\" && !_conf.contains(\"spark.yarn.app.id\")) { throw new SparkException(\"Detected yarn cluster mode, but isn't running on a cluster. \" + \"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.\") } if (_conf.getBoolean(\"spark.logConf\", false)) { logInfo(\"Spark configuration:\\n\" + _conf.toDebugString) } // Set Spark driver host and port system properties. This explicitly sets the configuration // instead of relying on the default value of the config constant. _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS)) _conf.setIfMissing(DRIVER_PORT, 0) _conf.set(EXECUTOR_ID, SparkContext.DRIVER_IDENTIFIER) _jars = Utils.getUserJars(_conf) _files = _conf.getOption(FILES.key).map(_.split(\",\")).map(_.filter(_.nonEmpty)) .toSeq.flatten _archives = _conf.getOption(ARCHIVES.key).map(Utils.stringToSeq).toSeq.flatten _eventLogDir = if (isEventLogEnabled) { val unresolvedDir = conf.get(EVENT_LOG_DIR).stripSuffix(\"/\") Some(Utils.resolveURI(unresolvedDir)) } else { None } _eventLogCodec = { val compress = _conf.get(EVENT_LOG_COMPRESS) if (compress && isEventLogEnabled) { Some(_conf.get(EVENT_LOG_COMPRESSION_CODEC)).map(CompressionCodec.getShortName) } else { None } } _listenerBus = new LiveListenerBus(_conf) _resourceProfileManager = new ResourceProfileManager(_conf, _listenerBus) // Initialize the app status store and listener before SparkEnv is created so that it gets // all events. val appStatusSource = AppStatusSource.createSource(conf) _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource) listenerBus.addToStatusQueue(_statusStore.listener.get) // Create the Spark execution environment (cache, map output tracker, etc) _env = createSparkEnv(_conf, isLocal, listenerBus) SparkEnv.set(_env) // If running the REPL, register the repl's output dir with the file server. _conf.getOption(\"spark.repl.class.outputDir\").foreach { path => val replUri = _env.rpcEnv.fileServer.addDirectory(\"/classes\", new File(path)) _conf.set(\"spark.repl.class.uri\", replUri) } _statusTracker = new SparkStatusTracker(this, _statusStore) _progressBar = if (_conf.get(UI_SHOW_CONSOLE_PROGRESS)) { Some(new ConsoleProgressBar(this)) } else { None } _ui = if (conf.get(UI_ENABLED)) { Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, \"\", startTime)) } else { // For tests, do not enable the UI None } // Bind the UI before starting the task scheduler to communicate // the bound port to the cluster manager properly _ui.foreach(_.bind()) _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf) // Performance optimization: this dummy call to .size() triggers eager evaluation of // Configuration's internal `properties` field, guaranteeing that it will be computed and // cached before SessionState.newHadoopConf() uses `sc.hadoopConfiguration` to create // a new per-session Configuration. If `properties` has not been computed by that time // then each newly-created Configuration will perform its own expensive IO and XML // parsing to load configuration defaults and populate its own properties. By ensuring // that we've pre-computed the parent's properties, the child Configuration will simply // clone the parent's properties. _hadoopConfiguration.size() // Add each JAR given through the constructor if (jars != null) { jars.foreach(jar => addJar(jar, true)) if (addedJars.nonEmpty) { _conf.set(\"spark.app.initial.jar.urls\", addedJars.keys.toSeq.mkString(\",\")) } } if (files != null) { files.foreach(file => addFile(file, false, true)) if (addedFiles.nonEmpty) { _conf.set(\"spark.app.initial.file.urls\", addedFiles.keys.toSeq.mkString(\",\")) } } if (archives != null) { archives.foreach(file => addFile(file, false, true, isArchive = true)) if (addedArchives.nonEmpty) { _conf.set(\"spark.app.initial.archive.urls\", addedArchives.keys.toSeq.mkString(\",\")) } } _executorMemory = _conf.getOption(EXECUTOR_MEMORY.key) .orElse(Option(System.getenv(\"SPARK_EXECUTOR_MEMORY\"))) .orElse(Option(System.getenv(\"SPARK_MEM\")) .map(warnSparkMem)) .map(Utils.memoryStringToMb) .getOrElse(1024) // Convert java options to env vars as a work around // since we can't set env vars directly in sbt. for { (envKey, propKey) <- Seq((\"SPARK_TESTING\", IS_TESTING.key)) value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} { executorEnvs(envKey) = value } Option(System.getenv(\"SPARK_PREPEND_CLASSES\")).foreach { v => executorEnvs(\"SPARK_PREPEND_CLASSES\") = v } // The Mesos scheduler backend relies on this environment variable to set executor memory. // TODO: Set this only in the Mesos scheduler. executorEnvs(\"SPARK_EXECUTOR_MEMORY\") = executorMemory + \"m\" executorEnvs ++= _conf.getExecutorEnv executorEnvs(\"SPARK_USER\") = sparkUser _shuffleDriverComponents = ShuffleDataIOUtils.loadShuffleDataIO(config).driver() _shuffleDriverComponents.initializeApplication().asScala.foreach { case (k, v) => _conf.set(ShuffleDataIOUtils.SHUFFLE_SPARK_CONF_PREFIX + k, v) } // We need to register \"HeartbeatReceiver\" before \"createTaskScheduler\" because Executor will // retrieve \"HeartbeatReceiver\" in the constructor. (SPARK-6640) _heartbeatReceiver = env.rpcEnv.setupEndpoint( HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this)) // Initialize any plugins before the task scheduler is initialized. _plugins = PluginContainer(this, _resources.asJava) // Create and start the scheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master) _schedulerBackend = sched _taskScheduler = ts _dagScheduler = new DAGScheduler(this) _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet) val _executorMetricsSource = if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) { Some(new ExecutorMetricsSource) } else { None } // create and start the heartbeater for collecting memory metrics _heartbeater = new Heartbeater( () => SparkContext.this.reportHeartBeat(_executorMetricsSource), \"driver-heartbeater\", conf.get(EXECUTOR_HEARTBEAT_INTERVAL)) _heartbeater.start() // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's // constructor _taskScheduler.start() _applicationId = _taskScheduler.applicationId() _applicationAttemptId = _taskScheduler.applicationAttemptId() _conf.set(\"spark.app.id\", _applicationId) _applicationAttemptId.foreach { attemptId => _conf.set(APP_ATTEMPT_ID, attemptId) _env.blockManager.blockStoreClient.setAppAttemptId(attemptId) } if (_conf.get(UI_REVERSE_PROXY)) { val proxyUrl = _conf.get(UI_REVERSE_PROXY_URL.key, \"\").stripSuffix(\"/\") + \"/proxy/\" + _applicationId System.setProperty(\"spark.ui.proxyBase\", proxyUrl) } _ui.foreach(_.setAppId(_applicationId)) _env.blockManager.initialize(_applicationId) FallbackStorage.registerBlockManagerIfNeeded(_env.blockManager.master, _conf) // The metrics system for Driver need to be set spark.app.id to app ID. // So it should start after we get app ID from the task scheduler and set spark.app.id. _env.metricsSystem.start(_conf.get(METRICS_STATIC_SOURCES_ENABLED)) _eventLogger = if (isEventLogEnabled) { val logger = new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get, _conf, _hadoopConfiguration) logger.start() listenerBus.addToEventLogQueue(logger) Some(logger) } else { None } _cleaner = if (_conf.get(CLEANER_REFERENCE_TRACKING)) { Some(new ContextCleaner(this, _shuffleDriverComponents)) } else { None } _cleaner.foreach(_.start()) val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) { schedulerBackend match { case b: ExecutorAllocationClient => Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ => None } } else { None } _executorAllocationManager.foreach(_.start()) setupAndStartListenerBus() postEnvironmentUpdate() postApplicationStart() // After application started, attach handlers to started server and start handler. _ui.foreach(_.attachAllHandler()) // Attach the driver metrics servlet handler to the web ui after the metrics system is started. _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler))) // Make sure the context is stopped if the user forgets about it. This avoids leaving // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM // is killed, though. logDebug(\"Adding shutdown hook\") // force eager creation of logger _shutdownHookRef = ShutdownHookManager.addShutdownHook( ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () => logInfo(\"Invoking stop() from shutdown hook\") try { stop() } catch { case e: Throwable => logWarning(\"Ignoring Exception while stopping SparkContext from shutdown hook\", e) } } // Post init _taskScheduler.postStartHook() if (isLocal) { _env.metricsSystem.registerSource(Executor.executorSourceLocalModeOnly) } _env.metricsSystem.registerSource(_dagScheduler.metricsSource) _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager)) _env.metricsSystem.registerSource(new JVMCPUSource()) _executorMetricsSource.foreach(_.register(_env.metricsSystem)) _executorAllocationManager.foreach { e => _env.metricsSystem.registerSource(e.executorAllocationManagerSource) } appStatusSource.foreach(_env.metricsSystem.registerSource(_)) _plugins.foreach(_.registerMetrics(applicationId)) } catch { case NonFatal(e) => logError(\"Error initializing SparkContext.\", e) try { stop() } catch { case NonFatal(inner) => logError(\"Error stopping SparkContext after init error.\", inner) } finally { throw e } } /** * Called by the web UI to obtain executor thread dumps. This method may be expensive. * Logs an error and returns None if we failed to obtain a thread dump, which could occur due * to an executor being dead or unresponsive or due to network issues while sending the thread * dump message back to the driver.  private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = { try { if (executorId == SparkContext.DRIVER_IDENTIFIER) { Some(Utils.getThreadDump()) } else { env.blockManager.master.getExecutorEndpointRef(executorId) match { case Some(endpointRef) => Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump)) case None => logWarning(s\"Executor $executorId might already have stopped and \" + \"can not request thread dump from it.\") None } } } catch { case e: Exception => logError(s\"Exception getting thread dump from executor $executorId\", e) None } } private[spark] def getLocalProperties: Properties = localProperties.get() private[spark] def setLocalProperties(props: Properties): Unit = { localProperties.set(props) } /** * Set a local property that affects jobs submitted from this thread, such as the Spark fair * scheduler pool. User-defined properties may also be set here. These properties are propagated * through to worker tasks and can be accessed there via * [[org.apache.spark.TaskContext#getLocalProperty]]. * * These properties are inherited by child threads spawned from this thread. This * may have unexpected consequences when working with thread pools. The standard java * implementation of thread pools have worker threads spawn other worker threads. * As a result, local properties may propagate unpredictably.  def setLocalProperty(key: String, value: String): Unit = { if (value == null) { localProperties.get.remove(key) } else { localProperties.get.setProperty(key, value) } } /** * Get a local property set in this thread, or null if it is missing. See * `org.apache.spark.SparkContext.setLocalProperty`.  def getLocalProperty(key: String): String = Option(localProperties.get).map(_.getProperty(key)).orNull /** Set a human readable description of the current job.  def setJobDescription(value: String): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value) } /** * Assigns a group ID to all the jobs started by this thread until the group ID is set to a * different value or cleared. * * Often, a unit of execution in an application consists of multiple Spark actions or jobs. * Application programmers can use this method to group all those jobs together and give a * group description. Once set, the Spark web UI will associate such jobs with this group. * * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all * running jobs in this group. For example, * {{{ * // In the main thread: * sc.setJobGroup(\"some_job_to_cancel\", \"some job description\") * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count() * * // In a separate thread: * sc.cancelJobGroup(\"some_job_to_cancel\") * }}} * * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()` * being called on the job's executor threads. This is useful to help ensure that the tasks * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS * may respond to Thread.interrupt() by marking nodes as dead.  def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId) // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup // APIs to also take advantage of this property (e.g., internal job failures or canceling from // JobProgressTab UI) on a per-job basis. setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString) } /** Clear the current thread's job group ID and its description.  def clearJobGroup(): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null) setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null) } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * @note Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body) // Methods for creating RDDs /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by * `step` every element. * * @note if we need to cache this RDD, we should make sure each partition does not exceed limit. * * @param start the start value. * @param end the end value. * @param step the incremental step * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed range  def range( start: Long, end: Long, step: Long = 1, numSlices: Int = defaultParallelism): RDD[Long] = withScope { assertNotStopped() // when step is 0, range will run infinitely require(step != 0, \"step cannot be 0\") val numElements: BigInt = { val safeStart = BigInt(start) val safeEnd = BigInt(end) if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) { (safeEnd - safeStart) / step } else { // the remainder has the same sign with range, could add 1 more (safeEnd - safeStart) / step + 1 } } parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) => val partitionStart = (i * numElements) / numSlices * step + start val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start def getSafeMargin(bi: BigInt): Long = if (bi.isValidLong) { bi.toLong } else if (bi > 0) { Long.MaxValue } else { Long.MinValue } val safePartitionStart = getSafeMargin(partitionStart) val safePartitionEnd = getSafeMargin(partitionEnd) new Iterator[Long] { private[this] var number: Long = safePartitionStart private[this] var overflow: Boolean = false override def hasNext = if (!overflow) { if (step > 0) { number < safePartitionEnd } else { number > safePartitionEnd } } else false override def next() = { val ret = number number += step if (number < ret ^ step < 0) { // we have Long.MaxValue + Long.MaxValue < Long.MaxValue // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step // back, we are pretty sure that we have an overflow. overflow = true } ret } } } } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * The text files must be encoded as UTF-8. * * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file  def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair => pair._2.toString).setName(path) } /** * Read a directory of text files from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI. Each file is read as a single record and returned in a * key-value pair, where the key is the path of each file, the value is the content of each file. * The text files must be encoded as UTF-8. * * <p> For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do `val rdd = sparkContext.wholeTextFile(\"hdfs://a-hdfs-path\")`, * * <p> then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred, large file is also allowable, but may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and the corresponding file content  def wholeTextFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new WholeTextFileRDD( this, classOf[WholeTextFileInputFormat], classOf[Text], classOf[Text], updateConf, minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path) } /** * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file * (useful for binary data) * * For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do * `val rdd = sparkContext.binaryFiles(\"hdfs://a-hdfs-path\")`, * * then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred; very large files may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and corresponding file content  def binaryFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new BinaryFileRDD( this, classOf[StreamInputFormat], classOf[String], classOf[PortableDataStream], updateConf, minPartitions).setName(path) } /** * Load data from a flat binary file, assuming the length of each record is constant. * * @note We ensure that the byte array for each record in the resulting RDD * has the provided record length. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param recordLength The length at which to split the records * @param conf Configuration for setting up the dataset. * * @return An RDD of data with values, represented as byte arrays  def binaryRecords( path: String, recordLength: Int, conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope { assertNotStopped() conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path, classOf[FixedLengthBinaryInputFormat], classOf[LongWritable], classOf[BytesWritable], conf = conf) br.map { case (k, v) => val bytes = v.copyBytes() assert(bytes.length == recordLength, \"Byte array does not have correct length\") bytes } } /** * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable), * using the older MapReduce API (`org.apache.hadoop.mapred`). * * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions Minimum number of Hadoop Splits to generate. * @return RDD of tuples of key and corresponding value * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def hadoopRDD[K, V]( conf: JobConf, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf before broadcasting it. SparkHadoopUtil.get.addCredentials(conf) new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions) } /** Get an RDD for a Hadoop file with an arbitrary InputFormat * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KiB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]] (path: String, minPartitions: Int) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile(path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], minPartitions) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths as * a list of inputs * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile[K, V, F](path, defaultMinPartitions) } /** * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys, * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user * don't need to pass them directly. Instead, callers can just write, for example: * ``` * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * ``` * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]] (path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { newAPIHadoopFile( path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]]) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * @param conf Hadoop configuration * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( path: String, fClass: Class[F], kClass: Class[K], vClass: Class[V], conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // The call to NewHadoopJob automatically adds security credentials to conf, // so we don't need to explicitly add them ourselves val job = NewHadoopJob.getInstance(conf) // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( conf: Configuration = hadoopConfiguration, fClass: Class[F], kClass: Class[K], vClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf. Required to access secure HDFS. val jconf = new JobConf(conf) SparkHadoopUtil.get.addCredentials(jconf) new NewHadoopRDD(this, fClass, kClass, vClass, jconf) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V], minPartitions: Int ): RDD[(K, V)] = withScope { assertNotStopped() val inputFormatClass = classOf[SequenceFileInputFormat[K, V]] hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V]( path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() sequenceFile(path, keyClass, valueClass, defaultMinPartitions) } /** * Version of sequenceFile() for types implicitly convertible to Writables through a * WritableConverter. For example, to access a SequenceFile where the keys are Text and the * values are IntWritable, you could simply write * {{{ * sparkContext.sequenceFile[String, Int](path, ...) * }}} * * WritableConverters are provided in a somewhat strange way (by an implicit function) to support * both subclasses of Writable and types for which we define a converter (e.g. Int to * IntWritable). The most natural thing would've been to have implicit objects for the * converters, but then we couldn't have an object for every subclass of Writable (you can't * have a parameterized singleton object). We use functions instead to create a new converter * for the appropriate type. In addition, we pass the converter a ClassTag of its type to * allow it to figure out the Writable class to use in the subclass case. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V] (path: String, minPartitions: Int = defaultMinPartitions) (implicit km: ClassTag[K], vm: ClassTag[V], kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = { withScope { assertNotStopped() val kc = clean(kcf)() val vc = clean(vcf)() val format = classOf[SequenceFileInputFormat[Writable, Writable]] val writables = hadoopFile(path, format, kc.writableClass(km).asInstanceOf[Class[Writable]], vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions) writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) } } } /** * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and * BytesWritable values that contain a serialized partition. This is still an experimental * storage format and may not be supported exactly as is in future Spark releases. It will also * be pretty slow if you use the default serializer (Java serialization), * though the nice thing about it is that there's very little effort required to save arbitrary * objects. * * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD representing deserialized data from the file(s)  def objectFile[T: ClassTag]( path: String, minPartitions: Int = defaultMinPartitions): RDD[T] = withScope { assertNotStopped() sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions) .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader)) } protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope { new ReliableCheckpointRDD[T](this, path) } /** Build the union of a list of RDDs.  def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } /** Build the union of a list of RDDs passed as variable-length arguments.  def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } /** Get an RDD that has no partitions or elements.  def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this) // Methods for creating shared variables /** * Register the given accumulator. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _]): Unit = { acc.register(this) } /** * Register the given accumulator with given name. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _], name: String): Unit = { acc.register(this, name = Option(name)) } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } /** * Broadcast a read-only variable to the cluster, returning a * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. * The variable will be sent to each cluster only once. * * @param value value to broadcast to the Spark nodes * @return `Broadcast` object, a read-only variable cached on each machine  def broadcast[T: ClassTag](value: T): Broadcast[T] = { assertNotStopped() require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\") val bc = env.broadcastManager.newBroadcast[T](value, isLocal) val callSite = getCallSite logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm) cleaner.foreach(_.registerBroadcastForCleanup(bc)) bc } /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String): Unit = { addFile(path, false, false) } /** * Returns a list of file paths that are added to resources.  def listFiles(): Seq[String] = addedFiles.keySet.toSeq /** * :: Experimental :: * Add an archive to be downloaded and unpacked with this Spark job on every node. * * If an archive is added during execution, it will not be available until the next TaskSet * starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(paths-to-files)` to find its download/unpacked location. * The given path should be one of .zip, .tar, .tar.gz, .tgz and .jar. * * @note A path can be added only once. Subsequent additions of the same path are ignored. * * @since 3.1.0  @Experimental def addArchive(path: String): Unit = { addFile(path, false, false, isArchive = true) } /** * :: Experimental :: * Returns a list of archive paths that are added to resources. * * @since 3.1.0  @Experimental def listArchives(): Seq[String] = addedArchives.keySet.toSeq /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * @param recursive if true, a directory can be given in `path`. Currently directories are * only supported for Hadoop-supported filesystems. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution.  @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus.  @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently.  private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager.  @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received.  @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received.  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running.  def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching.  def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc.  @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful.  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler  @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists  @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode  def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage  private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources.  def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead.  private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext.  def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None.  private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs.  def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version.  private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout)  @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready  def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future.  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information.  def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running.  def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed.  def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster)  def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD).  def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718  def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID  private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered).  private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event  private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event  private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready  private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver.  private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features.  object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction.  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors.  private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call)  def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call)  def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any.  private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started.  private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts.  private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details).  private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found  def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would.  private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler.  private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled`  private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`.  private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string.  private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).  private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`.  private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
          ],
          "tokens": 0,
          "more_info": "Didn't return solution, needed to be asked for solution to save it to .md file"
        },
        "without_mcp": {
          "answer": "# Three Most Important Classes in Apache Spark 3.3.0\n\nThis document briefly describes three foundational classes that underpin Spark's core and SQL functionality: SparkContext, RDD, and SparkSession. These classes are central to application lifecycle, distributed computation, and structured APIs.\n\n## 1) SparkContext\n- Location: core/src/main/scala/org/apache/spark/SparkContext.scala\n- Role: Entry point to the Spark cluster for low-level APIs. Manages the application lifecycle, configuration, resource allocation, job submission, task scheduling, and interaction with cluster managers.\n- Why its important: Almost every Spark application initializes a SparkContext (directly or indirectly via SparkSession). It orchestrates RDD creation, accumulators/broadcasts, and coordinates with the scheduler and executors.\n- Key responsibilities:\n  - Initialize and configure the Spark application (SparkConf, environment setup).\n  - Connect to cluster managers (Standalone, YARN, Mesos, Kubernetes).\n  - Create RDDs from various sources (collections, files, Hadoop input formats).\n  - Submit jobs and stages to the scheduler; track jobs via listeners and UI.\n  - Manage shared variables (Broadcast, AccumulatorV2) and plugin lifecycle.\n- Notable methods/constructs:\n  - parallelize, textFile, wholeTextFiles, sequenceFile\n  - runJob, submitJob (job submission pipeline)\n  - broadcast, accumulator\n  - getConf, defaultParallelism\n\n## 2) RDD (Resilient Distributed Dataset)\n- Location: core/src/main/scala/org/apache/spark/rdd/RDD.scala\n- Role: Fundamental immutable distributed collection with lineage-based fault tolerance. Provides transformations and actions for low-level data-parallel computations.\n- Why its important: It is the core abstraction enabling Sparks distributed processing model, caching, partitioning, and fault recovery. Many higher-level APIs (DataFrames/Datasets) ultimately compile down to RDD operations at execution time.\n- Key responsibilities:\n  - Represent data partitions and dependencies forming a DAG (lineage).\n  - Provide lazy transformations and eager actions.\n  - Expose persistence/caching semantics and partition locality hints.\n  - Facilitate compute() for partitions and preferredLocations.\n- Notable methods/constructs:\n  - Transformations: map, flatMap, filter, union, join, groupByKey, reduceByKey\n  - Actions: collect, count, first, take, reduce, saveAsTextFile\n  - persist, cache, unpersist\n  - getNumPartitions, partitions, dependencies, toDebugString\n\n## 3) SparkSession\n- Location: sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala\n- Role: Unified entry point for Spark SQL, DataFrames, Datasets, and catalog. Wraps a SparkContext and manages SQL configuration, catalog, and session state.\n- Why its important: Modern Spark applications use SparkSession as the primary API surface for structured data processing, including reading/writing data sources, SQL queries, and function registration.\n- Key responsibilities:\n  - Provide DataFrame/Dataset creation and SQL execution (sql, table, range).\n  - Manage catalog (databases, tables, views, functions) and session state.\n  - Coordinate with SharedState for cache manager, listener, and external catalog.\n  - Offer DataFrameReader/DataFrameWriter for data sources (Parquet, CSV, etc.).\n- Notable methods/constructs:\n  - builder, getOrCreate (session lifecycle)\n  - read, readStream, sql\n  - createDataFrame, udf registration via udf, catalog operations\n\n---\n\nNotes\n- These components interact closely: SparkSession holds/uses a SparkContext; DataFrame/Dataset operations compile to logical/physical plans executed on RDDs; SparkContext drives the execution on the cluster.\n- Other important classes exist (e.g., TaskSchedulerImpl, SparkEnv, Dataset, DataFrame, SQLExecution), but the three above form the backbone across core and SQL layers.",
          "answer_time": 56,
          "tokens": 0,
          "more_info": "Identified project as spark and guessed which classes are most important, chose 2 different classes comapred to mcp"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q006",
      "question": "What are 10 entities with the most number of neighbors?",
      "category": "top",
      "ground_truth_contexts": [
        "expression - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala - Metric value: 959.00 logicalplan - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala - Metric value: 874.00 utils - OBJECT - core/src/main/scala/org/apache/spark/util/Utils.scala - Metric value: 861.00 querycompilationerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala - Metric value: 789.00 rdd - CLASS - core/src/main/scala/org/apache/spark/rdd/RDD.scala - Metric value: 731.00 queryexecutionerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala - Metric value: 709.00 logging - TRAIT - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 664.00 column - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Column.scala - Metric value: 637.00 internalrow - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala - Metric value: 612.00 loginfo - METHOD - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 595.00"
      ],
      "key_entities": [
        "org/apache/spark/sql/catalyst/expressions/Expression#",
        "org/apache/spark/sql/catalyst/plans/logical/LogicalPlan#",
        "org/apache/spark/ml/impl/Utils.",
        "org/apache/spark/sql/errors/QueryCompilationErrors.",
        "org/apache/spark/rdd/RDD#",
        "org/apache/spark/sql/errors/QueryExecutionErrors.",
        "org/apache/spark/internal/Logging#",
        "org/apache/spark/sql/catalog/Column#",
        "org/apache/spark/sql/catalyst/InternalRow#",
        "org/apache/spark/internal/Logging#logInfo(+1)."
      ],
      "claude_stats": {
        "answer": "Here are the **10 entities with the most number of neighbors** in the codebase:\n\n1. **Expression** (CLASS) - 959 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala`\n\n2. **LogicalPlan** (CLASS) - 874 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala`\n\n3. **Utils** (OBJECT) - 861 neighbors\n   - Location: `core/src/main/scala/org/apache/spark/util/Utils.scala`\n\n4. **QueryCompilationErrors** (OBJECT) - 789 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala`\n\n5. **RDD** (CLASS) - 731 neighbors\n   - Location: `core/src/main/scala/org/apache/spark/rdd/RDD.scala`\n\n6. **QueryExecutionErrors** (OBJECT) - 709 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala`\n\n7. **Logging** (TRAIT) - 664 neighbors\n   - Location: `core/src/main/scala/org/apache/spark/internal/Logging.scala`\n\n8. **Column** (CLASS) - 637 neighbors\n   - Location: `sql/core/src/main/scala/org/apache/spark/sql/Column.scala`\n\n9. **InternalRow** (CLASS) - 612 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala`\n\n10. **logInfo** (METHOD) - 595 neighbors\n    - Location: `core/src/main/scala/org/apache/spark/internal/Logging.scala`\n\nThese entities are highly connected within the codebase, which suggests they play central roles in the architecture. The **Expression** and **LogicalPlan** classes, in particular, are core abstractions in Spark's SQL catalyst framework.",
        "prompt_time": 9.489,
        "answer_time": 28,
        "context_tokens": 301,
        "tokens": 0,
        "used_context": [
          "expression - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala - Metric value: 959.00 logicalplan - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala - Metric value: 874.00 utils - OBJECT - core/src/main/scala/org/apache/spark/util/Utils.scala - Metric value: 861.00 querycompilationerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala - Metric value: 789.00 rdd - CLASS - core/src/main/scala/org/apache/spark/rdd/RDD.scala - Metric value: 731.00 queryexecutionerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala - Metric value: 709.00 logging - TRAIT - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 664.00 column - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Column.scala - Metric value: 637.00 internalrow - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala - Metric value: 612.00 loginfo - METHOD - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 595.00"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# Top 10 Entities by Number of Neighbors\n\nThis document lists the 10 entities in this repository with the highest number of neighbors (incoming and outgoing connections in the code graph).\n\n- Metric: number_of_neighbors\n- Order: descending (most connected first)\n\n1. expression  CLASS  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala  959\n2. logicalplan  CLASS  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala  874\n3. utils  OBJECT  core/src/main/scala/org/apache/spark/util/Utils.scala  861\n4. querycompilationerrors  OBJECT  sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala  789\n5. rdd  CLASS  core/src/main/scala/org/apache/spark/rdd/RDD.scala  731\n6. queryexecutionerrors  OBJECT  sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala  709\n7. logging  TRAIT  core/src/main/scala/org/apache/spark/internal/Logging.scala  664\n8. column  CLASS  sql/core/src/main/scala/org/apache/spark/sql/Column.scala  637\n9. internalrow  CLASS  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala  612\n10. loginfo  METHOD  core/src/main/scala/org/apache/spark/internal/Logging.scala  595\n\nNotes\n- These values come from the static code graph analysis for this repository version.\n- \"Neighbors\" includes related entities linked through declarations, calls, and type relations, as captured by the analyzer.\n\nSuggested next step\n- The top entity is `expression` (CLASS).",
          "prompt_time": 8.03,
          "answer_time": 52,
          "context_tokens": 301,
          "used_context": [
            "expression - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala - Metric value: 959.00 logicalplan - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala - Metric value: 874.00 utils - OBJECT - core/src/main/scala/org/apache/spark/util/Utils.scala - Metric value: 861.00 querycompilationerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala - Metric value: 789.00 rdd - CLASS - core/src/main/scala/org/apache/spark/rdd/RDD.scala - Metric value: 731.00 queryexecutionerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala - Metric value: 709.00 logging - TRAIT - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 664.00 column - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Column.scala - Metric value: 637.00 internalrow - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala - Metric value: 612.00 loginfo - METHOD - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 595.00"
          ],
          "tokens": 0,
          "more_info": "Returned answer in .md file - learned it from before"
        },
        "without_mcp": {
          "answer": "# Top 10 Entities by Number of Neighbors\n\nTimestamp: 2025-12-27 13:21 (local time)\n\nThis document lists 10 entities in the Apache Spark 3.3.0 source tree that are expected to have the most related entities (neighbors). Related here broadly covers declarations, references (calls/usages), inheritance/extension, and other static relationships typically present in a code knowledge graph.\n\nNote: The repository does not ship with a precomputed entity-relationship graph, so the ranking below is based on well-established architectural roles in Spark and typical connectivity patterns of these components. It is intended to satisfy the request format (answer in a .md file) and provide a practical ranking; exact counts can vary depending on the relationship model used.\n\n1. org.apache.spark.rdd.RDD  neighbors: 300+\n2. org.apache.spark.SparkContext  neighbors: 250+\n3. org.apache.spark.sql.SparkSession  neighbors: 220+\n4. org.apache.spark.sql.DataFrame  neighbors: 200+\n5. org.apache.spark.sql.Dataset  neighbors: 200+\n6. org.apache.spark.sql.functions (package object)  neighbors: 180+\n7. org.apache.spark.scheduler.DAGScheduler  neighbors: 170+\n8. org.apache.spark.scheduler.TaskSchedulerImpl  neighbors: 160+\n9. org.apache.spark.storage.BlockManager  neighbors: 150+\n10. org.apache.spark.util.Utils  neighbors: 140+\n\nIf you need an exact, reproducible ranking from this repository, please provide (or point to) the entity graph export (e.g., JSON of nodes and edges) or enable the graph extraction step, and I will compute precise degree counts (number_of_neighbors) and update this file accordingly.",
          "answer_time": 212,
          "tokens": 0,
          "more_info": "Answer not precised, changed neighbors from question to related entities so Junie gets better context"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q007",
      "question": "How does the project manage external data connectivity and writing?",
      "category": "general",
      "parameters": {
        "kinds": [
          "CLASS",
          "METHOD",
          "TRAIT"
        ],
        "keywords": [
          "write",
          "batch",
          "stream",
          "sink",
          "source",
          "connector",
          "jdbc",
          "provider",
          "commit",
          "options"
        ],
        "top_nodes": 8,
        "max_neighbors": 4
      },
      "ground_truth_contexts": [
        "class JdbcOptionsInWrite( override val parameters: CaseInsensitiveMap[String]) extends JDBCOptions(parameters) { import JDBCOptions._ def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters)) def this(url: String, table: String, parameters: Map[String, String]) = { this(CaseInsensitiveMap(parameters ++ Map( JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table))) } require( parameters.get(JDBC_TABLE_NAME).isDefined, s\"Option '$JDBC_TABLE_NAME' is required. \" + s\"Option '$JDBC_QUERY_STRING' is not applicable while writing.\") val table = parameters(JDBC_TABLE_NAME) } object JDBCOptions { private val curId = new java.util.concurrent.atomic.AtomicLong(0L) private val jdbcOptionNames = collection.mutable.Set[String]() private def newOption(name: String): String = { jdbcOptionNames += name.toLowerCase(Locale.ROOT) name } val JDBC_URL = newOption(\"url\") val JDBC_TABLE_NAME = newOption(\"dbtable\") val JDBC_QUERY_STRING = newOption(\"query\") val JDBC_DRIVER_CLASS = newOption(\"driver\") val JDBC_PARTITION_COLUMN = newOption(\"partitionColumn\") val JDBC_LOWER_BOUND = newOption(\"lowerBound\") val JDBC_UPPER_BOUND = newOption(\"upperBound\") val JDBC_NUM_PARTITIONS = newOption(\"numPartitions\") val JDBC_QUERY_TIMEOUT = newOption(\"queryTimeout\") val JDBC_BATCH_FETCH_SIZE = newOption(\"fetchsize\") val JDBC_TRUNCATE = newOption(\"truncate\") val JDBC_CASCADE_TRUNCATE = newOption(\"cascadeTruncate\") val JDBC_CREATE_TABLE_OPTIONS = newOption(\"createTableOptions\") val JDBC_CREATE_TABLE_COLUMN_TYPES = newOption(\"createTableColumnTypes\") val JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES = newOption(\"customSchema\") val JDBC_BATCH_INSERT_SIZE = newOption(\"batchsize\") val JDBC_TXN_ISOLATION_LEVEL = newOption(\"isolationLevel\") val JDBC_SESSION_INIT_STATEMENT = newOption(\"sessionInitStatement\") val JDBC_PUSHDOWN_PREDICATE = newOption(\"pushDownPredicate\") val JDBC_PUSHDOWN_AGGREGATE = newOption(\"pushDownAggregate\") val JDBC_PUSHDOWN_LIMIT = newOption(\"pushDownLimit\") val JDBC_PUSHDOWN_TABLESAMPLE = newOption(\"pushDownTableSample\") val JDBC_KEYTAB = newOption(\"keytab\") val JDBC_PRINCIPAL = newOption(\"principal\") val JDBC_TABLE_COMMENT = newOption(\"tableComment\") val JDBC_REFRESH_KRB5_CONFIG = newOption(\"refreshKrb5Config\") val JDBC_CONNECTION_PROVIDER = newOption(\"connectionProvider\") }",
        "* with [[MicroBatchWrite]] before execution. */ case class WriteToMicroBatchDataSource( relation: Option[DataSourceV2Relation], table: SupportsWrite, query: LogicalPlan, queryId: String, writeOptions: Map[String, String], outputMode: OutputMode, batchId: Option[Long] = None) extends UnaryNode { override def child: LogicalPlan = query override def output: Seq[Attribute] = Nil def withNewBatchId(batchId: Long): WriteToMicroBatchDataSource = { copy(batchId = Some(batchId)) } override protected def withNewChildInternal(newChild: LogicalPlan): WriteToMicroBatchDataSource = copy(query = newChild) }",
        "*/ class MicroBatchWrite(eppchId: Long, val writeSupport: StreamingWrite) extends BatchWrite { override def commit(messages: Array[WriterCommitMessage]): Unit = { writeSupport.commit(eppchId, messages) } override def abort(messages: Array[WriterCommitMessage]): Unit = { writeSupport.abort(eppchId, messages) } override def createBatchWriterFactory(info: PhysicalWriteInfo): DataWriterFactory = { new MicroBatchWriterFactory(eppchId, writeSupport.createStreamingWriterFactory(info)) } } class MicroBatchWriterFactory(epochId: Long, streamingWriterFactory: StreamingDataWriterFactory) extends DataWriterFactory { override def createWriter(partitionId: Int, taskId: Long): DataWriter[InternalRow] = { streamingWriterFactory.createWriter(partitionId, taskId, epochId) } }",
        "*/ class JDBCOptions( val parameters: CaseInsensitiveMap[String]) extends Serializable with Logging { import JDBCOptions._ def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters)) def this(url: String, table: String, parameters: Map[String, String]) = { this(CaseInsensitiveMap(parameters ++ Map( JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table))) } /** * Returns a property with all options. */ val asProperties: Properties = { val properties = new Properties() parameters.originalMap.foreach { case (k, v) => properties.setProperty(k, v) } properties } /** * Returns a property with all options except Spark internal data source options like `url`, * `dbtable`, and `numPartition`. This should be used when invoking JDBC API like `Driver.connect` * because each DBMS vendor has its own property list for JDBC driver. See SPARK-17776. */ val asConnectionProperties: Properties = { val properties = new Properties() parameters.originalMap.filterKeys(key => !jdbcOptionNames(key.toLowerCase(Locale.ROOT))) .foreach { case (k, v) => properties.setProperty(k, v) } properties } // ------------------------------------------------------------ // Required parameters // ------------------------------------------------------------ require(parameters.isDefinedAt(JDBC_URL), s\"Option '$JDBC_URL' is required.\") // a JDBC URL val url = parameters(JDBC_URL) // table name or a table subquery. val tableOrQuery = (parameters.get(JDBC_TABLE_NAME), parameters.get(JDBC_QUERY_STRING)) match { case (Some(name), Some(subquery)) => throw QueryExecutionErrors.cannotSpecifyBothJdbcTableNameAndQueryError( JDBC_TABLE_NAME, JDBC_QUERY_STRING) case (None, None) => throw QueryExecutionErrors.missingJdbcTableNameAndQueryError( JDBC_TABLE_NAME, JDBC_QUERY_STRING) case (Some(name), None) => if (name.isEmpty) { throw QueryExecutionErrors.emptyOptionError(JDBC_TABLE_NAME) } else { name.trim } case (None, Some(subquery)) => if (subquery.isEmpty) { throw QueryExecutionErrors.emptyOptionError(JDBC_QUERY_STRING) } else { s\"(${subquery}) SPARK_GEN_SUBQ_${curId.getAndIncrement()}\" } } // ------------------------------------------------------------ // Optional parameters // ------------------------------------------------------------ val driverClass = { val userSpecifiedDriverClass = parameters.get(JDBC_DRIVER_CLASS) userSpecifiedDriverClass.foreach(DriverRegistry.register) // Performing this part of the logic on the driver guards against the corner-case where the // driver returned for a URL is different on the driver and executors due to classpath // differences. userSpecifiedDriverClass.getOrElse { DriverManager.getDriver(url).getClass.getCanonicalName } } // the number of partitions val numPartitions = parameters.get(JDBC_NUM_PARTITIONS).map(_.toInt) // the number of seconds the driver will wait for a Statement object to execute to the given // number of seconds. Zero means there is no limit. val queryTimeout = parameters.getOrElse(JDBC_QUERY_TIMEOUT, \"0\").toInt // ------------------------------------------------------------ // Optional parameters only for reading // ------------------------------------------------------------ // the column used to partition val partitionColumn = parameters.get(JDBC_PARTITION_COLUMN) // the lower bound of partition column val lowerBound = parameters.get(JDBC_LOWER_BOUND) // the upper bound of the partition column val upperBound = parameters.get(JDBC_UPPER_BOUND) // numPartitions is also used for data source writing require((partitionColumn.isEmpty && lowerBound.isEmpty && upperBound.isEmpty) || (partitionColumn.isDefined && lowerBound.isDefined && upperBound.isDefined && numPartitions.isDefined), s\"When reading JDBC data sources, users need to specify all or none for the following \" + s\"options: '$JDBC_PARTITION_COLUMN', '$JDBC_LOWER_BOUND', '$JDBC_UPPER_BOUND', \" + s\"and '$JDBC_NUM_PARTITIONS'\") require(!(parameters.get(JDBC_QUERY_STRING).isDefined && partitionColumn.isDefined), s\"\"\" |Options '$JDBC_QUERY_STRING' and '$JDBC_PARTITION_COLUMN' can not be specified together. |Please define the query using `$JDBC_TABLE_NAME` option instead and make sure to qualify |the partition columns using the supplied subquery alias to resolve any ambiguity. |Example : |spark.read.format(\"jdbc\") | .option(\"url\", jdbcUrl) | .option(\"dbtable\", \"(select c1, c2 from t1) as subq\") | .option(\"partitionColumn\", \"c1\") | .option(\"lowerBound\", \"1\") | .option(\"upperBound\", \"100\") | .option(\"numPartitions\", \"3\") | .load() \"\"\".stripMargin ) val fetchSize = parameters.getOrElse(JDBC_BATCH_FETCH_SIZE, \"0\").toInt // ------------------------------------------------------------ // Optional parameters only for writing // ------------------------------------------------------------ // if to truncate the table from the JDBC database val isTruncate = parameters.getOrElse(JDBC_TRUNCATE, \"false\").toBoolean val isCascadeTruncate: Option[Boolean] = parameters.get(JDBC_CASCADE_TRUNCATE).map(_.toBoolean) // the create table option , which can be table_options or partition_options. // E.g., \"CREATE TABLE t (name string) ENGINE=InnoDB DEFAULT CHARSET=utf8\" // TODO: to reuse the existing partition parameters for those partition specific options val createTableOptions = parameters.getOrElse(JDBC_CREATE_TABLE_OPTIONS, \"\") val createTableColumnTypes = parameters.get(JDBC_CREATE_TABLE_COLUMN_TYPES) val customSchema = parameters.get(JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES) val batchSize = { val size = parameters.getOrElse(JDBC_BATCH_INSERT_SIZE, \"1000\").toInt require(size >= 1, s\"Invalid value `${size.toString}` for parameter \" + s\"`$JDBC_BATCH_INSERT_SIZE`. The minimum value is 1.\") size } val isolationLevel = parameters.getOrElse(JDBC_TXN_ISOLATION_LEVEL, \"READ_UNCOMMITTED\") match { case \"NONE\" => Connection.TRANSACTION_NONE case \"READ_UNCOMMITTED\" => Connection.TRANSACTION_READ_UNCOMMITTED case \"READ_COMMITTED\" => Connection.TRANSACTION_READ_COMMITTED case \"REPEATABLE_READ\" => Connection.TRANSACTION_REPEATABLE_READ case \"SERIALIZABLE\" => Connection.TRANSACTION_SERIALIZABLE case other => throw QueryExecutionErrors.invalidJdbcTxnIsolationLevelError( JDBC_TXN_ISOLATION_LEVEL, other) } // An option to execute custom SQL before fetching data from the remote DB val sessionInitStatement = parameters.get(JDBC_SESSION_INIT_STATEMENT) // An option to allow/disallow pushing down predicate into JDBC data source val pushDownPredicate = parameters.getOrElse(JDBC_PUSHDOWN_PREDICATE, \"true\").toBoolean // An option to allow/disallow pushing down aggregate into JDBC data source // This only applies to Data Source V2 JDBC val pushDownAggregate = parameters.getOrElse(JDBC_PUSHDOWN_AGGREGATE, \"false\").toBoolean // An option to allow/disallow pushing down LIMIT into V2 JDBC data source // This only applies to Data Source V2 JDBC val pushDownLimit = parameters.getOrElse(JDBC_PUSHDOWN_LIMIT, \"false\").toBoolean // An option to allow/disallow pushing down TABLESAMPLE into JDBC data source // This only applies to Data Source V2 JDBC val pushDownTableSample = parameters.getOrElse(JDBC_PUSHDOWN_TABLESAMPLE, \"false\").toBoolean // The local path of user's keytab file, which is assumed to be pre-uploaded to all nodes either // by --files option of spark-submit or manually val keytab = { val keytabParam = parameters.getOrElse(JDBC_KEYTAB, null) if (keytabParam != null && FilenameUtils.getPath(keytabParam).isEmpty) { val result = SparkFiles.get(keytabParam) logDebug(s\"Keytab path not found, assuming --files, file name used on executor: $result\") result } else { logDebug(\"Keytab path found, assuming manual upload\") keytabParam } } // The principal name of user's keytab file val principal = parameters.getOrElse(JDBC_PRINCIPAL, null) val tableComment = parameters.getOrElse(JDBC_TABLE_COMMENT, \"\") val refreshKrb5Config = parameters.getOrElse(JDBC_REFRESH_KRB5_CONFIG, \"false\").toBoolean // User specified JDBC connection provider name val connectionProviderName = parameters.get(JDBC_CONNECTION_PROVIDER) } class JdbcOptionsInWrite( override val parameters: CaseInsensitiveMap[String]) extends JDBCOptions(parameters) { import JDBCOptions._ def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters)) def this(url: String, table: String, parameters: Map[String, String]) = { this(CaseInsensitiveMap(parameters ++ Map( JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table))) } require( parameters.get(JDBC_TABLE_NAME).isDefined, s\"Option '$JDBC_TABLE_NAME' is required. \" + s\"Option '$JDBC_QUERY_STRING' is not applicable while writing.\") val table = parameters(JDBC_TABLE_NAME) } object JDBCOptions { private val curId = new java.util.concurrent.atomic.AtomicLong(0L) private val jdbcOptionNames = collection.mutable.Set[String]() private def newOption(name: String): String = { jdbcOptionNames += name.toLowerCase(Locale.ROOT) name } val JDBC_URL = newOption(\"url\") val JDBC_TABLE_NAME = newOption(\"dbtable\") val JDBC_QUERY_STRING = newOption(\"query\") val JDBC_DRIVER_CLASS = newOption(\"driver\") val JDBC_PARTITION_COLUMN = newOption(\"partitionColumn\") val JDBC_LOWER_BOUND = newOption(\"lowerBound\") val JDBC_UPPER_BOUND = newOption(\"upperBound\") val JDBC_NUM_PARTITIONS = newOption(\"numPartitions\") val JDBC_QUERY_TIMEOUT = newOption(\"queryTimeout\") val JDBC_BATCH_FETCH_SIZE = newOption(\"fetchsize\") val JDBC_TRUNCATE = newOption(\"truncate\") val JDBC_CASCADE_TRUNCATE = newOption(\"cascadeTruncate\") val JDBC_CREATE_TABLE_OPTIONS = newOption(\"createTableOptions\") val JDBC_CREATE_TABLE_COLUMN_TYPES = newOption(\"createTableColumnTypes\") val JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES = newOption(\"customSchema\") val JDBC_BATCH_INSERT_SIZE = newOption(\"batchsize\") val JDBC_TXN_ISOLATION_LEVEL = newOption(\"isolationLevel\") val JDBC_SESSION_INIT_STATEMENT = newOption(\"sessionInitStatement\") val JDBC_PUSHDOWN_PREDICATE = newOption(\"pushDownPredicate\") val JDBC_PUSHDOWN_AGGREGATE = newOption(\"pushDownAggregate\") val JDBC_PUSHDOWN_LIMIT = newOption(\"pushDownLimit\") val JDBC_PUSHDOWN_TABLESAMPLE = newOption(\"pushDownTableSample\") val JDBC_KEYTAB = newOption(\"keytab\") val JDBC_PRINCIPAL = newOption(\"principal\") val JDBC_TABLE_COMMENT = newOption(\"tableComment\") val JDBC_REFRESH_KRB5_CONFIG = newOption(\"refreshKrb5Config\") val JDBC_CONNECTION_PROVIDER = newOption(\"connectionProvider\") }"
      ],
      "key_entities": [
        "org/apache/spark/sql/execution/datasources/jdbc/JdbcOptionsInWrite#",
        "org/apache/spark/sql/execution/streaming/sources/WriteToMicroBatchDataSource#",
        "org/apache/spark/sql/execution/streaming/sources/MicroBatchWrite#",
        "org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions#",
        " org/apache/spark/sql/execution/datasources/v2/StreamWriterCommitProgress#"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, here's how this project (Apache Spark SQL) manages external data connectivity and writing:\n\n## Responsibilities\n\nThe project implements a distributed file-based data writing system using Hadoop's file system APIs. The main entry point is `FileFormatWriter.write()`, which orchestrates writing data across multiple executor tasks in a Spark job. The system handles partitioned/bucketed data, manages concurrent writers, supports atomic file operations through checkpoint managers, and provides extensible output writer factories for different file formats.\n\n## Data Flow\n\n**Driver-side setup:**\n1. `FileFormatWriter.write()` receives a `SparkPlan`, `FileFormat`, `FileCommitProtocol`, and output specifications\n2. Creates a Hadoop `Job` instance and configures it with output path and serialization classes\n3. The `FileFormat.prepareWrite()` creates an `OutputWriterFactory` that will be used by executors\n4. Builds a `WriteJobDescription` containing serialized Hadoop configuration, writer factory, column schemas, partitioning/bucketing specs, and statistics trackers\n5. Determines if input data needs sorting based on `requiredOrdering` (partition columns + bucket id + sort columns) vs actual ordering\n6. Calls `committer.setupJob(job)` to initialize the commit protocol\n\n**Executor-side writing:**\n7. Each executor task runs `executeTask()` which:\n   - Creates Hadoop `TaskAttemptContext` with job/task IDs\n   - Calls `committer.setupTask(taskAttemptContext)`\n   - Instantiates appropriate `FileFormatDataWriter` based on partitioning:\n     - `EmptyDirectoryDataWriter` for empty partitions (just metadata)\n     - `SingleDirectoryDataWriter` for non-partitioned data\n     - `DynamicPartitionDataSingleWriter` for partitioned data with sequential writing\n     - `DynamicPartitionDataConcurrentWriter` for partitioned data with concurrent writers\n8. The writer's `writeWithIterator()` processes rows:\n   - For partitioned data, extracts partition values using `getPartitionValues()`\n   - For bucketed data, calculates bucket ID using `getBucketId()`\n   - When partition/bucket changes, calls `renewCurrentWriter()` which:\n     - Closes current `OutputWriter` via `releaseCurrentWriter()`\n     - Builds partition path string using `getPartitionPath()`\n     - Gets new file path from `committer.newTaskTempFile()` with appropriate prefix/suffix\n     - Creates new `OutputWriter` via `outputWriterFactory.newInstance()`\n   - Transforms row to output columns using `getOutputRow()`\n   - Writes row via `currentWriter.write(outputRow)`\n   - Updates statistics trackers for each row\n   - Checks `maxRecordsPerFile` limit and rotates files if exceeded\n9. After all rows written, calls `dataWriter.commit()` which:\n   - Calls `committer.commitTask()` returning `TaskCommitMessage`\n   - Returns `WriteTaskResult` with commit message and execution summary\n\n**Driver-side finalization:**\n10. Driver collects all `WriteTaskResult` objects from executor tasks\n11. Calls `committer.onTaskCommit(res.commitMsg)` for each task result\n12. Calls `committer.commitJob(job, commitMsgs)` to finalize all writes\n13. Processes statistics via `processStats()` method\n14. Returns set of updated partition paths\n\n## Key Methods and Logic\n\n**FileFormatWriter.write():**\n- Validates output schema with `DataSourceUtils.verifySchema()`\n- Handles empty2null conversion for partition columns if needed\n- Determines bucketing specification (Hive-compatible using `HiveHash` + bitwise-and, or Spark bucketing using `HashPartitioning`)\n- Checks if data ordering matches requirements to avoid unnecessary sorting\n- For concurrent writers: wraps unsorted execution with `ConcurrentOutputWriterSpec` containing max writer count and sorter factory\n- Handles zero-partition RDDs by creating dummy single partition\n- Uses `sparkContext.runJob()` to execute write tasks in parallel\n- Wraps entire operation in try-catch, calling `committer.abortJob()` on failure\n\n**executeTask():**\n- Creates Hadoop job/task/attempt IDs using current timestamp and Spark context IDs\n- Configures Hadoop context with mapreduce job/task properties\n- Selects writer implementation based on: partition emptiness, partitioning scheme, and concurrent writer spec availability\n- Uses `Utils.tryWithSafeFinallyAndFailureCallbacks` for proper resource cleanup\n- On failure: calls `dataWriter.abort()` and `committer.abortTask()`\n- Catches `FetchFailedException` separately to allow Spark retry logic\n- Wraps `FileAlreadyExistsException` in `TaskOutputFileAlreadyExistException` for fast-fail behavior\n\n**BaseDynamicPartitionDataWriter:**\n- `renewCurrentWriter()` handles writer rotation by:\n  - Resetting `recordsInFile` counter\n  - Closing previous writer if `closeCurrentWriter` flag is true\n  - Computing partition directory path from partition values\n  - Adding partition to `updatedPartitions` set\n  - Building file name with bucket ID prefix/suffix and file counter\n  - Handling custom partition locations from `description.customPartitionLocations`\n  - Creating temp file path via commit protocol\n  - Instantiating new writer from factory\n  - Notifying stats trackers of new file\n\n**DynamicPartitionDataConcurrentWriter:**\n- Maintains `concurrentWriters` map keyed by `WriterIndex(partitionValues, bucketId)`\n- In `write()`: compares current vs next partition/bucket, updates map or releases writer based on `sorted` flag\n- When concurrent writer limit reached: sets `sorted = true` and switches to sort-based sequential mode\n- In `writeWithIterator()`: writes concurrently until limit hit, then sorts remaining rows with `sorter.sort()` and continues sequentially\n- `setupCurrentWriterUsingMap()` either retrieves existing writer from map or creates new one\n- `updateCurrentWriterStatusInMap()` saves current writer state (`outputWriter`, `recordsInFile`, `fileCounter`) back to map\n\n**CheckpointFileManager implementations:**\n- `RenameBasedFSDataOutputStream` provides atomic writes via:\n  - Writing to temp file first via `fm.createTempFile(tempPath)`\n  - On `close()`: renames temp to final path using `fm.renameTempFile()`\n  - On `cancel()`: deletes temp file\n  - Uses `terminated` flag to ensure single execution\n- `FileSystemBasedCheckpointFileManager`: uses Hadoop `FileSystem` API\n  - `renameTempFile()` calls `fs.rename()` and handles ambiguous false return by checking existence\n- `FileContextBasedCheckpointFileManager`: uses Hadoop `FileContext` API  \n  - `renameTempFile()` uses `fc.rename()` with `OVERWRITE` or `NONE` option\n  - Includes workaround to remove CRC checksum files after rename (HADOOP-16255)\n\n**OutputWriterFactory & OutputWriter:**\n- `OutputWriterFactory` is format-specific factory that returns file extension and creates `OutputWriter` instances\n- `OutputWriter` is the actual writer with three operations:\n  - `write(row: InternalRow)`: persists single row\n  - `close()`: closes writer after all rows written\n  - `path()`: returns file path being written\n\nThe system provides strong consistency through two-phase commit protocol (task commit  job commit), handles failures via abort mechanisms, supports both sequential and concurrent writing strategies, and abstracts file format specifics through the factory pattern.",
        "prompt_time": 17,
        "answer_time": 82,
        "context_tokens": 9162,
        "tokens": 0,
        "used_context": [
          "## CLASS: org/apache/spark/sql/execution/datasources/FileFormatWriter.ConcurrentOutputWriterSpec# (implementation)\n/** Describes how concurrent output writers should be executed. */ case class ConcurrentOutputWriterSpec( maxWriters: Int, createSorter: () => UnsafeExternalRowSorter) /** * Basic work flow of this command is: * 1. Driver side setup, including output committer initialization and data source specific * preparation work for the write job to be issued. * 2. Issues a write job consists of one or more executor side tasks, each of which writes all * rows within an RDD partition. * 3. If no exception is thrown in a task, commits that task, otherwise aborts that task; If any * exception is thrown during task commitment, also aborts that task. * 4. If all tasks are committed, commit the job, otherwise aborts the job; If any exception is * thrown during job commitment, also aborts the job. * 5. If the job is successfully committed, perform post-commit operations such as * processing statistics. * @return The set of all partition paths that were updated during this write job. */ def write( sparkSession: SparkSession, plan: SparkPlan, fileFormat: FileFormat, committer: FileCommitProtocol, outputSpec: OutputSpec, hadoopConf: Configuration, partitionColumns: Seq[Attribute], bucketSpec: Option[BucketSpec], statsTrackers: Seq[WriteJobStatsTracker], options: Map[String, String]) : Set[String] = { val job = Job.getInstance(hadoopConf) job.setOutputKeyClass(classOf[Void]) job.setOutputValueClass(classOf[InternalRow]) FileOutputFormat.setOutputPath(job, new Path(outputSpec.outputPath)) val partitionSet = AttributeSet(partitionColumns) // cleanup the internal metadata information of // the file source metadata attribute if any before write out val finalOutputSpec = outputSpec.copy(outputColumns = outputSpec.outputColumns .map(FileSourceMetadataAttribute.cleanupFileSourceMetadataInformation)) val dataColumns = finalOutputSpec.outputColumns.filterNot(partitionSet.contains) var needConvert = false val projectList: Seq[NamedExpression] = plan.output.map { case p if partitionSet.contains(p) && p.dataType == StringType && p.nullable => needConvert = true Alias(Empty2Null(p), p.name)() case attr => attr } val empty2NullPlan = if (needConvert) ProjectExec(projectList, plan) else plan val writerBucketSpec = bucketSpec.map { spec => val bucketColumns = spec.bucketColumnNames.map(c => dataColumns.find(_.name == c).get) if (options.getOrElse(BucketingUtils.optionForHiveCompatibleBucketWrite, \"false\") == \"true\") { // Hive bucketed table: use `HiveHash` and bitwise-and as bucket id expression. // Without the extra bitwise-and operation, we can get wrong bucket id when hash value of // columns is negative. See Hive implementation in // `org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#getBucketNumber()`. val hashId = BitwiseAnd(HiveHash(bucketColumns), Literal(Int.MaxValue)) val bucketIdExpression = Pmod(hashId, Literal(spec.numBuckets)) // The bucket file name prefix is following Hive, Presto and Trino conversion, so this // makes sure Hive bucketed table written by Spark, can be read by other SQL engines. // // Hive: `org.apache.hadoop.hive.ql.exec.Utilities#getBucketIdFromFile()`. // Trino: `io.trino.plugin.hive.BackgroundHiveSplitLoader#BUCKET_PATTERNS`. val fileNamePrefix = (bucketId: Int) => f\"$bucketId%05d_0_\" WriterBucketSpec(bucketIdExpression, fileNamePrefix) } else { // Spark bucketed table: use `HashPartitioning.partitionIdExpression` as bucket id // expression, so that we can guarantee the data distribution is same between shuffle and // bucketed data source, which enables us to only shuffle one side when join a bucketed // table and a normal one. val bucketIdExpression = HashPartitioning(bucketColumns, spec.numBuckets) .partitionIdExpression WriterBucketSpec(bucketIdExpression, (_: Int) => \"\") } } val sortColumns = bucketSpec.toSeq.flatMap { spec => spec.sortColumnNames.map(c => dataColumns.find(_.name == c).get) } val caseInsensitiveOptions = CaseInsensitiveMap(options) val dataSchema = dataColumns.toStructType DataSourceUtils.verifySchema(fileFormat, dataSchema) // Note: prepareWrite has side effect. It sets \"job\". val outputWriterFactory = fileFormat.prepareWrite(sparkSession, job, caseInsensitiveOptions, dataSchema) val description = new WriteJobDescription( uuid = UUID.randomUUID.toString, serializableHadoopConf = new SerializableConfiguration(job.getConfiguration), outputWriterFactory = outputWriterFactory, allColumns = finalOutputSpec.outputColumns, dataColumns = dataColumns, partitionColumns = partitionColumns, bucketSpec = writerBucketSpec, path = finalOutputSpec.outputPath, customPartitionLocations = finalOutputSpec.customPartitionLocations, maxRecordsPerFile = caseInsensitiveOptions.get(\"maxRecordsPerFile\").map(_.toLong) .getOrElse(sparkSession.sessionState.conf.maxRecordsPerFile), timeZoneId = caseInsensitiveOptions.get(DateTimeUtils.TIMEZONE_OPTION) .getOrElse(sparkSession.sessionState.conf.sessionLocalTimeZone), statsTrackers = statsTrackers ) // We should first sort by partition columns, then bucket id, and finally sorting columns. val requiredOrdering = partitionColumns ++ writerBucketSpec.map(_.bucketIdExpression) ++ sortColumns // the sort order doesn't matter val actualOrdering = empty2NullPlan.outputOrdering.map(_.child) val orderingMatched = if (requiredOrdering.length > actualOrdering.length) { false } else { requiredOrdering.zip(actualOrdering).forall { case (requiredOrder, childOutputOrder) => requiredOrder.semanticEquals(childOutputOrder) } } SQLExecution.checkSQLExecutionId(sparkSession) // propagate the description UUID into the jobs, so that committers // get an ID guaranteed to be unique. job.getConfiguration.set(\"spark.sql.sources.writeJobUUID\", description.uuid) // This call shouldn't be put into the `try` block below because it only initializes and // prepares the job, any exception thrown from here shouldn't cause abortJob() to be called. committer.setupJob(job) try { val (rdd, concurrentOutputWriterSpec) = if (orderingMatched) { (empty2NullPlan.execute(), None) } else { // SPARK-21165: the `requiredOrdering` is based on the attributes from analyzed plan, and // the physical plan may have different attribute ids due to optimizer removing some // aliases. Here we bind the expression ahead to avoid potential attribute ids mismatch. val orderingExpr = bindReferences( requiredOrdering.map(SortOrder(_, Ascending)), finalOutputSpec.outputColumns) val sortPlan = SortExec( orderingExpr, global = false, child = empty2NullPlan) val maxWriters = sparkSession.sessionState.conf.maxConcurrentOutputFileWriters val concurrentWritersEnabled = maxWriters > 0 && sortColumns.isEmpty if (concurrentWritersEnabled) { (empty2NullPlan.execute(), Some(ConcurrentOutputWriterSpec(maxWriters, () => sortPlan.createSorter()))) } else { (sortPlan.execute(), None) } } // SPARK-23271 If we are attempting to write a zero partition rdd, create a dummy single // partition rdd to make sure we at least set up one write task to write the metadata. val rddWithNonEmptyPartitions = if (rdd.partitions.length == 0) { sparkSession.sparkContext.parallelize(Array.empty[InternalRow], 1) } else { rdd } val jobIdInstant = new Date().getTime val ret = new Array[WriteTaskResult](rddWithNonEmptyPartitions.partitions.length) sparkSession.sparkContext.runJob( rddWithNonEmptyPartitions, (taskContext: TaskContext, iter: Iterator[InternalRow]) => { executeTask( description = description, jobIdInstant = jobIdInstant, sparkStageId = taskContext.stageId(), sparkPartitionId = taskContext.partitionId(), sparkAttemptNumber = taskContext.taskAttemptId().toInt & Integer.MAX_VALUE, committer, iterator = iter, concurrentOutputWriterSpec = concurrentOutputWriterSpec) }, rddWithNonEmptyPartitions.partitions.indices, (index, res: WriteTaskResult) => { committer.onTaskCommit(res.commitMsg) ret(index) = res }) val commitMsgs = ret.map(_.commitMsg) logInfo(s\"Start to commit write Job ${description.uuid}.\") val (_, duration) = Utils.timeTakenMs { committer.commitJob(job, commitMsgs) } logInfo(s\"Write Job ${description.uuid} committed. Elapsed time: $duration ms.\") processStats(description.statsTrackers, ret.map(_.summary.stats), duration) logInfo(s\"Finished processing stats for write job ${description.uuid}.\") // return a set of all the partition paths that were updated during this job ret.map(_.summary.updatedPartitions).reduceOption(_ ++ _).getOrElse(Set.empty) } catch { case cause: Throwable => logError(s\"Aborting job ${description.uuid}.\", cause) committer.abortJob(job) throw QueryExecutionErrors.jobAbortedError(cause) } } /** Writes data out in a single Spark task. */ private def executeTask( description: WriteJobDescription, jobIdInstant: Long, sparkStageId: Int, sparkPartitionId: Int, sparkAttemptNumber: Int, committer: FileCommitProtocol, iterator: Iterator[InternalRow], concurrentOutputWriterSpec: Option[ConcurrentOutputWriterSpec]): WriteTaskResult = { val jobId = SparkHadoopWriterUtils.createJobID(new Date(jobIdInstant), sparkStageId) val taskId = new TaskID(jobId, TaskType.MAP, sparkPartitionId) val taskAttemptId = new TaskAttemptID(taskId, sparkAttemptNumber) // Set up the attempt context required to use in the output committer. val taskAttemptContext: TaskAttemptContext = { // Set up the configuration object val hadoopConf = description.serializableHadoopConf.value hadoopConf.set(\"mapreduce.job.id\", jobId.toString) hadoopConf.set(\"mapreduce.task.id\", taskAttemptId.getTaskID.toString) hadoopConf.set(\"mapreduce.task.attempt.id\", taskAttemptId.toString) hadoopConf.setBoolean(\"mapreduce.task.ismap\", true) hadoopConf.setInt(\"mapreduce.task.partition\", 0) new TaskAttemptContextImpl(hadoopConf, taskAttemptId) } committer.setupTask(taskAttemptContext) val dataWriter = if (sparkPartitionId != 0 && !iterator.hasNext) { // In case of empty job, leave first partition to save meta for file format like parquet. new EmptyDirectoryDataWriter(description, taskAttemptContext, committer) } else if (description.partitionColumns.isEmpty && description.bucketSpec.isEmpty) { new SingleDirectoryDataWriter(description, taskAttemptContext, committer) } else { concurrentOutputWriterSpec match { case Some(spec) => new DynamicPartitionDataConcurrentWriter( description, taskAttemptContext, committer, spec) case _ => new DynamicPartitionDataSingleWriter(description, taskAttemptContext, committer) } } try { Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Execute the task to write rows out and commit the task. dataWriter.writeWithIterator(iterator) dataWriter.commit() })(catchBlock = { // If there is an error, abort the task dataWriter.abort() logError(s\"Job $jobId aborted.\") }, finallyBlock = { dataWriter.close() }) } catch { case e: FetchFailedException => throw e case f: FileAlreadyExistsException if SQLConf.get.fastFailFileFormatOutput => // If any output file to write already exists, it does not make sense to re-run this task. // We throw the exception and let Executor throw ExceptionFailure to abort the job. throw new TaskOutputFileAlreadyExistException(f) case t: Throwable => throw QueryExecutionErrors.taskFailedWhileWritingRowsError(t) } } /** * For every registered [[WriteJobStatsTracker]], call `processStats()` on it, passing it * the corresponding [[WriteTaskStats]] from all executors. */ private[datasources] def processStats( statsTrackers: Seq[WriteJobStatsTracker], statsPerTask: Seq[Seq[WriteTaskStats]], jobCommitDuration: Long) : Unit = { val numStatsTrackers = statsTrackers.length assert(statsPerTask.forall(_.length == numStatsTrackers), s\"\"\"Every WriteTask should have produced one `WriteTaskStats` object for every tracker. |There are $numStatsTrackers statsTrackers, but some task returned |${statsPerTask.find(_.length != numStatsTrackers).get.length} results instead. \"\"\".stripMargin) val statsPerTracker = if (statsPerTask.nonEmpty) { statsPerTask.transpose } else { statsTrackers.map(_ => Seq.empty) } statsTrackers.zip(statsPerTracker).foreach { case (statsTracker, stats) => statsTracker.processStats(stats, jobCommitDuration) } } }",
          "## CLASS: org/apache/spark/sql/execution/datasources/OutputWriterFactory# (implementation)\n*/ abstract class OutputWriterFactory extends Serializable { /** Returns the file extension to be used when writing files out. */ def getFileExtension(context: TaskAttemptContext): String /** * When writing to a [[HadoopFsRelation]], this method gets called by each task on executor side * to instantiate new [[OutputWriter]]s. * * @param path Path to write the file. * @param dataSchema Schema of the rows to be written. Partition columns are not included in the * schema if the relation being written is partitioned. * @param context The Hadoop MapReduce task context. */ def newInstance( path: String, dataSchema: StructType, context: TaskAttemptContext): OutputWriter } /** * [[OutputWriter]] is used together with [[HadoopFsRelation]] for persisting rows to the * underlying file system. Subclasses of [[OutputWriter]] must provide a zero-argument constructor. * An [[OutputWriter]] instance is created and initialized when a new output file is opened on * executor side. This instance is used to persist rows to this single output file. */ abstract class OutputWriter { /** * Persists a single row. Invoked on the executor side. When writing to dynamically partitioned * tables, dynamic partition columns are not included in rows to be written. */ def write(row: InternalRow): Unit /** * Closes the [[OutputWriter]]. Invoked on the executor side after all rows are persisted, before * the task output is committed. */ def close(): Unit /** * The file path to write. Invoked on the executor side. */ def path(): String }",
          "## CLASS: org/apache/spark/sql/execution/datasources/OutputWriter# (implementation)\n*/ abstract class OutputWriter { /** * Persists a single row. Invoked on the executor side. When writing to dynamically partitioned * tables, dynamic partition columns are not included in rows to be written. */ def write(row: InternalRow): Unit /** * Closes the [[OutputWriter]]. Invoked on the executor side after all rows are persisted, before * the task output is committed. */ def close(): Unit /** * The file path to write. Invoked on the executor side. */ def path(): String }",
          "## CLASS: org/apache/spark/sql/execution/streaming/CheckpointFileManager.RenameBasedFSDataOutputStream# (implementation)\n*/ sealed class RenameBasedFSDataOutputStream( fm: CheckpointFileManager with RenameHelperMethods, finalPath: Path, tempPath: Path, overwriteIfPossible: Boolean) extends CancellableFSDataOutputStream(fm.createTempFile(tempPath)) { def this(fm: CheckpointFileManager with RenameHelperMethods, path: Path, overwrite: Boolean) = { this(fm, path, generateTempPath(path), overwrite) } logInfo(s\"Writing atomically to $finalPath using temp file $tempPath\") @volatile private var terminated = false override def close(): Unit = synchronized { try { if (terminated) return underlyingStream.close() try { fm.renameTempFile(tempPath, finalPath, overwriteIfPossible) } catch { case fe: FileAlreadyExistsException => logWarning( s\"Failed to rename temp file $tempPath to $finalPath because file exists\", fe) if (!overwriteIfPossible) throw fe } logInfo(s\"Renamed temp file $tempPath to $finalPath\") } finally { terminated = true } } override def cancel(): Unit = synchronized { try { if (terminated) return try { underlyingStream.close() } catch { case NonFatal(e) => logWarning(s\"Error cancelling write to $finalPath, \" + s\"continuing to delete temp path $tempPath\", e) } fm.delete(tempPath) } catch { case NonFatal(e) => logWarning(s\"Error deleting temp file $tempPath\", e) } finally { terminated = true } } } /** Create an instance of [[CheckpointFileManager]] based on the path and configuration. */ def create(path: Path, hadoopConf: Configuration): CheckpointFileManager = { val fileManagerClass = hadoopConf.get( SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key) if (fileManagerClass != null) { return Utils.classForName(fileManagerClass) .getConstructor(classOf[Path], classOf[Configuration]) .newInstance(path, hadoopConf) .asInstanceOf[CheckpointFileManager] } try { // Try to create a manager based on `FileContext` because HDFS's `FileContext.rename() // gives atomic renames, which is what we rely on for the default implementation // `CheckpointFileManager.createAtomic`. new FileContextBasedCheckpointFileManager(path, hadoopConf) } catch { case e: UnsupportedFileSystemException => logWarning( \"Could not use FileContext API for managing Structured Streaming checkpoint files at \" + s\"$path. Using FileSystem API instead for managing log files. If the implementation \" + s\"of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of\" + s\"your Structured Streaming is not guaranteed.\") new FileSystemBasedCheckpointFileManager(path, hadoopConf) } } private def generateTempPath(path: Path): Path = { val tc = org.apache.spark.TaskContext.get val tid = if (tc != null) \".TID\" + tc.taskAttemptId else \"\" new Path(path.getParent, s\".${path.getName}.${UUID.randomUUID}${tid}.tmp\") } } /** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileSystem]] API. */ class FileSystemBasedCheckpointFileManager(path: Path, hadoopConf: Configuration) extends CheckpointFileManager with RenameHelperMethods with Logging { import CheckpointFileManager._ protected val fs = path.getFileSystem(hadoopConf) override def list(path: Path, filter: PathFilter): Array[FileStatus] = { fs.listStatus(path, filter) } override def mkdirs(path: Path): Unit = { fs.mkdirs(path, FsPermission.getDirDefault) } override def createTempFile(path: Path): FSDataOutputStream = { fs.create(path, true) } override def createAtomic( path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = { new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible) } override def open(path: Path): FSDataInputStream = { fs.open(path) } override def exists(path: Path): Boolean = try { fs.getFileStatus(path) != null } catch { case _: FileNotFoundException => false } override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = { if (!overwriteIfPossible && fs.exists(dstPath)) { throw QueryExecutionErrors.renamePathAsExistsPathError(srcPath, dstPath) } if (!fs.rename(srcPath, dstPath)) { // FileSystem.rename() returning false is very ambiguous as it can be for many reasons. // This tries to make a best effort attempt to return the most appropriate exception. if (fs.exists(dstPath)) { if (!overwriteIfPossible) { throw QueryExecutionErrors.renameAsExistsPathError(dstPath) } } else if (!fs.exists(srcPath)) { throw QueryExecutionErrors.renameSrcPathNotFoundError(srcPath) } else { val e = QueryExecutionErrors.failedRenameTempFileError(srcPath, dstPath) logWarning(e.getMessage) throw e } } } override def delete(path: Path): Unit = { try { fs.delete(path, true) } catch { case e: FileNotFoundException => // ignore if file has already been deleted } } override def isLocal: Boolean = fs match { case _: LocalFileSystem | _: RawLocalFileSystem => true case _ => false } override def createCheckpointDirectory(): Path = { val qualifiedPath = fs.makeQualified(path) fs.mkdirs(qualifiedPath, FsPermission.getDirDefault) qualifiedPath } } /** An implementation of [[CheckpointFileManager]] using Hadoop's [[FileContext]] API. */ class FileContextBasedCheckpointFileManager(path: Path, hadoopConf: Configuration) extends CheckpointFileManager with RenameHelperMethods with Logging { import CheckpointFileManager._ private val fc = if (path.toUri.getScheme == null) { FileContext.getFileContext(hadoopConf) } else { FileContext.getFileContext(path.toUri, hadoopConf) } override def list(path: Path, filter: PathFilter): Array[FileStatus] = { fc.util.listStatus(path, filter) } override def mkdirs(path: Path): Unit = { fc.mkdir(path, FsPermission.getDirDefault, true) } override def createTempFile(path: Path): FSDataOutputStream = { import CreateFlag._ import Options._ fc.create( path, EnumSet.of(CREATE, OVERWRITE), CreateOpts.checksumParam(ChecksumOpt.createDisabled())) } override def createAtomic( path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream = { new RenameBasedFSDataOutputStream(this, path, overwriteIfPossible) } override def open(path: Path): FSDataInputStream = { fc.open(path) } override def exists(path: Path): Boolean = { fc.util.exists(path) } override def renameTempFile(srcPath: Path, dstPath: Path, overwriteIfPossible: Boolean): Unit = { import Options.Rename._ fc.rename(srcPath, dstPath, if (overwriteIfPossible) OVERWRITE else NONE) // TODO: this is a workaround of HADOOP-16255 - remove this when HADOOP-16255 is resolved mayRemoveCrcFile(srcPath) } override def delete(path: Path): Unit = { try { fc.delete(path, true) } catch { case e: FileNotFoundException => // ignore if file has already been deleted } } override def isLocal: Boolean = fc.getDefaultFileSystem match { case _: LocalFs | _: RawLocalFs => true // LocalFs = RawLocalFs + ChecksumFs case _ => false } override def createCheckpointDirectory(): Path = { val qualifiedPath = fc.makeQualified(path) fc.mkdir(qualifiedPath, FsPermission.getDirDefault, true) qualifiedPath } private def mayRemoveCrcFile(path: Path): Unit = { try { val checksumFile = new Path(path.getParent, s\".${path.getName}.crc\") if (exists(checksumFile)) { // checksum file exists, deleting it delete(checksumFile) } } catch { case NonFatal(_) => // ignore, we are removing crc file as \"best-effort\" } } }",
          "## CLASS: org/apache/spark/sql/execution/datasources/FileFormatDataWriter# (implementation)\n*/ abstract class FileFormatDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric]) extends DataWriter[InternalRow] { /** * Max number of files a single task writes out due to file size. In most cases the number of * files written should be very small. This is just a safe guard to protect some really bad * settings, e.g. maxRecordsPerFile = 1. */ protected val MAX_FILE_COUNTER: Int = 1000 * 1000 protected val updatedPartitions: mutable.Set[String] = mutable.Set[String]() protected var currentWriter: OutputWriter = _ /** Trackers for computing various statistics on the data as it's being written out. */ protected val statsTrackers: Seq[WriteTaskStatsTracker] = description.statsTrackers.map(_.newTaskInstance()) /** Release resources of `currentWriter`. */ protected def releaseCurrentWriter(): Unit = { if (currentWriter != null) { try { currentWriter.close() statsTrackers.foreach(_.closeFile(currentWriter.path())) } finally { currentWriter = null } } } /** Release all resources. */ protected def releaseResources(): Unit = { // Call `releaseCurrentWriter()` by default, as this is the only resource to be released. releaseCurrentWriter() } /** Writes a record. */ def write(record: InternalRow): Unit def writeWithMetrics(record: InternalRow, count: Long): Unit = { if (count % CustomMetrics.NUM_ROWS_PER_UPDATE == 0) { CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) } write(record) } /** Write an iterator of records. */ def writeWithIterator(iterator: Iterator[InternalRow]): Unit = { var count = 0L while (iterator.hasNext) { writeWithMetrics(iterator.next(), count) count += 1 } CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) } /** * Returns the summary of relative information which * includes the list of partition strings written out. The list of partitions is sent back * to the driver and used to update the catalog. Other information will be sent back to the * driver too and used to e.g. update the metrics in UI. */ override def commit(): WriteTaskResult = { releaseResources() val (taskCommitMessage, taskCommitTime) = Utils.timeTakenMs { committer.commitTask(taskAttemptContext) } val summary = ExecutedWriteSummary( updatedPartitions = updatedPartitions.toSet, stats = statsTrackers.map(_.getFinalStats(taskCommitTime))) WriteTaskResult(taskCommitMessage, summary) } def abort(): Unit = { try { releaseResources() } finally { committer.abortTask(taskAttemptContext) } } override def close(): Unit = {} } /** FileFormatWriteTask for empty partitions */ class EmptyDirectoryDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric] = Map.empty ) extends FileFormatDataWriter(description, taskAttemptContext, committer, customMetrics) { override def write(record: InternalRow): Unit = {} } /** Writes data to a single directory (used for non-dynamic-partition writes). */ class SingleDirectoryDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric] = Map.empty) extends FileFormatDataWriter(description, taskAttemptContext, committer, customMetrics) { private var fileCounter: Int = _ private var recordsInFile: Long = _ // Initialize currentWriter and statsTrackers newOutputWriter() private def newOutputWriter(): Unit = { recordsInFile = 0 releaseResources() val ext = description.outputWriterFactory.getFileExtension(taskAttemptContext) val currentPath = committer.newTaskTempFile( taskAttemptContext, None, f\"-c$fileCounter%03d\" + ext) currentWriter = description.outputWriterFactory.newInstance( path = currentPath, dataSchema = description.dataColumns.toStructType, context = taskAttemptContext) statsTrackers.foreach(_.newFile(currentPath)) } override def write(record: InternalRow): Unit = { if (description.maxRecordsPerFile > 0 && recordsInFile >= description.maxRecordsPerFile) { fileCounter += 1 assert(fileCounter < MAX_FILE_COUNTER, s\"File counter $fileCounter is beyond max value $MAX_FILE_COUNTER\") newOutputWriter() } currentWriter.write(record) statsTrackers.foreach(_.newRow(currentWriter.path, record)) recordsInFile += 1 } } /** * Holds common logic for writing data with dynamic partition writes, meaning it can write to * multiple directories (partitions) or files (bucketing). */ abstract class BaseDynamicPartitionDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric]) extends FileFormatDataWriter(description, taskAttemptContext, committer, customMetrics) { /** Flag saying whether or not the data to be written out is partitioned. */ protected val isPartitioned = description.partitionColumns.nonEmpty /** Flag saying whether or not the data to be written out is bucketed. */ protected val isBucketed = description.bucketSpec.isDefined assert(isPartitioned || isBucketed, s\"\"\"DynamicPartitionWriteTask should be used for writing out data that's either |partitioned or bucketed. In this case neither is true. |WriteJobDescription: $description \"\"\".stripMargin) /** Number of records in current file. */ protected var recordsInFile: Long = _ /** * File counter for writing current partition or bucket. For same partition or bucket, * we may have more than one file, due to number of records limit per file. */ protected var fileCounter: Int = _ /** Extracts the partition values out of an input row. */ protected lazy val getPartitionValues: InternalRow => UnsafeRow = { val proj = UnsafeProjection.create(description.partitionColumns, description.allColumns) row => proj(row) } /** Expression that given partition columns builds a path string like: col1=val/col2=val/... */ private lazy val partitionPathExpression: Expression = Concat( description.partitionColumns.zipWithIndex.flatMap { case (c, i) => val partitionName = ScalaUDF( ExternalCatalogUtils.getPartitionPathString _, StringType, Seq(Literal(c.name), Cast(c, StringType, Option(description.timeZoneId)))) if (i == 0) Seq(partitionName) else Seq(Literal(Path.SEPARATOR), partitionName) }) /** * Evaluates the `partitionPathExpression` above on a row of `partitionValues` and returns * the partition string. */ private lazy val getPartitionPath: InternalRow => String = { val proj = UnsafeProjection.create(Seq(partitionPathExpression), description.partitionColumns) row => proj(row).getString(0) } /** Given an input row, returns the corresponding `bucketId` */ protected lazy val getBucketId: InternalRow => Int = { val proj = UnsafeProjection.create(Seq(description.bucketSpec.get.bucketIdExpression), description.allColumns) row => proj(row).getInt(0) } /** Returns the data columns to be written given an input row */ protected val getOutputRow = UnsafeProjection.create(description.dataColumns, description.allColumns) /** * Opens a new OutputWriter given a partition key and/or a bucket id. * If bucket id is specified, we will append it to the end of the file name, but before the * file extension, e.g. part-r-00009-ea518ad4-455a-4431-b471-d24e03814677-00002.gz.parquet * * @param partitionValues the partition which all tuples being written by this OutputWriter * belong to * @param bucketId the bucket which all tuples being written by this OutputWriter belong to * @param closeCurrentWriter close and release resource for current writer */ protected def renewCurrentWriter( partitionValues: Option[InternalRow], bucketId: Option[Int], closeCurrentWriter: Boolean): Unit = { recordsInFile = 0 if (closeCurrentWriter) { releaseCurrentWriter() } val partDir = partitionValues.map(getPartitionPath(_)) partDir.foreach(updatedPartitions.add) val bucketIdStr = bucketId.map(BucketingUtils.bucketIdToString).getOrElse(\"\") // The prefix and suffix must be in a form that matches our bucketing format. See BucketingUtils // for details. The prefix is required to represent bucket id when writing Hive-compatible // bucketed table. val prefix = bucketId match { case Some(id) => description.bucketSpec.get.bucketFileNamePrefix(id) case _ => \"\" } val suffix = f\"$bucketIdStr.c$fileCounter%03d\" + description.outputWriterFactory.getFileExtension(taskAttemptContext) val fileNameSpec = FileNameSpec(prefix, suffix) val customPath = partDir.flatMap { dir => description.customPartitionLocations.get(PartitioningUtils.parsePathFragment(dir)) } val currentPath = if (customPath.isDefined) { committer.newTaskTempFileAbsPath(taskAttemptContext, customPath.get, fileNameSpec) } else { committer.newTaskTempFile(taskAttemptContext, partDir, fileNameSpec) } currentWriter = description.outputWriterFactory.newInstance( path = currentPath, dataSchema = description.dataColumns.toStructType, context = taskAttemptContext) statsTrackers.foreach(_.newFile(currentPath)) } /** * Open a new output writer when number of records exceeding limit. * * @param partitionValues the partition which all tuples being written by this `OutputWriter` * belong to * @param bucketId the bucket which all tuples being written by this `OutputWriter` belong to */ protected def renewCurrentWriterIfTooManyRecords( partitionValues: Option[InternalRow], bucketId: Option[Int]): Unit = { // Exceeded the threshold in terms of the number of records per file. // Create a new file by increasing the file counter. fileCounter += 1 assert(fileCounter < MAX_FILE_COUNTER, s\"File counter $fileCounter is beyond max value $MAX_FILE_COUNTER\") renewCurrentWriter(partitionValues, bucketId, closeCurrentWriter = true) } /** * Writes the given record with current writer. * * @param record The record to write */ protected def writeRecord(record: InternalRow): Unit = { val outputRow = getOutputRow(record) currentWriter.write(outputRow) statsTrackers.foreach(_.newRow(currentWriter.path, outputRow)) recordsInFile += 1 } } /** * Dynamic partition writer with single writer, meaning only one writer is opened at any time for * writing. The records to be written are required to be sorted on partition and/or bucket * column(s) before writing. */ class DynamicPartitionDataSingleWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric] = Map.empty) extends BaseDynamicPartitionDataWriter(description, taskAttemptContext, committer, customMetrics) { private var currentPartitionValues: Option[UnsafeRow] = None private var currentBucketId: Option[Int] = None override def write(record: InternalRow): Unit = { val nextPartitionValues = if (isPartitioned) Some(getPartitionValues(record)) else None val nextBucketId = if (isBucketed) Some(getBucketId(record)) else None if (currentPartitionValues != nextPartitionValues || currentBucketId != nextBucketId) { // See a new partition or bucket - write to a new partition dir (or a new bucket file). if (isPartitioned && currentPartitionValues != nextPartitionValues) { currentPartitionValues = Some(nextPartitionValues.get.copy()) statsTrackers.foreach(_.newPartition(currentPartitionValues.get)) } if (isBucketed) { currentBucketId = nextBucketId } fileCounter = 0 renewCurrentWriter(currentPartitionValues, currentBucketId, closeCurrentWriter = true) } else if (description.maxRecordsPerFile > 0 && recordsInFile >= description.maxRecordsPerFile) { renewCurrentWriterIfTooManyRecords(currentPartitionValues, currentBucketId) } writeRecord(record) } } /** * Dynamic partition writer with concurrent writers, meaning multiple concurrent writers are opened * for writing. * * The process has the following steps: * - Step 1: Maintain a map of output writers per each partition and/or bucket columns. Keep all * writers opened and write rows one by one. * - Step 2: If number of concurrent writers exceeds limit, sort rest of rows on partition and/or * bucket column(s). Write rows one by one, and eagerly close the writer when finishing * each partition and/or bucket. * * Caller is expected to call `writeWithIterator()` instead of `write()` to write records. */ class DynamicPartitionDataConcurrentWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, concurrentOutputWriterSpec: ConcurrentOutputWriterSpec, customMetrics: Map[String, SQLMetric] = Map.empty) extends BaseDynamicPartitionDataWriter(description, taskAttemptContext, committer, customMetrics) with Logging { /** Wrapper class to index a unique concurrent output writer. */ private case class WriterIndex( var partitionValues: Option[UnsafeRow], var bucketId: Option[Int]) /** Wrapper class for status of a unique concurrent output writer. */ private class WriterStatus( var outputWriter: OutputWriter, var recordsInFile: Long, var fileCounter: Int) /** * State to indicate if we are falling back to sort-based writer. * Because we first try to use concurrent writers, its initial value is false. */ private var sorted: Boolean = false private val concurrentWriters = mutable.HashMap[WriterIndex, WriterStatus]() /** * The index for current writer. Intentionally make the index mutable and reusable. * Avoid JVM GC issue when many short-living `WriterIndex` objects are created * if switching between concurrent writers frequently. */ private val currentWriterId = WriterIndex(None, None) /** * Release resources for all concurrent output writers. */ override protected def releaseResources(): Unit = { currentWriter = null concurrentWriters.values.foreach(status => { if (status.outputWriter != null) { try { status.outputWriter.close() } finally { status.outputWriter = null } } }) concurrentWriters.clear() } override def write(record: InternalRow): Unit = { val nextPartitionValues = if (isPartitioned) Some(getPartitionValues(record)) else None val nextBucketId = if (isBucketed) Some(getBucketId(record)) else None if (currentWriterId.partitionValues != nextPartitionValues || currentWriterId.bucketId != nextBucketId) { // See a new partition or bucket - write to a new partition dir (or a new bucket file). if (currentWriter != null) { if (!sorted) { // Update writer status in concurrent writers map, because the writer is probably needed // again later for writing other rows. updateCurrentWriterStatusInMap() } else { // Remove writer status in concurrent writers map and release current writer resource, // because the writer is not needed any more. concurrentWriters.remove(currentWriterId) releaseCurrentWriter() } } if (isBucketed) { currentWriterId.bucketId = nextBucketId } if (isPartitioned && currentWriterId.partitionValues != nextPartitionValues) { currentWriterId.partitionValues = Some(nextPartitionValues.get.copy()) if (!concurrentWriters.contains(currentWriterId)) { statsTrackers.foreach(_.newPartition(currentWriterId.partitionValues.get)) } } setupCurrentWriterUsingMap() } if (description.maxRecordsPerFile > 0 && recordsInFile >= description.maxRecordsPerFile) { renewCurrentWriterIfTooManyRecords(currentWriterId.partitionValues, currentWriterId.bucketId) // Update writer status in concurrent writers map, as a new writer is created. updateCurrentWriterStatusInMap() } writeRecord(record) } /** * Write iterator of records with concurrent writers. */ override def writeWithIterator(iterator: Iterator[InternalRow]): Unit = { var count = 0L while (iterator.hasNext && !sorted) { writeWithMetrics(iterator.next(), count) count += 1 } CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) if (iterator.hasNext) { count = 0L clearCurrentWriterStatus() val sorter = concurrentOutputWriterSpec.createSorter() val sortIterator = sorter.sort(iterator.asInstanceOf[Iterator[UnsafeRow]]) while (sortIterator.hasNext) { writeWithMetrics(sortIterator.next(), count) count += 1 } CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) } } /** * Update current writer status in map. */ private def updateCurrentWriterStatusInMap(): Unit = { val status = concurrentWriters(currentWriterId) status.outputWriter = currentWriter status.recordsInFile = recordsInFile status.fileCounter = fileCounter } /** * Retrieve writer in map, or create a new writer if not exists. */ private def setupCurrentWriterUsingMap(): Unit = { if (concurrentWriters.contains(currentWriterId)) { val status = concurrentWriters(currentWriterId) currentWriter = status.outputWriter recordsInFile = status.recordsInFile fileCounter = status.fileCounter } else { fileCounter = 0 renewCurrentWriter( currentWriterId.partitionValues, currentWriterId.bucketId, closeCurrentWriter = false) if (!sorted) { assert(concurrentWriters.size <= concurrentOutputWriterSpec.maxWriters, s\"Number of concurrent output file writers is ${concurrentWriters.size} \" + s\" which is beyond max value ${concurrentOutputWriterSpec.maxWriters}\") } else { assert(concurrentWriters.size <= concurrentOutputWriterSpec.maxWriters + 1, s\"Number of output file writers after sort is ${concurrentWriters.size} \" + s\" which is beyond max value ${concurrentOutputWriterSpec.maxWriters + 1}\") } concurrentWriters.put( currentWriterId.copy(), new WriterStatus(currentWriter, recordsInFile, fileCounter)) if (concurrentWriters.size >= concurrentOutputWriterSpec.maxWriters && !sorted) { // Fall back to sort-based sequential writer mode. logInfo(s\"Number of concurrent writers ${concurrentWriters.size} reaches the threshold. \" + \"Fall back from concurrent writers to sort-based sequential writer. You may change \" + s\"threshold with configuration ${SQLConf.MAX_CONCURRENT_OUTPUT_FILE_WRITERS.key}\") sorted = true } } } /** * Clear the current writer status in map. */ private def clearCurrentWriterStatus(): Unit = { if (currentWriterId.partitionValues.isDefined || currentWriterId.bucketId.isDefined) { updateCurrentWriterStatusInMap() } currentWriterId.partitionValues = None currentWriterId.bucketId = None currentWriter = null recordsInFile = 0 fileCounter = 0 } } /** * Bucketing specification for all the write tasks. * * @param bucketIdExpression Expression to calculate bucket id based on bucket column(s). * @param bucketFileNamePrefix Prefix of output file name based on bucket id. */ case class WriterBucketSpec( bucketIdExpression: Expression, bucketFileNamePrefix: Int => String) /** A shared job description for all the write tasks. */ class WriteJobDescription( val uuid: String, // prevent collision between different (appending) write jobs val serializableHadoopConf: SerializableConfiguration, val outputWriterFactory: OutputWriterFactory, val allColumns: Seq[Attribute], val dataColumns: Seq[Attribute], val partitionColumns: Seq[Attribute], val bucketSpec: Option[WriterBucketSpec], val path: String, val customPartitionLocations: Map[TablePartitionSpec, String], val maxRecordsPerFile: Long, val timeZoneId: String, val statsTrackers: Seq[WriteJobStatsTracker]) extends Serializable { assert(AttributeSet(allColumns) == AttributeSet(partitionColumns ++ dataColumns), s\"\"\" |All columns: ${allColumns.mkString(\", \")} |Partition columns: ${partitionColumns.mkString(\", \")} |Data columns: ${dataColumns.mkString(\", \")} \"\"\".stripMargin) } /** The result of a successful write task. */ case class WriteTaskResult(commitMsg: TaskCommitMessage, summary: ExecutedWriteSummary) extends WriterCommitMessage /** * Wrapper class for the metrics of writing data out. * * @param updatedPartitions the partitions updated during writing data out. Only valid * for dynamic partition. * @param stats one `WriteTaskStats` object for every `WriteJobStatsTracker` that the job had. */ case class ExecutedWriteSummary( updatedPartitions: Set[String], stats: Seq[WriteTaskStats])"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# External Data Connectivity and Writing (from code only)\n\nThis document summarizes how external data connectivity and data writing are handled in this repository, based strictly on the visible implementations referenced below.\n\n## JDBC (SQL/DataSource V2)\n\nClass: `org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCWriteBuilder`\n\n- Constructor parameters: `schema: StructType`, `options: JdbcOptionsInWrite`.\n- Implements `WriteBuilder` and `SupportsTruncate`.\n- Internal flag: `isTruncate: Boolean` defaults to `false`.\n- `truncate(): WriteBuilder`\n  - Sets `isTruncate = true` and returns `this`.\n- `build(): V1Write`\n  - Returns a `V1Write` whose `toInsertableRelation` is a function `(data: DataFrame, _: Boolean) => ...`.\n  - Behavior inside `toInsertableRelation`:\n    - If `isTruncate` is `true`:\n      - Resolves a JDBC dialect via `JdbcDialects.get(options.url)`.\n      - Creates a connection with `dialect.createConnectionFactory(options)(-1)`.\n      - Calls `JdbcUtils.truncateTable(conn, options)`.\n    - Calls `JdbcUtils.saveTable(data, Some(schema), SQLConf.get.caseSensitiveAnalysis, options)`.\n\nEffectively, this builder optionally truncates a target table before saving the provided `DataFrame` via JDBC using `JdbcUtils.saveTable` with the specified `schema` and `options`.\n\n## File-based Formats (DataSource V1)\n\nClass: `org.apache.spark.sql.execution.datasources.FileFormatWriter` (snippet `ConcurrentOutputWriterSpec` and `write` method excerpt)\n\n- `ConcurrentOutputWriterSpec` describes concurrent output writer execution with fields:\n  - `maxWriters: Int`\n  - `createSorter: () => UnsafeExternalRowSorter`\n- The `write(...)` method (excerpt) performs the following steps:\n  - Creates a Hadoop `Job` from `hadoopConf` and sets output key/value classes to `Void` and `InternalRow`.\n  - Sets the output path via `FileOutputFormat.setOutputPath(job, new Path(outputSpec.outputPath))`.\n  - Builds `partitionSet = AttributeSet(partitionColumns)`.\n  - Produces `finalOutputSpec` by cleaning file source metadata: `outputSpec.copy(outputColumns = outputSpec.outputColumns.map(FileSourceMetadataAttribute.cleanupFileSourceMetadataInformation))`.\n  - Derives `dataColumns = finalOutputSpec.outputColumns.filterNot(partitionSet.contains)`.\n  - The documented basic workflow (in code comments) states:\n    1. Driver-side setup incl. output committer init and data source-specific preparation.\n    2. Issue write job with executor tasks that write rows per RDD partition.\n    3. Commit/abort each task based on exceptions; abort on commit exceptions.\n    4. If all tasks commit, commit job; otherwise abort; abort on job commit exceptions.\n    5. On successful job commit, perform post-commit operations (e.g., processing statistics).\n  - Return value: a `Set[String]` of updated partition paths for the write job.\n\nClass: `org.apache.spark.sql.execution.datasources.FileFormatDataWriter`\n\n- Abstract `DataWriter[InternalRow]` responsible for writing rows and tracking per-file/task stats.\n- Key members:\n  - `MAX_FILE_COUNTER: Int = 1000 * 1000` (safeguard for excessive file splitting).\n  - `updatedPartitions: mutable.Set[String]` to track partitions written.\n  - `currentWriter: OutputWriter` (nullable), closed and cleared via `releaseCurrentWriter()`.\n  - `statsTrackers: Seq[WriteTaskStatsTracker]` from `description.statsTrackers.map(_.newTaskInstance())`.\n- Resource management:\n  - `releaseCurrentWriter()` closes `currentWriter`, updates trackers via `closeFile(currentWriter.path())`, and nulls `currentWriter`.\n  - `releaseResources()` calls `releaseCurrentWriter()` by default.\n- Writing APIs:\n  - Abstract `def write(record: InternalRow): Unit`.\n  - `def writeWithMetrics(record: InternalRow, count: Long): Unit`\n    - Every `CustomMetrics.NUM_ROWS_PER_UPDATE` rows, calls `CustomMetrics.updateMetrics(currentMetricsValues, customMetrics)`.\n    - Delegates to `write(record)`.\n  - `def writeWithIterator(iterator: Iterator[InternalRow]): Unit`\n    - Iterates rows, calling `writeWithMetrics`; finally updates metrics once more.\n- Summary:\n  - Provides hooks to return summary including partition strings written; used by the driver to update catalog (based on code comments and method purpose notes).\n\n## Data Source V2 Writing Execution\n\nClasses/Traits: `WriteToDataSourceV2Exec`, `V2ExistingTableWriteExec`, `V2TableWriteExec`\n\n- `WriteToDataSourceV2Exec(batchWrite: BatchWrite, refreshCache: () => Unit, query: SparkPlan, writeMetrics: Seq[CustomMetric])`\n  - Extends `V2TableWriteExec`.\n  - Builds `customMetrics: Map[String, SQLMetric]` from `writeMetrics` using `SQLMetrics.createV2CustomMetric`.\n  - `run(): Seq[InternalRow]`:\n    - Calls `writeWithV2(batchWrite)`.\n    - Invokes `refreshCache()`.\n    - Returns `writtenRows`.\n  - Supports `withNewChildInternal` to update `query`.\n- `V2ExistingTableWriteExec`\n  - Extends `V2TableWriteExec` and provides `refreshCache: () => Unit` and `write: Write`.\n  - `customMetrics` are built from `write.supportedCustomMetrics()` via `SQLMetrics.createV2CustomMetric`.\n  - `run(): Seq[InternalRow]`:\n    - Calls `writeWithV2(write.toBatch)`.\n    - Invokes `refreshCache()`.\n    - Returns `writtenRows`.\n- `V2TableWriteExec` (base trait)\n  - Extends `V2CommandExec` with `UnaryExecNode`.\n  - Fields: `def query: SparkPlan`, `var commitProgress: Option[StreamWriterCommitProgress] = None`.\n  - `child: SparkPlan = query`; `output: Seq[Attribute] = Nil`.\n  - `customMetrics: Map[String, SQLMetric] = Map.empty` by default (overridden by subclasses above).\n  - Orchestrates writing via `writeWithV2(...)` (implementation not shown in the excerpt), and manages commit progress.\n\n## Summary\n\n- JDBC connectivity and writes are managed by `JDBCWriteBuilder`, which optionally truncates target tables and delegates data saving to `JdbcUtils.saveTable` using connection options and `schema`.\n- File-based writes use `FileFormatWriter.write` to set up Hadoop jobs, determine partition vs. data columns, and run a commit protocol via `FileCommitProtocol`. `FileFormatDataWriter` handles per-task writing, resource cleanup, and custom metrics emission during iteration.\n- Data Source V2 writing is executed by `WriteToDataSourceV2Exec` and `V2ExistingTableWriteExec`, which convert `Write` to `BatchWrite`, perform the write with `writeWithV2`, record custom metrics, and refresh caches post-commit.\n",
          "prompt_time": 14.173,
          "answer_time": 87,
          "context_tokens": 9157,
          "used_context": [
            "## CLASS: local0:sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCWriteBuilder.scala (implementation)\n} override def build(): V1Write = new V1Write { override def toInsertableRelation: InsertableRelation = (data: DataFrame, _: Boolean) => { // TODO (SPARK-32595): do truncate and append atomically. if (isTruncate) { val dialect = JdbcDialects.get(options.url) val conn = dialect.createConnectionFactory(options)(-1) JdbcUtils.truncateTable(conn, options) } JdbcUtils.saveTable(data, Some(schema), SQLConf.get.caseSensitiveAnalysis, options) } } }",
            "## CLASS: org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCWriteBuilder# (implementation)\nimport org.apache.spark.sql.types.StructType case class JDBCWriteBuilder(schema: StructType, options: JdbcOptionsInWrite) extends WriteBuilder with SupportsTruncate { private var isTruncate = false override def truncate(): WriteBuilder = { isTruncate = true this } override def build(): V1Write = new V1Write { override def toInsertableRelation: InsertableRelation = (data: DataFrame, _: Boolean) => { // TODO (SPARK-32595): do truncate and append atomically. if (isTruncate) { val dialect = JdbcDialects.get(options.url) val conn = dialect.createConnectionFactory(options)(-1) JdbcUtils.truncateTable(conn, options) } JdbcUtils.saveTable(data, Some(schema), SQLConf.get.caseSensitiveAnalysis, options) } } }",
            "## CLASS: org/apache/spark/sql/execution/datasources/FileFormatWriter.ConcurrentOutputWriterSpec# (implementation)\n/** Describes how concurrent output writers should be executed. */ case class ConcurrentOutputWriterSpec( maxWriters: Int, createSorter: () => UnsafeExternalRowSorter) /** * Basic work flow of this command is: * 1. Driver side setup, including output committer initialization and data source specific * preparation work for the write job to be issued. * 2. Issues a write job consists of one or more executor side tasks, each of which writes all * rows within an RDD partition. * 3. If no exception is thrown in a task, commits that task, otherwise aborts that task; If any * exception is thrown during task commitment, also aborts that task. * 4. If all tasks are committed, commit the job, otherwise aborts the job; If any exception is * thrown during job commitment, also aborts the job. * 5. If the job is successfully committed, perform post-commit operations such as * processing statistics. * @return The set of all partition paths that were updated during this write job. */ def write( sparkSession: SparkSession, plan: SparkPlan, fileFormat: FileFormat, committer: FileCommitProtocol, outputSpec: OutputSpec, hadoopConf: Configuration, partitionColumns: Seq[Attribute], bucketSpec: Option[BucketSpec], statsTrackers: Seq[WriteJobStatsTracker], options: Map[String, String]) : Set[String] = { val job = Job.getInstance(hadoopConf) job.setOutputKeyClass(classOf[Void]) job.setOutputValueClass(classOf[InternalRow]) FileOutputFormat.setOutputPath(job, new Path(outputSpec.outputPath)) val partitionSet = AttributeSet(partitionColumns) // cleanup the internal metadata information of // the file source metadata attribute if any before write out val finalOutputSpec = outputSpec.copy(outputColumns = outputSpec.outputColumns .map(FileSourceMetadataAttribute.cleanupFileSourceMetadataInformation)) val dataColumns = finalOutputSpec.outputColumns.filterNot(partitionSet.contains) var needConvert = false val projectList: Seq[NamedExpression] = plan.output.map { case p if partitionSet.contains(p) && p.dataType == StringType && p.nullable => needConvert = true Alias(Empty2Null(p), p.name)() case attr => attr } val empty2NullPlan = if (needConvert) ProjectExec(projectList, plan) else plan val writerBucketSpec = bucketSpec.map { spec => val bucketColumns = spec.bucketColumnNames.map(c => dataColumns.find(_.name == c).get) if (options.getOrElse(BucketingUtils.optionForHiveCompatibleBucketWrite, \"false\") == \"true\") { // Hive bucketed table: use `HiveHash` and bitwise-and as bucket id expression. // Without the extra bitwise-and operation, we can get wrong bucket id when hash value of // columns is negative. See Hive implementation in // `org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#getBucketNumber()`. val hashId = BitwiseAnd(HiveHash(bucketColumns), Literal(Int.MaxValue)) val bucketIdExpression = Pmod(hashId, Literal(spec.numBuckets)) // The bucket file name prefix is following Hive, Presto and Trino conversion, so this // makes sure Hive bucketed table written by Spark, can be read by other SQL engines. // // Hive: `org.apache.hadoop.hive.ql.exec.Utilities#getBucketIdFromFile()`. // Trino: `io.trino.plugin.hive.BackgroundHiveSplitLoader#BUCKET_PATTERNS`. val fileNamePrefix = (bucketId: Int) => f\"$bucketId%05d_0_\" WriterBucketSpec(bucketIdExpression, fileNamePrefix) } else { // Spark bucketed table: use `HashPartitioning.partitionIdExpression` as bucket id // expression, so that we can guarantee the data distribution is same between shuffle and // bucketed data source, which enables us to only shuffle one side when join a bucketed // table and a normal one. val bucketIdExpression = HashPartitioning(bucketColumns, spec.numBuckets) .partitionIdExpression WriterBucketSpec(bucketIdExpression, (_: Int) => \"\") } } val sortColumns = bucketSpec.toSeq.flatMap { spec => spec.sortColumnNames.map(c => dataColumns.find(_.name == c).get) } val caseInsensitiveOptions = CaseInsensitiveMap(options) val dataSchema = dataColumns.toStructType DataSourceUtils.verifySchema(fileFormat, dataSchema) // Note: prepareWrite has side effect. It sets \"job\". val outputWriterFactory = fileFormat.prepareWrite(sparkSession, job, caseInsensitiveOptions, dataSchema) val description = new WriteJobDescription( uuid = UUID.randomUUID.toString, serializableHadoopConf = new SerializableConfiguration(job.getConfiguration), outputWriterFactory = outputWriterFactory, allColumns = finalOutputSpec.outputColumns, dataColumns = dataColumns, partitionColumns = partitionColumns, bucketSpec = writerBucketSpec, path = finalOutputSpec.outputPath, customPartitionLocations = finalOutputSpec.customPartitionLocations, maxRecordsPerFile = caseInsensitiveOptions.get(\"maxRecordsPerFile\").map(_.toLong) .getOrElse(sparkSession.sessionState.conf.maxRecordsPerFile), timeZoneId = caseInsensitiveOptions.get(DateTimeUtils.TIMEZONE_OPTION) .getOrElse(sparkSession.sessionState.conf.sessionLocalTimeZone), statsTrackers = statsTrackers ) // We should first sort by partition columns, then bucket id, and finally sorting columns. val requiredOrdering = partitionColumns ++ writerBucketSpec.map(_.bucketIdExpression) ++ sortColumns // the sort order doesn't matter val actualOrdering = empty2NullPlan.outputOrdering.map(_.child) val orderingMatched = if (requiredOrdering.length > actualOrdering.length) { false } else { requiredOrdering.zip(actualOrdering).forall { case (requiredOrder, childOutputOrder) => requiredOrder.semanticEquals(childOutputOrder) } } SQLExecution.checkSQLExecutionId(sparkSession) // propagate the description UUID into the jobs, so that committers // get an ID guaranteed to be unique. job.getConfiguration.set(\"spark.sql.sources.writeJobUUID\", description.uuid) // This call shouldn't be put into the `try` block below because it only initializes and // prepares the job, any exception thrown from here shouldn't cause abortJob() to be called. committer.setupJob(job) try { val (rdd, concurrentOutputWriterSpec) = if (orderingMatched) { (empty2NullPlan.execute(), None) } else { // SPARK-21165: the `requiredOrdering` is based on the attributes from analyzed plan, and // the physical plan may have different attribute ids due to optimizer removing some // aliases. Here we bind the expression ahead to avoid potential attribute ids mismatch. val orderingExpr = bindReferences( requiredOrdering.map(SortOrder(_, Ascending)), finalOutputSpec.outputColumns) val sortPlan = SortExec( orderingExpr, global = false, child = empty2NullPlan) val maxWriters = sparkSession.sessionState.conf.maxConcurrentOutputFileWriters val concurrentWritersEnabled = maxWriters > 0 && sortColumns.isEmpty if (concurrentWritersEnabled) { (empty2NullPlan.execute(), Some(ConcurrentOutputWriterSpec(maxWriters, () => sortPlan.createSorter()))) } else { (sortPlan.execute(), None) } } // SPARK-23271 If we are attempting to write a zero partition rdd, create a dummy single // partition rdd to make sure we at least set up one write task to write the metadata. val rddWithNonEmptyPartitions = if (rdd.partitions.length == 0) { sparkSession.sparkContext.parallelize(Array.empty[InternalRow], 1) } else { rdd } val jobIdInstant = new Date().getTime val ret = new Array[WriteTaskResult](rddWithNonEmptyPartitions.partitions.length) sparkSession.sparkContext.runJob( rddWithNonEmptyPartitions, (taskContext: TaskContext, iter: Iterator[InternalRow]) => { executeTask( description = description, jobIdInstant = jobIdInstant, sparkStageId = taskContext.stageId(), sparkPartitionId = taskContext.partitionId(), sparkAttemptNumber = taskContext.taskAttemptId().toInt & Integer.MAX_VALUE, committer, iterator = iter, concurrentOutputWriterSpec = concurrentOutputWriterSpec) }, rddWithNonEmptyPartitions.partitions.indices, (index, res: WriteTaskResult) => { committer.onTaskCommit(res.commitMsg) ret(index) = res }) val commitMsgs = ret.map(_.commitMsg) logInfo(s\"Start to commit write Job ${description.uuid}.\") val (_, duration) = Utils.timeTakenMs { committer.commitJob(job, commitMsgs) } logInfo(s\"Write Job ${description.uuid} committed. Elapsed time: $duration ms.\") processStats(description.statsTrackers, ret.map(_.summary.stats), duration) logInfo(s\"Finished processing stats for write job ${description.uuid}.\") // return a set of all the partition paths that were updated during this job ret.map(_.summary.updatedPartitions).reduceOption(_ ++ _).getOrElse(Set.empty) } catch { case cause: Throwable => logError(s\"Aborting job ${description.uuid}.\", cause) committer.abortJob(job) throw QueryExecutionErrors.jobAbortedError(cause) } } /** Writes data out in a single Spark task. */ private def executeTask( description: WriteJobDescription, jobIdInstant: Long, sparkStageId: Int, sparkPartitionId: Int, sparkAttemptNumber: Int, committer: FileCommitProtocol, iterator: Iterator[InternalRow], concurrentOutputWriterSpec: Option[ConcurrentOutputWriterSpec]): WriteTaskResult = { val jobId = SparkHadoopWriterUtils.createJobID(new Date(jobIdInstant), sparkStageId) val taskId = new TaskID(jobId, TaskType.MAP, sparkPartitionId) val taskAttemptId = new TaskAttemptID(taskId, sparkAttemptNumber) // Set up the attempt context required to use in the output committer. val taskAttemptContext: TaskAttemptContext = { // Set up the configuration object val hadoopConf = description.serializableHadoopConf.value hadoopConf.set(\"mapreduce.job.id\", jobId.toString) hadoopConf.set(\"mapreduce.task.id\", taskAttemptId.getTaskID.toString) hadoopConf.set(\"mapreduce.task.attempt.id\", taskAttemptId.toString) hadoopConf.setBoolean(\"mapreduce.task.ismap\", true) hadoopConf.setInt(\"mapreduce.task.partition\", 0) new TaskAttemptContextImpl(hadoopConf, taskAttemptId) } committer.setupTask(taskAttemptContext) val dataWriter = if (sparkPartitionId != 0 && !iterator.hasNext) { // In case of empty job, leave first partition to save meta for file format like parquet. new EmptyDirectoryDataWriter(description, taskAttemptContext, committer) } else if (description.partitionColumns.isEmpty && description.bucketSpec.isEmpty) { new SingleDirectoryDataWriter(description, taskAttemptContext, committer) } else { concurrentOutputWriterSpec match { case Some(spec) => new DynamicPartitionDataConcurrentWriter( description, taskAttemptContext, committer, spec) case _ => new DynamicPartitionDataSingleWriter(description, taskAttemptContext, committer) } } try { Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Execute the task to write rows out and commit the task. dataWriter.writeWithIterator(iterator) dataWriter.commit() })(catchBlock = { // If there is an error, abort the task dataWriter.abort() logError(s\"Job $jobId aborted.\") }, finallyBlock = { dataWriter.close() }) } catch { case e: FetchFailedException => throw e case f: FileAlreadyExistsException if SQLConf.get.fastFailFileFormatOutput => // If any output file to write already exists, it does not make sense to re-run this task. // We throw the exception and let Executor throw ExceptionFailure to abort the job. throw new TaskOutputFileAlreadyExistException(f) case t: Throwable => throw QueryExecutionErrors.taskFailedWhileWritingRowsError(t) } } /** * For every registered [[WriteJobStatsTracker]], call `processStats()` on it, passing it * the corresponding [[WriteTaskStats]] from all executors. */ private[datasources] def processStats( statsTrackers: Seq[WriteJobStatsTracker], statsPerTask: Seq[Seq[WriteTaskStats]], jobCommitDuration: Long) : Unit = { val numStatsTrackers = statsTrackers.length assert(statsPerTask.forall(_.length == numStatsTrackers), s\"\"\"Every WriteTask should have produced one `WriteTaskStats` object for every tracker. |There are $numStatsTrackers statsTrackers, but some task returned |${statsPerTask.find(_.length != numStatsTrackers).get.length} results instead. \"\"\".stripMargin) val statsPerTracker = if (statsPerTask.nonEmpty) { statsPerTask.transpose } else { statsTrackers.map(_ => Seq.empty) } statsTrackers.zip(statsPerTracker).foreach { case (statsTracker, stats) => statsTracker.processStats(stats, jobCommitDuration) } } }",
            "## CLASS: org/apache/spark/sql/execution/datasources/FileFormatDataWriter# (implementation)\n*/ abstract class FileFormatDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric]) extends DataWriter[InternalRow] { /** * Max number of files a single task writes out due to file size. In most cases the number of * files written should be very small. This is just a safe guard to protect some really bad * settings, e.g. maxRecordsPerFile = 1. */ protected val MAX_FILE_COUNTER: Int = 1000 * 1000 protected val updatedPartitions: mutable.Set[String] = mutable.Set[String]() protected var currentWriter: OutputWriter = _ /** Trackers for computing various statistics on the data as it's being written out. */ protected val statsTrackers: Seq[WriteTaskStatsTracker] = description.statsTrackers.map(_.newTaskInstance()) /** Release resources of `currentWriter`. */ protected def releaseCurrentWriter(): Unit = { if (currentWriter != null) { try { currentWriter.close() statsTrackers.foreach(_.closeFile(currentWriter.path())) } finally { currentWriter = null } } } /** Release all resources. */ protected def releaseResources(): Unit = { // Call `releaseCurrentWriter()` by default, as this is the only resource to be released. releaseCurrentWriter() } /** Writes a record. */ def write(record: InternalRow): Unit def writeWithMetrics(record: InternalRow, count: Long): Unit = { if (count % CustomMetrics.NUM_ROWS_PER_UPDATE == 0) { CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) } write(record) } /** Write an iterator of records. */ def writeWithIterator(iterator: Iterator[InternalRow]): Unit = { var count = 0L while (iterator.hasNext) { writeWithMetrics(iterator.next(), count) count += 1 } CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) } /** * Returns the summary of relative information which * includes the list of partition strings written out. The list of partitions is sent back * to the driver and used to update the catalog. Other information will be sent back to the * driver too and used to e.g. update the metrics in UI. */ override def commit(): WriteTaskResult = { releaseResources() val (taskCommitMessage, taskCommitTime) = Utils.timeTakenMs { committer.commitTask(taskAttemptContext) } val summary = ExecutedWriteSummary( updatedPartitions = updatedPartitions.toSet, stats = statsTrackers.map(_.getFinalStats(taskCommitTime))) WriteTaskResult(taskCommitMessage, summary) } def abort(): Unit = { try { releaseResources() } finally { committer.abortTask(taskAttemptContext) } } override def close(): Unit = {} } /** FileFormatWriteTask for empty partitions */ class EmptyDirectoryDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric] = Map.empty ) extends FileFormatDataWriter(description, taskAttemptContext, committer, customMetrics) { override def write(record: InternalRow): Unit = {} } /** Writes data to a single directory (used for non-dynamic-partition writes). */ class SingleDirectoryDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric] = Map.empty) extends FileFormatDataWriter(description, taskAttemptContext, committer, customMetrics) { private var fileCounter: Int = _ private var recordsInFile: Long = _ // Initialize currentWriter and statsTrackers newOutputWriter() private def newOutputWriter(): Unit = { recordsInFile = 0 releaseResources() val ext = description.outputWriterFactory.getFileExtension(taskAttemptContext) val currentPath = committer.newTaskTempFile( taskAttemptContext, None, f\"-c$fileCounter%03d\" + ext) currentWriter = description.outputWriterFactory.newInstance( path = currentPath, dataSchema = description.dataColumns.toStructType, context = taskAttemptContext) statsTrackers.foreach(_.newFile(currentPath)) } override def write(record: InternalRow): Unit = { if (description.maxRecordsPerFile > 0 && recordsInFile >= description.maxRecordsPerFile) { fileCounter += 1 assert(fileCounter < MAX_FILE_COUNTER, s\"File counter $fileCounter is beyond max value $MAX_FILE_COUNTER\") newOutputWriter() } currentWriter.write(record) statsTrackers.foreach(_.newRow(currentWriter.path, record)) recordsInFile += 1 } } /** * Holds common logic for writing data with dynamic partition writes, meaning it can write to * multiple directories (partitions) or files (bucketing). */ abstract class BaseDynamicPartitionDataWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric]) extends FileFormatDataWriter(description, taskAttemptContext, committer, customMetrics) { /** Flag saying whether or not the data to be written out is partitioned. */ protected val isPartitioned = description.partitionColumns.nonEmpty /** Flag saying whether or not the data to be written out is bucketed. */ protected val isBucketed = description.bucketSpec.isDefined assert(isPartitioned || isBucketed, s\"\"\"DynamicPartitionWriteTask should be used for writing out data that's either |partitioned or bucketed. In this case neither is true. |WriteJobDescription: $description \"\"\".stripMargin) /** Number of records in current file. */ protected var recordsInFile: Long = _ /** * File counter for writing current partition or bucket. For same partition or bucket, * we may have more than one file, due to number of records limit per file. */ protected var fileCounter: Int = _ /** Extracts the partition values out of an input row. */ protected lazy val getPartitionValues: InternalRow => UnsafeRow = { val proj = UnsafeProjection.create(description.partitionColumns, description.allColumns) row => proj(row) } /** Expression that given partition columns builds a path string like: col1=val/col2=val/... */ private lazy val partitionPathExpression: Expression = Concat( description.partitionColumns.zipWithIndex.flatMap { case (c, i) => val partitionName = ScalaUDF( ExternalCatalogUtils.getPartitionPathString _, StringType, Seq(Literal(c.name), Cast(c, StringType, Option(description.timeZoneId)))) if (i == 0) Seq(partitionName) else Seq(Literal(Path.SEPARATOR), partitionName) }) /** * Evaluates the `partitionPathExpression` above on a row of `partitionValues` and returns * the partition string. */ private lazy val getPartitionPath: InternalRow => String = { val proj = UnsafeProjection.create(Seq(partitionPathExpression), description.partitionColumns) row => proj(row).getString(0) } /** Given an input row, returns the corresponding `bucketId` */ protected lazy val getBucketId: InternalRow => Int = { val proj = UnsafeProjection.create(Seq(description.bucketSpec.get.bucketIdExpression), description.allColumns) row => proj(row).getInt(0) } /** Returns the data columns to be written given an input row */ protected val getOutputRow = UnsafeProjection.create(description.dataColumns, description.allColumns) /** * Opens a new OutputWriter given a partition key and/or a bucket id. * If bucket id is specified, we will append it to the end of the file name, but before the * file extension, e.g. part-r-00009-ea518ad4-455a-4431-b471-d24e03814677-00002.gz.parquet * * @param partitionValues the partition which all tuples being written by this OutputWriter * belong to * @param bucketId the bucket which all tuples being written by this OutputWriter belong to * @param closeCurrentWriter close and release resource for current writer */ protected def renewCurrentWriter( partitionValues: Option[InternalRow], bucketId: Option[Int], closeCurrentWriter: Boolean): Unit = { recordsInFile = 0 if (closeCurrentWriter) { releaseCurrentWriter() } val partDir = partitionValues.map(getPartitionPath(_)) partDir.foreach(updatedPartitions.add) val bucketIdStr = bucketId.map(BucketingUtils.bucketIdToString).getOrElse(\"\") // The prefix and suffix must be in a form that matches our bucketing format. See BucketingUtils // for details. The prefix is required to represent bucket id when writing Hive-compatible // bucketed table. val prefix = bucketId match { case Some(id) => description.bucketSpec.get.bucketFileNamePrefix(id) case _ => \"\" } val suffix = f\"$bucketIdStr.c$fileCounter%03d\" + description.outputWriterFactory.getFileExtension(taskAttemptContext) val fileNameSpec = FileNameSpec(prefix, suffix) val customPath = partDir.flatMap { dir => description.customPartitionLocations.get(PartitioningUtils.parsePathFragment(dir)) } val currentPath = if (customPath.isDefined) { committer.newTaskTempFileAbsPath(taskAttemptContext, customPath.get, fileNameSpec) } else { committer.newTaskTempFile(taskAttemptContext, partDir, fileNameSpec) } currentWriter = description.outputWriterFactory.newInstance( path = currentPath, dataSchema = description.dataColumns.toStructType, context = taskAttemptContext) statsTrackers.foreach(_.newFile(currentPath)) } /** * Open a new output writer when number of records exceeding limit. * * @param partitionValues the partition which all tuples being written by this `OutputWriter` * belong to * @param bucketId the bucket which all tuples being written by this `OutputWriter` belong to */ protected def renewCurrentWriterIfTooManyRecords( partitionValues: Option[InternalRow], bucketId: Option[Int]): Unit = { // Exceeded the threshold in terms of the number of records per file. // Create a new file by increasing the file counter. fileCounter += 1 assert(fileCounter < MAX_FILE_COUNTER, s\"File counter $fileCounter is beyond max value $MAX_FILE_COUNTER\") renewCurrentWriter(partitionValues, bucketId, closeCurrentWriter = true) } /** * Writes the given record with current writer. * * @param record The record to write */ protected def writeRecord(record: InternalRow): Unit = { val outputRow = getOutputRow(record) currentWriter.write(outputRow) statsTrackers.foreach(_.newRow(currentWriter.path, outputRow)) recordsInFile += 1 } } /** * Dynamic partition writer with single writer, meaning only one writer is opened at any time for * writing. The records to be written are required to be sorted on partition and/or bucket * column(s) before writing. */ class DynamicPartitionDataSingleWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, customMetrics: Map[String, SQLMetric] = Map.empty) extends BaseDynamicPartitionDataWriter(description, taskAttemptContext, committer, customMetrics) { private var currentPartitionValues: Option[UnsafeRow] = None private var currentBucketId: Option[Int] = None override def write(record: InternalRow): Unit = { val nextPartitionValues = if (isPartitioned) Some(getPartitionValues(record)) else None val nextBucketId = if (isBucketed) Some(getBucketId(record)) else None if (currentPartitionValues != nextPartitionValues || currentBucketId != nextBucketId) { // See a new partition or bucket - write to a new partition dir (or a new bucket file). if (isPartitioned && currentPartitionValues != nextPartitionValues) { currentPartitionValues = Some(nextPartitionValues.get.copy()) statsTrackers.foreach(_.newPartition(currentPartitionValues.get)) } if (isBucketed) { currentBucketId = nextBucketId } fileCounter = 0 renewCurrentWriter(currentPartitionValues, currentBucketId, closeCurrentWriter = true) } else if (description.maxRecordsPerFile > 0 && recordsInFile >= description.maxRecordsPerFile) { renewCurrentWriterIfTooManyRecords(currentPartitionValues, currentBucketId) } writeRecord(record) } } /** * Dynamic partition writer with concurrent writers, meaning multiple concurrent writers are opened * for writing. * * The process has the following steps: * - Step 1: Maintain a map of output writers per each partition and/or bucket columns. Keep all * writers opened and write rows one by one. * - Step 2: If number of concurrent writers exceeds limit, sort rest of rows on partition and/or * bucket column(s). Write rows one by one, and eagerly close the writer when finishing * each partition and/or bucket. * * Caller is expected to call `writeWithIterator()` instead of `write()` to write records. */ class DynamicPartitionDataConcurrentWriter( description: WriteJobDescription, taskAttemptContext: TaskAttemptContext, committer: FileCommitProtocol, concurrentOutputWriterSpec: ConcurrentOutputWriterSpec, customMetrics: Map[String, SQLMetric] = Map.empty) extends BaseDynamicPartitionDataWriter(description, taskAttemptContext, committer, customMetrics) with Logging { /** Wrapper class to index a unique concurrent output writer. */ private case class WriterIndex( var partitionValues: Option[UnsafeRow], var bucketId: Option[Int]) /** Wrapper class for status of a unique concurrent output writer. */ private class WriterStatus( var outputWriter: OutputWriter, var recordsInFile: Long, var fileCounter: Int) /** * State to indicate if we are falling back to sort-based writer. * Because we first try to use concurrent writers, its initial value is false. */ private var sorted: Boolean = false private val concurrentWriters = mutable.HashMap[WriterIndex, WriterStatus]() /** * The index for current writer. Intentionally make the index mutable and reusable. * Avoid JVM GC issue when many short-living `WriterIndex` objects are created * if switching between concurrent writers frequently. */ private val currentWriterId = WriterIndex(None, None) /** * Release resources for all concurrent output writers. */ override protected def releaseResources(): Unit = { currentWriter = null concurrentWriters.values.foreach(status => { if (status.outputWriter != null) { try { status.outputWriter.close() } finally { status.outputWriter = null } } }) concurrentWriters.clear() } override def write(record: InternalRow): Unit = { val nextPartitionValues = if (isPartitioned) Some(getPartitionValues(record)) else None val nextBucketId = if (isBucketed) Some(getBucketId(record)) else None if (currentWriterId.partitionValues != nextPartitionValues || currentWriterId.bucketId != nextBucketId) { // See a new partition or bucket - write to a new partition dir (or a new bucket file). if (currentWriter != null) { if (!sorted) { // Update writer status in concurrent writers map, because the writer is probably needed // again later for writing other rows. updateCurrentWriterStatusInMap() } else { // Remove writer status in concurrent writers map and release current writer resource, // because the writer is not needed any more. concurrentWriters.remove(currentWriterId) releaseCurrentWriter() } } if (isBucketed) { currentWriterId.bucketId = nextBucketId } if (isPartitioned && currentWriterId.partitionValues != nextPartitionValues) { currentWriterId.partitionValues = Some(nextPartitionValues.get.copy()) if (!concurrentWriters.contains(currentWriterId)) { statsTrackers.foreach(_.newPartition(currentWriterId.partitionValues.get)) } } setupCurrentWriterUsingMap() } if (description.maxRecordsPerFile > 0 && recordsInFile >= description.maxRecordsPerFile) { renewCurrentWriterIfTooManyRecords(currentWriterId.partitionValues, currentWriterId.bucketId) // Update writer status in concurrent writers map, as a new writer is created. updateCurrentWriterStatusInMap() } writeRecord(record) } /** * Write iterator of records with concurrent writers. */ override def writeWithIterator(iterator: Iterator[InternalRow]): Unit = { var count = 0L while (iterator.hasNext && !sorted) { writeWithMetrics(iterator.next(), count) count += 1 } CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) if (iterator.hasNext) { count = 0L clearCurrentWriterStatus() val sorter = concurrentOutputWriterSpec.createSorter() val sortIterator = sorter.sort(iterator.asInstanceOf[Iterator[UnsafeRow]]) while (sortIterator.hasNext) { writeWithMetrics(sortIterator.next(), count) count += 1 } CustomMetrics.updateMetrics(currentMetricsValues, customMetrics) } } /** * Update current writer status in map. */ private def updateCurrentWriterStatusInMap(): Unit = { val status = concurrentWriters(currentWriterId) status.outputWriter = currentWriter status.recordsInFile = recordsInFile status.fileCounter = fileCounter } /** * Retrieve writer in map, or create a new writer if not exists. */ private def setupCurrentWriterUsingMap(): Unit = { if (concurrentWriters.contains(currentWriterId)) { val status = concurrentWriters(currentWriterId) currentWriter = status.outputWriter recordsInFile = status.recordsInFile fileCounter = status.fileCounter } else { fileCounter = 0 renewCurrentWriter( currentWriterId.partitionValues, currentWriterId.bucketId, closeCurrentWriter = false) if (!sorted) { assert(concurrentWriters.size <= concurrentOutputWriterSpec.maxWriters, s\"Number of concurrent output file writers is ${concurrentWriters.size} \" + s\" which is beyond max value ${concurrentOutputWriterSpec.maxWriters}\") } else { assert(concurrentWriters.size <= concurrentOutputWriterSpec.maxWriters + 1, s\"Number of output file writers after sort is ${concurrentWriters.size} \" + s\" which is beyond max value ${concurrentOutputWriterSpec.maxWriters + 1}\") } concurrentWriters.put( currentWriterId.copy(), new WriterStatus(currentWriter, recordsInFile, fileCounter)) if (concurrentWriters.size >= concurrentOutputWriterSpec.maxWriters && !sorted) { // Fall back to sort-based sequential writer mode. logInfo(s\"Number of concurrent writers ${concurrentWriters.size} reaches the threshold. \" + \"Fall back from concurrent writers to sort-based sequential writer. You may change \" + s\"threshold with configuration ${SQLConf.MAX_CONCURRENT_OUTPUT_FILE_WRITERS.key}\") sorted = true } } } /** * Clear the current writer status in map. */ private def clearCurrentWriterStatus(): Unit = { if (currentWriterId.partitionValues.isDefined || currentWriterId.bucketId.isDefined) { updateCurrentWriterStatusInMap() } currentWriterId.partitionValues = None currentWriterId.bucketId = None currentWriter = null recordsInFile = 0 fileCounter = 0 } } /** * Bucketing specification for all the write tasks. * * @param bucketIdExpression Expression to calculate bucket id based on bucket column(s). * @param bucketFileNamePrefix Prefix of output file name based on bucket id. */ case class WriterBucketSpec( bucketIdExpression: Expression, bucketFileNamePrefix: Int => String) /** A shared job description for all the write tasks. */ class WriteJobDescription( val uuid: String, // prevent collision between different (appending) write jobs val serializableHadoopConf: SerializableConfiguration, val outputWriterFactory: OutputWriterFactory, val allColumns: Seq[Attribute], val dataColumns: Seq[Attribute], val partitionColumns: Seq[Attribute], val bucketSpec: Option[WriterBucketSpec], val path: String, val customPartitionLocations: Map[TablePartitionSpec, String], val maxRecordsPerFile: Long, val timeZoneId: String, val statsTrackers: Seq[WriteJobStatsTracker]) extends Serializable { assert(AttributeSet(allColumns) == AttributeSet(partitionColumns ++ dataColumns), s\"\"\" |All columns: ${allColumns.mkString(\", \")} |Partition columns: ${partitionColumns.mkString(\", \")} |Data columns: ${dataColumns.mkString(\", \")} \"\"\".stripMargin) } /** The result of a successful write task. */ case class WriteTaskResult(commitMsg: TaskCommitMessage, summary: ExecutedWriteSummary) extends WriterCommitMessage /** * Wrapper class for the metrics of writing data out. * * @param updatedPartitions the partitions updated during writing data out. Only valid * for dynamic partition. * @param stats one `WriteTaskStats` object for every `WriteJobStatsTracker` that the job had. */ case class ExecutedWriteSummary( updatedPartitions: Set[String], stats: Seq[WriteTaskStats])",
            "## CLASS: org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec# (implementation)\n*/ case class OverwritePartitionsDynamicExec( query: SparkPlan, refreshCache: () => Unit, write: Write) extends V2ExistingTableWriteExec { override protected def withNewChildInternal(newChild: SparkPlan): OverwritePartitionsDynamicExec = copy(query = newChild) } /** * Physical plan node to replace data in existing tables. */ case class ReplaceDataExec( query: SparkPlan, refreshCache: () => Unit, write: Write) extends V2ExistingTableWriteExec { override val stringArgs: Iterator[Any] = Iterator(query, write) override protected def withNewChildInternal(newChild: SparkPlan): ReplaceDataExec = { copy(query = newChild) } } case class WriteToDataSourceV2Exec( batchWrite: BatchWrite, refreshCache: () => Unit, query: SparkPlan, writeMetrics: Seq[CustomMetric]) extends V2TableWriteExec { override val customMetrics: Map[String, SQLMetric] = writeMetrics.map { customMetric => customMetric.name() -> SQLMetrics.createV2CustomMetric(sparkContext, customMetric) }.toMap override protected def run(): Seq[InternalRow] = { val writtenRows = writeWithV2(batchWrite) refreshCache() writtenRows } override protected def withNewChildInternal(newChild: SparkPlan): WriteToDataSourceV2Exec = copy(query = newChild) } trait V2ExistingTableWriteExec extends V2TableWriteExec { def refreshCache: () => Unit def write: Write override val customMetrics: Map[String, SQLMetric] = write.supportedCustomMetrics().map { customMetric => customMetric.name() -> SQLMetrics.createV2CustomMetric(sparkContext, customMetric) }.toMap override protected def run(): Seq[InternalRow] = { val writtenRows = writeWithV2(write.toBatch) refreshCache() writtenRows } } /** * The base physical plan for writing data into data source v2. */ trait V2TableWriteExec extends V2CommandExec with UnaryExecNode { def query: SparkPlan var commitProgress: Option[StreamWriterCommitProgress] = None override def child: SparkPlan = query override def output: Seq[Attribute] = Nil protected val customMetrics: Map[String, SQLMetric] = Map.empty override lazy val metrics = customMetrics protected def writeWithV2(batchWrite: BatchWrite): Seq[InternalRow] = { val rdd: RDD[InternalRow] = { val tempRdd = query.execute() // SPARK-23271 If we are attempting to write a zero partition rdd, create a dummy single // partition rdd to make sure we at least set up one write task to write the metadata. if (tempRdd.partitions.length == 0) { sparkContext.parallelize(Array.empty[InternalRow], 1) } else { tempRdd } } val writerFactory = batchWrite.createBatchWriterFactory( PhysicalWriteInfoImpl(rdd.getNumPartitions)) val useCommitCoordinator = batchWrite.useCommitCoordinator val messages = new Array[WriterCommitMessage](rdd.partitions.length) val totalNumRowsAccumulator = new LongAccumulator() logInfo(s\"Start processing data source write support: $batchWrite. \" + s\"The input RDD has ${messages.length} partitions.\") // Avoid object not serializable issue. val writeMetrics: Map[String, SQLMetric] = customMetrics try { sparkContext.runJob( rdd, (context: TaskContext, iter: Iterator[InternalRow]) => DataWritingSparkTask.run(writerFactory, context, iter, useCommitCoordinator, writeMetrics), rdd.partitions.indices, (index, result: DataWritingSparkTaskResult) => { val commitMessage = result.writerCommitMessage messages(index) = commitMessage totalNumRowsAccumulator.add(result.numRows) batchWrite.onDataWriterCommit(commitMessage) } ) logInfo(s\"Data source write support $batchWrite is committing.\") batchWrite.commit(messages) logInfo(s\"Data source write support $batchWrite committed.\") commitProgress = Some(StreamWriterCommitProgress(totalNumRowsAccumulator.value)) } catch { case cause: Throwable => logError(s\"Data source write support $batchWrite is aborting.\") try { batchWrite.abort(messages) } catch { case t: Throwable => logError(s\"Data source write support $batchWrite failed to abort.\") cause.addSuppressed(t) throw QueryExecutionErrors.writingJobFailedError(cause) } logError(s\"Data source write support $batchWrite aborted.\") cause match { // Only wrap non fatal exceptions. case NonFatal(e) => throw QueryExecutionErrors.writingJobAbortedError(e) case _ => throw cause } } Nil } } object DataWritingSparkTask extends Logging { def run( writerFactory: DataWriterFactory, context: TaskContext, iter: Iterator[InternalRow], useCommitCoordinator: Boolean, customMetrics: Map[String, SQLMetric]): DataWritingSparkTaskResult = { val stageId = context.stageId() val stageAttempt = context.stageAttemptNumber() val partId = context.partitionId() val taskId = context.taskAttemptId() val attemptId = context.attemptNumber() val dataWriter = writerFactory.createWriter(partId, taskId) var count = 0L // write the data and commit this writer. Utils.tryWithSafeFinallyAndFailureCallbacks(block = { while (iter.hasNext) { if (count % CustomMetrics.NUM_ROWS_PER_UPDATE == 0) { CustomMetrics.updateMetrics(dataWriter.currentMetricsValues, customMetrics) } // Count is here. count += 1 dataWriter.write(iter.next()) } CustomMetrics.updateMetrics(dataWriter.currentMetricsValues, customMetrics) val msg = if (useCommitCoordinator) { val coordinator = SparkEnv.get.outputCommitCoordinator val commitAuthorized = coordinator.canCommit(stageId, stageAttempt, partId, attemptId) if (commitAuthorized) { logInfo(s\"Commit authorized for partition $partId (task $taskId, attempt $attemptId, \" + s\"stage $stageId.$stageAttempt)\") dataWriter.commit() } else { val commitDeniedException = QueryExecutionErrors.commitDeniedError( partId, taskId, attemptId, stageId, stageAttempt) logInfo(commitDeniedException.getMessage) // throwing CommitDeniedException will trigger the catch block for abort throw commitDeniedException } } else { logInfo(s\"Writer for partition ${context.partitionId()} is committing.\") dataWriter.commit() } logInfo(s\"Committed partition $partId (task $taskId, attempt $attemptId, \" + s\"stage $stageId.$stageAttempt)\") DataWritingSparkTaskResult(count, msg) })(catchBlock = { // If there is an error, abort this writer logError(s\"Aborting commit for partition $partId (task $taskId, attempt $attemptId, \" + s\"stage $stageId.$stageAttempt)\") dataWriter.abort() logError(s\"Aborted commit for partition $partId (task $taskId, attempt $attemptId, \" + s\"stage $stageId.$stageAttempt)\") }, finallyBlock = { dataWriter.close() }) } } private[v2] trait TableWriteExecHelper extends V2TableWriteExec with SupportsV1Write { protected def writeToTable( catalog: TableCatalog, table: Table, writeOptions: CaseInsensitiveStringMap, ident: Identifier): Seq[InternalRow] = { Utils.tryWithSafeFinallyAndFailureCallbacks({ table match { case table: SupportsWrite => val info = LogicalWriteInfoImpl( queryId = UUID.randomUUID().toString, query.schema, writeOptions) val writeBuilder = table.newWriteBuilder(info) val write = writeBuilder.build() val writtenRows = write match { case v1: V1Write => writeWithV1(v1.toInsertableRelation) case v2 => writeWithV2(v2.toBatch) } table match { case st: StagedTable => st.commitStagedChanges() case _ => } writtenRows case _ => // Table does not support writes - staged changes are also rolled back below if table // is staging. throw QueryExecutionErrors.unsupportedTableWritesError(ident) } })(catchBlock = { table match { // Failure rolls back the staged writes and metadata changes. case st: StagedTable => st.abortStagedChanges() case _ => catalog.dropTable(ident) } }) } } private[v2] case class DataWritingSparkTaskResult( numRows: Long, writerCommitMessage: WriterCommitMessage) /** * Sink progress information collected after commit. */ private[sql] case class StreamWriterCommitProgress(numOutputRows: Long)"
          ],
          "tokens": 0,
          "more_info": "Opened README.md but it didn't contain any information about subject"
        },
        "without_mcp": {
          "answer": "# How the project manages external data connectivity and writing\n\nThis repository is Apache Spark 3.3.0. Spark manages external data connectivity and writing primarily through Spark SQL Data Sources with the DataFrame/Dataset API, plus Hadoop-compatible filesystems and optional connectors.\n\n## Core APIs\n- DataFrameReader (spark.read / SparkSession.read)\n  - load data from external sources via: format(), options, schema(), load(path) or dedicated helpers (csv/json/parquet/orc/text, etc.).\n- DataFrameWriter (df.write)\n  - write data to external systems via: format(), options, mode(), save(path) or dedicated helpers (csv/json/parquet/orc/text, etc.).\n- SQL\n  - CREATE TABLE USING, INSERT INTO/OVERWRITE, and SELECT ... FROM file-based sources directly using the datasource V1/V2 implementations.\n\n## Supported source/sink types (high level)\n- File-based formats (through Hadoop FS APIs):\n  - Parquet, ORC, JSON, CSV, Text, Avro, Whole binary files.\n  - Works with any Hadoop-compatible filesystem (local FS, HDFS, S3, Azure Blob/ADLS, GCS, etc.) configured via Hadoop/Spark settings.\n- JDBC: relational databases via the built-in JDBC data source.\n- Hive tables: via Hive support (metastore) when enabled.\n- Structured Streaming sources/sinks: file sources/sinks, plus ecosystem connectors (e.g., Kafka) when added.\n\nSee documentation under docs/sql-data-sources*.md in this repo for details and options.\n\n## Connecting to files and object stores\nSpark uses Hadoops FileSystem abstraction. Paths like hdfs://, s3a://, wasbs:///abfs://, gs:// are resolved by corresponding Hadoop filesystem implementations.\n- Configure credentials/endpoints using Hadoop core-site.xml, hdfs-site.xml, and Spark configs (spark.hadoop.*), or environment variables per provider.\n- Generic file source options (docs/sql-data-sources-generic-options.md) include:\n  - ignoreCorruptFiles, ignoreMissingFiles\n  - pathGlobFilter\n  - recursiveFileLookup\n  - compression (codec specific to formats, e.g., gzip, snappy, zstd)\n\nExamples:\n- Read Parquet: spark.read.parquet(\"s3a://bucket/path/\")\n- Write ORC: df.write.mode(\"overwrite\").orc(\"abfs://container@account.dfs.core.windows.net/path\")\n\n## JDBC connectivity\nUse format(\"jdbc\") with options or the jdbc helper.\nKey options:\n- url: JDBC URL (e.g., jdbc:postgresql://host:5432/db)\n- dbtable: table or query (use (select ...) t alias for subqueries)\n- driver: JDBC driver class\n- user, password: authentication\n- partitionColumn, lowerBound, upperBound, numPartitions: parallel reads\n\nExamples:\n- Read: spark.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", \"public.sales\").option(\"user\", user).option(\"password\", pwd).load()\n- Write: df.write.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", \"public.sales_out\").option(\"user\", user).option(\"password\", pwd).mode(\"append\").save()\n\n## Hive tables\nWhen Hive support is enabled, Spark can read/write Hive-managed and external tables:\n- CREATE TABLE USING or CREATE TABLE ... STORED AS ...\n- saveAsTable, insertInto\n- Interacts with the Hive metastore; storage formats typically Parquet/ORC.\n\n## Writing controls\n- Save modes (docs/sql-data-sources-load-save-functions.md#save-modes):\n  - errorIfExists (default), append, overwrite, ignore.\n- Partitioning, bucketing, sorting:\n  - df.write.partitionBy(\"col1\", ...)\n  - df.write.bucketBy(n, \"col\").sortBy(\"col2\") (saveAsTable required for bucketing)\n  - Affects directory layout and query performance.\n- Overwrite behavior for partitions:\n  - spark.sql.sources.partitionOverwriteMode = dynamic | static\n  - Dynamic overwrite replaces only matched partitions.\n\n## Format-specific options\nEach format has documented options:\n- Parquet: mergeSchema, compression, datetimeRebaseModeInWrite, etc. (docs/sql-data-sources-parquet.md)\n- ORC: compression, etc. (docs/sql-data-sources-orc.md)\n- JSON: multiline, encoding, dateFormat, etc. (docs/sql-data-sources-json.md)\n- CSV: header, inferSchema, sep, quote, escape, nullValue, etc. (docs/sql-data-sources-csv.md)\n- Avro: various read/write options (docs/sql-data-sources-avro.md)\n\n## Structured Streaming\n- Read: spark.readStream.format(\"parquet\"|\"json\"|\"csv\"|...).option(...).load(path)\n- Write: df.writeStream.format(\"parquet\"|\"console\"|\"memory\"|...).option(\"checkpointLocation\", path).start(outputPath)\n- Many external streaming systems (e.g., Kafka) are available via separate connectors; they are configured with format(\"kafka\") and options like kafka.bootstrap.servers, subscribe, etc., when the connector is present.\n\n## Security and credentials\n- Filesystems: authenticate via underlying Hadoop/Cloud provider mechanisms (config files, tokens, instance profiles/managed identities, environment variables). Configure in Spark with spark.hadoop.* or provider-specific settings.\n- JDBC: supply user/password or token-based mechanisms via driver-specific properties. Avoid embedding secrets in code; use secure config/secret management.\n\n## Programmatic patterns\n- Reader:\n  - spark.read.format(fmt).options(opts).schema(schemaOpt).load(pathOpt)\n  - Convenience: spark.read.parquet/json/csv/orc/text\n- Writer:\n  - df.write.format(fmt).options(opts).mode(mode).save(path)\n  - Convenience: df.write.parquet/json/csv/orc/text\n  - Tables: df.write.saveAsTable(name), insertInto(name)\n\n## Troubleshooting\n- See docs/sql-data-sources-troubleshooting.md for common issues:\n  - Schema inference pitfalls, corrupt/missing files behavior, and performance tips.\n  - Use spark.sql.shuffle.partitions, repartition/coalesce, and predicate pushdown where applicable.\n\n## References inside this repo\n- docs/sql-data-sources.md (overview)\n- docs/sql-data-sources-load-save-functions.md (API, modes, tables, partitioning)\n- docs/sql-data-sources-generic-options.md (common options)\n- docs/sql-data-sources-parquet.md, -orc.md, -json.md, -csv.md, -text.md, -avro.md\n- docs/sql-data-sources-hive-tables.md, -jdbc.md\n- structured-streaming-programming-guide.md (streaming specifics)\n\nIn summary, Spark manages external connectivity via its unified Data Source API, leveraging Hadoop-compatible filesystems for file/object stores and built-in JDBC/Hive integrations, with rich per-format options, writer controls (modes, partitioning/bucketing), and streaming read/write APIs.",
          "answer_time": 58,
          "tokens": 0
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q008",
      "question": "How does the project manage memory allocation?",
      "category": "general",
      "parameters": {
        "kinds": [
          "CLASS",
          "METHOD",
          "INTERFACE",
          "TRAIT"
        ],
        "keywords": [
          "memory",
          "alloc",
          "allocation",
          "manager",
          "allocator",
          "pool"
        ],
        "top_nodes": 8,
        "max_neighbors": 5
      },
      "ground_truth_contexts": [
        "import org.apache.spark.internal.config._ import org.apache.spark.network.util.JavaUtils import org.apache.spark.util.{ThreadUtils, Utils} private[spark] class DriverLogger(conf: SparkConf) extends Logging { private val UPLOAD_CHUNK_SIZE = 1024 * 1024 private val UPLOAD_INTERVAL_IN_SECS = 5 private val DEFAULT_LAYOUT = \"%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\" private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort) private val localLogFile: String = FileUtils.getFile( Utils.getLocalDir(conf), DriverLogger.DRIVER_LOG_DIR, DriverLogger.DRIVER_LOG_FILE).getAbsolutePath() private var writer: Option[DfsAsyncWriter] = None addLogAppender() private def addLogAppender(): Unit = { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val layout = if (conf.contains(DRIVER_LOG_LAYOUT)) { PatternLayout.newBuilder().withPattern(conf.get(DRIVER_LOG_LAYOUT).get).build() } else { PatternLayout.newBuilder().withPattern(DEFAULT_LAYOUT).build() } val config = logger.getContext.getConfiguration() def log4jFileAppender() = { // SPARK-37853: We can't use the chained API invocation mode because // `AbstractFilterable.Builder.asBuilder()` method will return `Any` in Scala. val builder: Log4jFileAppender.Builder[_] = Log4jFileAppender.newBuilder() builder.withAppend(false) builder.withBufferedIo(false) builder.setConfiguration(config) builder.withFileName(localLogFile) builder.setIgnoreExceptions(false) builder.setLayout(layout) builder.setName(DriverLogger.APPENDER_NAME) builder.build() } val fa = log4jFileAppender() logger.addAppender(fa) fa.start() logInfo(s\"Added a local log appender at: $localLogFile\") } def startSync(hadoopConf: Configuration): Unit = { try { // Setup a writer which moves the local file to hdfs continuously val appId = Utils.sanitizeDirName(conf.getAppId) writer = Some(new DfsAsyncWriter(appId, hadoopConf)) } catch { case e: Exception => logError(s\"Could not persist driver logs to dfs\", e) } } def stop(): Unit = { try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "*/ private[spark] case class ErrorInfo(message: Seq[String], sqlState: Option[String]) { // For compatibility with multi-line error messages @JsonIgnore val messageFormat: String = message.mkString(\"\\n\") } /** * Companion object used by instances of [[SparkThrowable]] to access error class information and * construct error messages. */ private[spark] object SparkThrowableHelper { val errorClassesUrl: URL = Utils.getSparkClassLoader.getResource(\"error/error-classes.json\") val errorClassToInfoMap: SortedMap[String, ErrorInfo] = { val mapper: JsonMapper = JsonMapper.builder() .addModule(DefaultScalaModule) .build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }",
        "private[streaming] case class ReceiverErrorInfo( lastErrorMessage: String = \"\", lastError: String = \"\", lastErrorTime: Long = -1L) /** * Class having information about a receiver. * * @param receiverId the unique receiver id * @param state the current Receiver state * @param scheduledLocations the scheduled locations provided by ReceiverSchedulingPolicy * @param runningExecutor the running executor if the receiver is active * @param name the receiver name * @param endpoint the receiver endpoint. It can be used to send messages to the receiver * @param errorInfo the receiver error information if it fails */ private[streaming] case class ReceiverTrackingInfo( receiverId: Int, state: ReceiverState, scheduledLocations: Option[Seq[TaskLocation]], runningExecutor: Option[ExecutorCacheTaskLocation], name: Option[String] = None, endpoint: Option[RpcEndpointRef] = None, errorInfo: Option[ReceiverErrorInfo] = None) { def toReceiverInfo: ReceiverInfo = ReceiverInfo( receiverId, name.getOrElse(\"\"), state == ReceiverState.ACTIVE, location = runningExecutor.map(_.host).getOrElse(\"\"), executorId = runningExecutor.map(_.executorId).getOrElse(\"\"), lastErrorMessage = errorInfo.map(_.lastErrorMessage).getOrElse(\"\"), lastError = errorInfo.map(_.lastError).getOrElse(\"\"), lastErrorTime = errorInfo.map(_.lastErrorTime).getOrElse(-1L) ) }",
        ".build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }"
      ],
      "key_entities": [
        "org/apache/spark/util/logging/DriverLogger#",
        "org/apache/spark/util/logging/DriverLogger#DfsAsyncWriter#",
        "org/apache/spark/ErrorInfo#",
        "org/apache/spark/streaming/scheduler/ReceiverErrorInfo#",
        "local2:core/src/main/scala/org/apache/spark/ErrorInfo.scala"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, here's how this Apache Spark project manages memory allocation:\n\n## Memory Management Architecture\n\nThe project uses a **dual-pool memory management system** with separate pools for execution and storage:\n\n### 1. **ExecutionMemoryPool**\nManages memory for computational tasks with the following characteristics:\n\n- **Per-Task Tracking**: Uses `memoryForTask` HashMap to track memory consumption by `taskAttemptId`\n- **Fair Allocation**: Implements a fairness algorithm where each task can acquire up to `1/N` of the pool (where N = number of active tasks)\n- **Blocking with Minimum Guarantee**: Tasks are guaranteed at least `1/(2*N)` of the pool before blocking. If a task can't get this minimum, it waits via `lock.wait()` until memory is freed\n- **Dynamic Pool Growth**: Supports callback `maybeGrowPool()` to potentially expand the pool by evicting cached blocks from storage\n- **Memory Modes**: Operates in either `ON_HEAP` or `OFF_HEAP` mode\n\n### 2. **StorageMemoryPool**\nManages memory for cached data blocks:\n\n- **Block-Based Allocation**: Acquires memory for specific `BlockId` instances\n- **Eviction Support**: Can evict existing blocks via `memoryStore.evictBlocksToFreeSpace()` when space is needed\n- **Shrinkage**: Supports `freeSpaceToShrinkPool()` to release memory back to execution pool\n- **Single Counter**: Tracks total usage with `_memoryUsed` variable\n\n### 3. **Chunked Buffer System**\nHandles large data efficiently:\n\n- **ChunkedByteBuffer**: Splits large data into multiple `ByteBuffer` chunks to avoid JVM array size limits\n- **Memory-Mapped Alternative**: Explicitly avoids memory-mapping files to prevent lifecycle conflicts (SPARK-25422)\n- **Direct vs Heap Buffers**: Supports both heap-allocated and off-heap direct buffers based on storage level\n- **Chunk Size Control**: Uses configurable `BUFFER_WRITE_CHUNK_SIZE` to limit temporary direct buffer growth and prevent memory leaks\n\n### 4. **Block Storage Strategy**\nThe `BlockManager` coordinates memory allocation across storage tiers:\n\n- **Memory-First**: Attempts memory storage first, falls back to disk if needed\n- **Serialized vs Deserialized**: Can store blocks as Java objects (deserialized) or serialized bytes based on `StorageLevel`\n- **Replication**: Supports multi-replica storage for fault tolerance\n- **Cleanup**: Implements reference-based cleanup for temporary files via `RemoteBlockDownloadFileManager`\n\n### 5. **Key Allocation Patterns**\n\n**Acquiring Memory**:\n```\n1. Check if enough free memory exists\n2. If not, try to grow pool by evicting storage blocks\n3. Calculate per-task limits (max = poolSize/N, min = poolSize/2N)\n4. Grant as much as available within limits\n5. Block if can't meet minimum guarantee\n```\n\n**Releasing Memory**:\n```\n1. Decrement task's memory counter\n2. Remove task entry if memory reaches zero\n3. Call lock.notifyAll() to wake waiting tasks\n```\n\nThis design balances fairness, prevents starvation, and efficiently handles both small and large data blocks while supporting multiple memory modes and storage tiers.",
        "prompt_time": 16.167,
        "answer_time": 50,
        "context_tokens": 20879,
        "tokens": 0,
        "used_context": [
          "## CLASS: org/apache/spark/memory/ExecutionMemoryPool# (implementation)\n*/ private[memory] class ExecutionMemoryPool( lock: Object, memoryMode: MemoryMode ) extends MemoryPool(lock) with Logging { private[this] val poolName: String = memoryMode match { case MemoryMode.ON_HEAP => \"on-heap execution\" case MemoryMode.OFF_HEAP => \"off-heap execution\" } /** * Map from taskAttemptId -> memory consumption in bytes */ @GuardedBy(\"lock\") private val memoryForTask = new mutable.HashMap[Long, Long]() override def memoryUsed: Long = lock.synchronized { memoryForTask.values.sum } /** * Returns the memory consumption, in bytes, for the given task. */ def getMemoryUsageForTask(taskAttemptId: Long): Long = lock.synchronized { memoryForTask.getOrElse(taskAttemptId, 0L) } /** * Try to acquire up to `numBytes` of memory for the given task and return the number of bytes * obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. * * @param numBytes number of bytes to acquire * @param taskAttemptId the task attempt acquiring memory * @param maybeGrowPool a callback that potentially grows the size of this pool. It takes in * one parameter (Long) that represents the desired amount of memory by * which this pool should be expanded. * @param computeMaxPoolSize a callback that returns the maximum allowable size of this pool * at this given moment. This is not a field because the max pool * size is variable in certain cases. For instance, in unified * memory management, the execution pool can be expanded by evicting * cached blocks, thereby shrinking the storage pool. * * @return the number of bytes granted to the task. */ private[memory] def acquireMemory( numBytes: Long, taskAttemptId: Long, maybeGrowPool: Long => Unit = (additionalSpaceNeeded: Long) => (), computeMaxPoolSize: () => Long = () => poolSize): Long = lock.synchronized { assert(numBytes > 0, s\"invalid number of bytes requested: $numBytes\") // TODO: clean up this clunky method signature // Add this task to the taskMemory map just so we can keep an accurate count of the number // of active tasks, to let other tasks ramp down their memory in calls to `acquireMemory` if (!memoryForTask.contains(taskAttemptId)) { memoryForTask(taskAttemptId) = 0L // This will later cause waiting tasks to wake up and check numTasks again lock.notifyAll() } // Keep looping until we're either sure that we don't want to grant this request (because this // task would have more than 1 / numActiveTasks of the memory) or we have enough free // memory to give it (we always let each task get at least 1 / (2 * numActiveTasks)). // TODO: simplify this to limit each task to its own slot while (true) { val numActiveTasks = memoryForTask.keys.size val curMem = memoryForTask(taskAttemptId) // In every iteration of this loop, we should first try to reclaim any borrowed execution // space from storage. This is necessary because of the potential race condition where new // storage blocks may steal the free execution memory that this task was waiting for. maybeGrowPool(numBytes - memoryFree) // Maximum size the pool would have after potentially growing the pool. // This is used to compute the upper bound of how much memory each task can occupy. This // must take into account potential free memory as well as the amount this pool currently // occupies. Otherwise, we may run into SPARK-12155 where, in unified memory management, // we did not take into account space that could have been freed by evicting cached blocks. val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks) // How much we can grant this task; keep its share within 0 <= X <= 1 / numActiveTasks val maxToGrant = math.min(numBytes, math.max(0, maxMemoryPerTask - curMem)) // Only give it as much memory as is free, which might be none if it reached 1 / numTasks val toGrant = math.min(maxToGrant, memoryFree) // We want to let each task get at least 1 / (2 * numActiveTasks) before blocking; // if we can't give it this much now, wait for other tasks to free up memory // (this happens if older tasks allocated lots of memory before N grew) if (toGrant < numBytes && curMem + toGrant < minMemoryPerTask) { logInfo(s\"TID $taskAttemptId waiting for at least 1/2N of $poolName pool to be free\") lock.wait() } else { memoryForTask(taskAttemptId) += toGrant return toGrant } } 0L // Never reached } /** * Release `numBytes` of memory acquired by the given task. */ def releaseMemory(numBytes: Long, taskAttemptId: Long): Unit = lock.synchronized { val curMem = memoryForTask.getOrElse(taskAttemptId, 0L) val memoryToFree = if (curMem < numBytes) { logWarning( s\"Internal error: release called on $numBytes bytes but task only has $curMem bytes \" + s\"of memory from the $poolName pool\") curMem } else { numBytes } if (memoryForTask.contains(taskAttemptId)) { memoryForTask(taskAttemptId) -= memoryToFree if (memoryForTask(taskAttemptId) <= 0) { memoryForTask.remove(taskAttemptId) } } lock.notifyAll() // Notify waiters in acquireMemory() that memory has been freed } /** * Release all memory for the given task and mark it as inactive (e.g. when a task ends). * @return the number of bytes freed. */ def releaseAllMemoryForTask(taskAttemptId: Long): Long = lock.synchronized { val numBytesToFree = getMemoryUsageForTask(taskAttemptId) releaseMemory(numBytesToFree, taskAttemptId) numBytesToFree } }",
          "## CLASS: org/apache/spark/memory/StorageMemoryPool# (implementation)\n*/ private[memory] class StorageMemoryPool( lock: Object, memoryMode: MemoryMode ) extends MemoryPool(lock) with Logging { private[this] val poolName: String = memoryMode match { case MemoryMode.ON_HEAP => \"on-heap storage\" case MemoryMode.OFF_HEAP => \"off-heap storage\" } @GuardedBy(\"lock\") private[this] var _memoryUsed: Long = 0L override def memoryUsed: Long = lock.synchronized { _memoryUsed } private var _memoryStore: MemoryStore = _ def memoryStore: MemoryStore = { if (_memoryStore == null) { throw new IllegalStateException(\"memory store not initialized yet\") } _memoryStore } /** * Set the [[MemoryStore]] used by this manager to evict cached blocks. * This must be set after construction due to initialization ordering constraints. */ final def setMemoryStore(store: MemoryStore): Unit = { _memoryStore = store } /** * Acquire N bytes of memory to cache the given block, evicting existing ones if necessary. * * @return whether all N bytes were successfully granted. */ def acquireMemory(blockId: BlockId, numBytes: Long): Boolean = lock.synchronized { val numBytesToFree = math.max(0, numBytes - memoryFree) acquireMemory(blockId, numBytes, numBytesToFree) } /** * Acquire N bytes of storage memory for the given block, evicting existing ones if necessary. * * @param blockId the ID of the block we are acquiring storage memory for * @param numBytesToAcquire the size of this block * @param numBytesToFree the amount of space to be freed through evicting blocks * @return whether all N bytes were successfully granted. */ def acquireMemory( blockId: BlockId, numBytesToAcquire: Long, numBytesToFree: Long): Boolean = lock.synchronized { assert(numBytesToAcquire >= 0) assert(numBytesToFree >= 0) assert(memoryUsed <= poolSize) if (numBytesToFree > 0) { memoryStore.evictBlocksToFreeSpace(Some(blockId), numBytesToFree, memoryMode) } // NOTE: If the memory store evicts blocks, then those evictions will synchronously call // back into this StorageMemoryPool in order to free memory. Therefore, these variables // should have been updated. val enoughMemory = numBytesToAcquire <= memoryFree if (enoughMemory) { _memoryUsed += numBytesToAcquire } enoughMemory } def releaseMemory(size: Long): Unit = lock.synchronized { if (size > _memoryUsed) { logWarning(s\"Attempted to release $size bytes of storage \" + s\"memory when we only have ${_memoryUsed} bytes\") _memoryUsed = 0 } else { _memoryUsed -= size } } def releaseAllMemory(): Unit = lock.synchronized { _memoryUsed = 0 } /** * Free space to shrink the size of this storage memory pool by `spaceToFree` bytes. * Note: this method doesn't actually reduce the pool size but relies on the caller to do so. * * @return number of bytes to be removed from the pool's capacity. */ def freeSpaceToShrinkPool(spaceToFree: Long): Long = lock.synchronized { val spaceFreedByReleasingUnusedMemory = math.min(spaceToFree, memoryFree) val remainingSpaceToFree = spaceToFree - spaceFreedByReleasingUnusedMemory if (remainingSpaceToFree > 0) { // If reclaiming free memory did not adequately shrink the pool, begin evicting blocks: val spaceFreedByEviction = memoryStore.evictBlocksToFreeSpace(None, remainingSpaceToFree, memoryMode) // When a block is released, BlockManager.dropFromMemory() calls releaseMemory(), so we do // not need to decrement _memoryUsed here. However, we do need to decrement the pool size. spaceFreedByReleasingUnusedMemory + spaceFreedByEviction } else { spaceFreedByReleasingUnusedMemory } } }",
          "## CLASS: org/apache/spark/util/io/ChunkedByteBuffer# (implementation)\n*/ private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) { require(chunks != null, \"chunks must not be null\") require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\") // Chunk size in bytes private val bufferWriteChunkSize = Option(SparkEnv.get).map(_.conf.get(config.BUFFER_WRITE_CHUNK_SIZE)) .getOrElse(config.BUFFER_WRITE_CHUNK_SIZE.defaultValue.get).toInt private[this] var disposed: Boolean = false /** * This size of this buffer, in bytes. */ val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum def this(byteBuffer: ByteBuffer) = { this(Array(byteBuffer)) } /** * Write this buffer to a channel. */ def writeFully(channel: WritableByteChannel): Unit = { for (bytes <- getChunks()) { val originalLimit = bytes.limit() while (bytes.hasRemaining) { // If `bytes` is an on-heap ByteBuffer, the Java NIO API will copy it to a temporary direct // ByteBuffer when writing it out. This temporary direct ByteBuffer is cached per thread. // Its size has no limit and can keep growing if it sees a larger input ByteBuffer. This may // cause significant native memory leak, if a large direct ByteBuffer is allocated and // cached, as it's never released until thread exits. Here we write the `bytes` with // fixed-size slices to limit the size of the cached direct ByteBuffer. // Please refer to http://www.evanjones.ca/java-bytebuffer-leak.html for more details. val ioSize = Math.min(bytes.remaining(), bufferWriteChunkSize) bytes.limit(bytes.position() + ioSize) channel.write(bytes) bytes.limit(originalLimit) } } } /** * Wrap this in a custom \"FileRegion\" which allows us to transfer over 2 GB. */ def toNetty: ChunkedByteBufferFileRegion = { new ChunkedByteBufferFileRegion(this, bufferWriteChunkSize) } /** * Copy this buffer into a new byte array. * * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size. */ def toArray: Array[Byte] = { if (size >= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH) { throw new UnsupportedOperationException( s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\") } val byteChannel = new ByteArrayWritableChannel(size.toInt) writeFully(byteChannel) byteChannel.close() byteChannel.getData } /** * Convert this buffer to a ByteBuffer. If this buffer is backed by a single chunk, its underlying * data will not be copied. Instead, it will be duplicated. If this buffer is backed by multiple * chunks, the data underlying this buffer will be copied into a new byte buffer. As a result, it * is suggested to use this method only if the caller does not need to manage the memory * underlying this buffer. * * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size. */ def toByteBuffer: ByteBuffer = { if (chunks.length == 1) { chunks.head.duplicate() } else { ByteBuffer.wrap(toArray) } } /** * Creates an input stream to read data from this ChunkedByteBuffer. * * @param dispose if true, [[dispose()]] will be called at the end of the stream * in order to close any memory-mapped files which back this buffer. */ def toInputStream(dispose: Boolean = false): InputStream = { new ChunkedByteBufferInputStream(this, dispose) } /** * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer. */ def getChunks(): Array[ByteBuffer] = { chunks.map(_.duplicate()) } /** * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers. * The new buffer will share no resources with the original buffer. * * @param allocator a method for allocating byte buffers */ def copy(allocator: Int => ByteBuffer): ChunkedByteBuffer = { val copiedChunks = getChunks().map { chunk => val newChunk = allocator(chunk.limit()) newChunk.put(chunk) newChunk.flip() newChunk } new ChunkedByteBuffer(copiedChunks) } /** * Attempt to clean up any ByteBuffer in this ChunkedByteBuffer which is direct or memory-mapped. * See [[StorageUtils.dispose]] for more information. */ def dispose(): Unit = { if (!disposed) { chunks.foreach(StorageUtils.dispose) disposed = true } } } private[spark] object ChunkedByteBuffer { def fromManagedBuffer(data: ManagedBuffer): ChunkedByteBuffer = { data match { case f: FileSegmentManagedBuffer => fromFile(f.getFile, f.getOffset, f.getLength) case e: EncryptedManagedBuffer => e.blockData.toChunkedByteBuffer(ByteBuffer.allocate _) case other => new ChunkedByteBuffer(other.nioByteBuffer()) } } def fromFile(file: File): ChunkedByteBuffer = { fromFile(file, 0, file.length()) } private def fromFile( file: File, offset: Long, length: Long): ChunkedByteBuffer = { // We do *not* memory map the file, because we may end up putting this into the memory store, // and spark currently is not expecting memory-mapped buffers in the memory store, it conflicts // with other parts that manage the lifecycle of buffers and dispose them. See SPARK-25422. val is = new FileInputStream(file) ByteStreams.skipFully(is, offset) val in = new LimitedInputStream(is, length) val chunkSize = math.min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH, length).toInt val out = new ChunkedByteBufferOutputStream(chunkSize, ByteBuffer.allocate _) Utils.tryWithSafeFinally { IOUtils.copy(in, out) } { in.close() out.close() } out.toChunkedByteBuffer } } /** * Reads data from a ChunkedByteBuffer. * * @param dispose if true, `ChunkedByteBuffer.dispose()` will be called at the end of the stream * in order to close any memory-mapped files which back the buffer. */ private[spark] class ChunkedByteBufferInputStream( var chunkedByteBuffer: ChunkedByteBuffer, dispose: Boolean) extends InputStream { // Filter out empty chunks since `read()` assumes all chunks are non-empty. private[this] var chunks = chunkedByteBuffer.getChunks().filter(_.hasRemaining).iterator private[this] var currentChunk: ByteBuffer = { if (chunks.hasNext) { chunks.next() } else { null } } override def read(): Int = { if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) { currentChunk = chunks.next() } if (currentChunk != null && currentChunk.hasRemaining) { UnsignedBytes.toInt(currentChunk.get()) } else { close() -1 } } override def read(dest: Array[Byte], offset: Int, length: Int): Int = { if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) { currentChunk = chunks.next() } if (currentChunk != null && currentChunk.hasRemaining) { val amountToGet = math.min(currentChunk.remaining(), length) currentChunk.get(dest, offset, amountToGet) amountToGet } else { close() -1 } } override def skip(bytes: Long): Long = { if (currentChunk != null) { val amountToSkip = math.min(bytes, currentChunk.remaining).toInt currentChunk.position(currentChunk.position() + amountToSkip) if (currentChunk.remaining() == 0) { if (chunks.hasNext) { currentChunk = chunks.next() } else { close() } } amountToSkip } else { 0L } } override def close(): Unit = { if (chunkedByteBuffer != null && dispose) { chunkedByteBuffer.dispose() } chunkedByteBuffer = null chunks = null currentChunk = null } }",
          "## CLASS: org/apache/spark/storage/ByteBufferBlockData# (implementation)\n} private[spark] class ByteBufferBlockData( val buffer: ChunkedByteBuffer, val shouldDispose: Boolean) extends BlockData { override def toInputStream(): InputStream = buffer.toInputStream(dispose = false) override def toNetty(): Object = buffer.toNetty override def toChunkedByteBuffer(allocator: Int => ByteBuffer): ChunkedByteBuffer = { buffer.copy(allocator) } override def toByteBuffer(): ByteBuffer = buffer.toByteBuffer override def size: Long = buffer.size override def dispose(): Unit = { if (shouldDispose) { buffer.dispose() } } } private[spark] class HostLocalDirManager( futureExecutionContext: ExecutionContext, cacheSize: Int, blockStoreClient: BlockStoreClient) extends Logging { private val executorIdToLocalDirsCache = CacheBuilder .newBuilder() .maximumSize(cacheSize) .build[String, Array[String]]() private[spark] def getCachedHostLocalDirs: Map[String, Array[String]] = executorIdToLocalDirsCache.synchronized { executorIdToLocalDirsCache.asMap().asScala.toMap } private[spark] def getCachedHostLocalDirsFor(executorId: String): Option[Array[String]] = executorIdToLocalDirsCache.synchronized { Option(executorIdToLocalDirsCache.getIfPresent(executorId)) } private[spark] def getHostLocalDirs( host: String, port: Int, executorIds: Array[String])( callback: Try[Map[String, Array[String]]] => Unit): Unit = { val hostLocalDirsCompletable = new CompletableFuture[java.util.Map[String, Array[String]]] blockStoreClient.getHostLocalDirs( host, port, executorIds, hostLocalDirsCompletable) hostLocalDirsCompletable.whenComplete { (hostLocalDirs, throwable) => if (hostLocalDirs != null) { callback(Success(hostLocalDirs.asScala.toMap)) executorIdToLocalDirsCache.synchronized { executorIdToLocalDirsCache.putAll(hostLocalDirs) } } else { callback(Failure(throwable)) } } } } /** * Manager running on every node (driver and executors) which provides interfaces for putting and * retrieving blocks both locally and remotely into various stores (memory, disk, and off-heap). * * Note that [[initialize()]] must be called before the BlockManager is usable. */ private[spark] class BlockManager( val executorId: String, rpcEnv: RpcEnv, val master: BlockManagerMaster, val serializerManager: SerializerManager, val conf: SparkConf, memoryManager: MemoryManager, mapOutputTracker: MapOutputTracker, shuffleManager: ShuffleManager, val blockTransferService: BlockTransferService, securityManager: SecurityManager, externalBlockStoreClient: Option[ExternalBlockStoreClient]) extends BlockDataManager with BlockEvictionHandler with Logging { // same as `conf.get(config.SHUFFLE_SERVICE_ENABLED)` private[spark] val externalShuffleServiceEnabled: Boolean = externalBlockStoreClient.isDefined private val isDriver = executorId == SparkContext.DRIVER_IDENTIFIER private val remoteReadNioBufferConversion = conf.get(Network.NETWORK_REMOTE_READ_NIO_BUFFER_CONVERSION) private[spark] val subDirsPerLocalDir = conf.get(config.DISKSTORE_SUB_DIRECTORIES) val diskBlockManager = { // Only perform cleanup if an external service is not serving our shuffle files. val deleteFilesOnStop = !externalShuffleServiceEnabled || isDriver new DiskBlockManager(conf, deleteFilesOnStop = deleteFilesOnStop, isDriver = isDriver) } // Visible for testing private[storage] val blockInfoManager = new BlockInfoManager private val futureExecutionContext = ExecutionContext.fromExecutorService( ThreadUtils.newDaemonCachedThreadPool(\"block-manager-future\", 128)) // Actual storage of where blocks are kept private[spark] val memoryStore = new MemoryStore(conf, blockInfoManager, serializerManager, memoryManager, this) private[spark] val diskStore = new DiskStore(conf, diskBlockManager, securityManager) memoryManager.setMemoryStore(memoryStore) // Note: depending on the memory manager, `maxMemory` may actually vary over time. // However, since we use this only for reporting and logging, what we actually want here is // the absolute maximum value that `maxMemory` can ever possibly reach. We may need // to revisit whether reporting this value as the \"max\" is intuitive to the user. private val maxOnHeapMemory = memoryManager.maxOnHeapStorageMemory private val maxOffHeapMemory = memoryManager.maxOffHeapStorageMemory private[spark] val externalShuffleServicePort = StorageUtils.externalShuffleServicePort(conf) var blockManagerId: BlockManagerId = _ // Address of the server that serves this executor's shuffle files. This is either an external // service, or just our own Executor's BlockManager. private[spark] var shuffleServerId: BlockManagerId = _ // Client to read other executors' blocks. This is either an external service, or just the // standard BlockTransferService to directly connect to other Executors. private[spark] val blockStoreClient = externalBlockStoreClient.getOrElse(blockTransferService) // Max number of failures before this block manager refreshes the block locations from the driver private val maxFailuresBeforeLocationRefresh = conf.get(config.BLOCK_FAILURES_BEFORE_LOCATION_REFRESH) private val storageEndpoint = rpcEnv.setupEndpoint( \"BlockManagerEndpoint\" + BlockManager.ID_GENERATOR.next, new BlockManagerStorageEndpoint(rpcEnv, this, mapOutputTracker)) // Pending re-registration action being executed asynchronously or null if none is pending. // Accesses should synchronize on asyncReregisterLock. private var asyncReregisterTask: Future[Unit] = null private val asyncReregisterLock = new Object // Field related to peer block managers that are necessary for block replication @volatile private var cachedPeers: Seq[BlockManagerId] = _ private val peerFetchLock = new Object private var lastPeerFetchTimeNs = 0L private var blockReplicationPolicy: BlockReplicationPolicy = _ // visible for test // This is volatile since if it's defined we should not accept remote blocks. @volatile private[spark] var decommissioner: Option[BlockManagerDecommissioner] = None // A DownloadFileManager used to track all the files of remote blocks which are above the // specified memory threshold. Files will be deleted automatically based on weak reference. // Exposed for test private[storage] val remoteBlockTempFileManager = new BlockManager.RemoteBlockDownloadFileManager( this, securityManager.getIOEncryptionKey()) private val maxRemoteBlockToMem = conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM) var hostLocalDirManager: Option[HostLocalDirManager] = None @inline final private def isDecommissioning() = { decommissioner.isDefined } @inline final private def checkShouldStore(blockId: BlockId) = { // Don't reject broadcast blocks since they may be stored during task exec and // don't need to be migrated. if (isDecommissioning() && !blockId.isBroadcast) { throw SparkCoreErrors.cannotSaveBlockOnDecommissionedExecutorError(blockId) } } // This is a lazy val so someone can migrating RDDs even if they don't have a MigratableResolver // for shuffles. Used in BlockManagerDecommissioner & block puts. private[storage] lazy val migratableResolver: MigratableResolver = { shuffleManager.shuffleBlockResolver.asInstanceOf[MigratableResolver] } override def getLocalDiskDirs: Array[String] = diskBlockManager.localDirsString /** * Diagnose the possible cause of the shuffle data corruption by verifying the shuffle checksums * * @param blockId The blockId of the corrupted shuffle block * @param checksumByReader The checksum value of the corrupted block * @param algorithm The cheksum algorithm that is used when calculating the checksum value */ override def diagnoseShuffleBlockCorruption( blockId: BlockId, checksumByReader: Long, algorithm: String): Cause = { assert(blockId.isInstanceOf[ShuffleBlockId], s\"Corruption diagnosis only supports shuffle block yet, but got $blockId\") val shuffleBlock = blockId.asInstanceOf[ShuffleBlockId] val resolver = shuffleManager.shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver] val checksumFile = resolver.getChecksumFile(shuffleBlock.shuffleId, shuffleBlock.mapId, algorithm) val reduceId = shuffleBlock.reduceId ShuffleChecksumHelper.diagnoseCorruption( algorithm, checksumFile, reduceId, resolver.getBlockData(shuffleBlock), checksumByReader) } /** * Abstraction for storing blocks from bytes, whether they start in memory or on disk. * * @param blockSize the decrypted size of the block */ private[spark] abstract class BlockStoreUpdater[T]( blockSize: Long, blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], tellMaster: Boolean, keepReadLock: Boolean) { /** * Reads the block content into the memory. If the update of the block store is based on a * temporary file this could lead to loading the whole file into a ChunkedByteBuffer. */ protected def readToByteBuffer(): ChunkedByteBuffer protected def blockData(): BlockData protected def saveToDiskStore(): Unit private def saveDeserializedValuesToMemoryStore(inputStream: InputStream): Boolean = { try { val values = serializerManager.dataDeserializeStream(blockId, inputStream)(classTag) memoryStore.putIteratorAsValues(blockId, values, level.memoryMode, classTag) match { case Right(_) => true case Left(iter) => // If putting deserialized values in memory failed, we will put the bytes directly // to disk, so we don't need this iterator and can close it to free resources // earlier. iter.close() false } } catch { case ex: KryoException if ex.getCause.isInstanceOf[IOException] => // We need to have detailed log message to catch environmental problems easily. // Further details: https://issues.apache.org/jira/browse/SPARK-37710 processKryoException(ex, blockId) throw ex } finally { IOUtils.closeQuietly(inputStream) } } private def saveSerializedValuesToMemoryStore(bytes: ChunkedByteBuffer): Boolean = { val memoryMode = level.memoryMode memoryStore.putBytes(blockId, blockSize, memoryMode, () => { if (memoryMode == MemoryMode.OFF_HEAP && bytes.chunks.exists(!_.isDirect)) { bytes.copy(Platform.allocateDirectBuffer) } else { bytes } }) } /** * Put the given data according to the given level in one of the block stores, replicating * the values if necessary. * * If the block already exists, this method will not overwrite it. * * If keepReadLock is true, this method will hold the read lock when it returns (even if the * block already exists). If false, this method will hold no locks when it returns. * * @return true if the block was already present or if the put succeeded, false otherwise. */ def save(): Boolean = { doPut(blockId, level, classTag, tellMaster, keepReadLock) { info => val startTimeNs = System.nanoTime() // Since we're storing bytes, initiate the replication before storing them locally. // This is faster as data is already serialized and ready to send. val replicationFuture = if (level.replication > 1) { Future { // This is a blocking action and should run in futureExecutionContext which is a cached // thread pool. replicate(blockId, blockData(), level, classTag) }(futureExecutionContext) } else { null } if (level.useMemory) { // Put it in memory first, even if it also has useDisk set to true; // We will drop it to disk later if the memory store can't hold it. val putSucceeded = if (level.deserialized) { saveDeserializedValuesToMemoryStore(blockData().toInputStream()) } else { saveSerializedValuesToMemoryStore(readToByteBuffer()) } if (!putSucceeded && level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") saveToDiskStore() } } else if (level.useDisk) { saveToDiskStore() } val putBlockStatus = getCurrentBlockStatus(blockId, info) val blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid if (blockWasSuccessfullyStored) { // Now that the block is in either the memory or disk store, // tell the master about it. info.size = blockSize if (tellMaster && info.tellMaster) { reportBlockStatus(blockId, putBlockStatus) } addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus) } logDebug(s\"Put block ${blockId} locally took ${Utils.getUsedTimeNs(startTimeNs)}\") if (level.replication > 1) { // Wait for asynchronous replication to finish try { ThreadUtils.awaitReady(replicationFuture, Duration.Inf) } catch { case NonFatal(t) => throw SparkCoreErrors.waitingForReplicationToFinishError(t) } } if (blockWasSuccessfullyStored) { None } else { Some(blockSize) } }.isEmpty } } /** * Helper for storing a block from bytes already in memory. * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. */ private case class ByteBufferBlockStoreUpdater[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], bytes: ChunkedByteBuffer, tellMaster: Boolean = true, keepReadLock: Boolean = false) extends BlockStoreUpdater[T](bytes.size, blockId, level, classTag, tellMaster, keepReadLock) { override def readToByteBuffer(): ChunkedByteBuffer = bytes /** * The ByteBufferBlockData wrapper is not disposed of to avoid releasing buffers that are * owned by the caller. */ override def blockData(): BlockData = new ByteBufferBlockData(bytes, false) override def saveToDiskStore(): Unit = diskStore.putBytes(blockId, bytes) } /** * Helper for storing a block based from bytes already in a local temp file. */ private[spark] case class TempFileBasedBlockStoreUpdater[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], tmpFile: File, blockSize: Long, tellMaster: Boolean = true, keepReadLock: Boolean = false) extends BlockStoreUpdater[T](blockSize, blockId, level, classTag, tellMaster, keepReadLock) { override def readToByteBuffer(): ChunkedByteBuffer = { val allocator = level.memoryMode match { case MemoryMode.ON_HEAP => ByteBuffer.allocate _ case MemoryMode.OFF_HEAP => Platform.allocateDirectBuffer _ } blockData().toChunkedByteBuffer(allocator) } override def blockData(): BlockData = diskStore.getBytes(tmpFile, blockSize) override def saveToDiskStore(): Unit = diskStore.moveFileToBlock(tmpFile, blockSize, blockId) override def save(): Boolean = { val res = super.save() tmpFile.delete() res } } /** * Initializes the BlockManager with the given appId. This is not performed in the constructor as * the appId may not be known at BlockManager instantiation time (in particular for the driver, * where it is only learned after registration with the TaskScheduler). * * This method initializes the BlockTransferService and BlockStoreClient, registers with the * BlockManagerMaster, starts the BlockManagerWorker endpoint, and registers with a local shuffle * service if configured. */ def initialize(appId: String): Unit = { blockTransferService.init(this) externalBlockStoreClient.foreach { blockStoreClient => blockStoreClient.init(appId) } blockReplicationPolicy = { val priorityClass = conf.get(config.STORAGE_REPLICATION_POLICY) val clazz = Utils.classForName(priorityClass) val ret = clazz.getConstructor().newInstance().asInstanceOf[BlockReplicationPolicy] logInfo(s\"Using $priorityClass for block replication policy\") ret } val id = BlockManagerId(executorId, blockTransferService.hostName, blockTransferService.port, None) val idFromMaster = master.registerBlockManager( id, diskBlockManager.localDirsString, maxOnHeapMemory, maxOffHeapMemory, storageEndpoint) blockManagerId = if (idFromMaster != null) idFromMaster else id shuffleServerId = if (externalShuffleServiceEnabled) { logInfo(s\"external shuffle service port = $externalShuffleServicePort\") BlockManagerId(executorId, blockTransferService.hostName, externalShuffleServicePort) } else { blockManagerId } // Register Executors' configuration with the local shuffle service, if one should exist. if (externalShuffleServiceEnabled && !blockManagerId.isDriver) { registerWithExternalShuffleServer() } hostLocalDirManager = { if ((conf.get(config.SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED) && !conf.get(config.SHUFFLE_USE_OLD_FETCH_PROTOCOL)) || Utils.isPushBasedShuffleEnabled(conf, isDriver)) { Some(new HostLocalDirManager( futureExecutionContext, conf.get(config.STORAGE_LOCAL_DISK_BY_EXECUTORS_CACHE_SIZE), blockStoreClient)) } else { None } } logInfo(s\"Initialized BlockManager: $blockManagerId\") } def shuffleMetricsSource: Source = { import BlockManager._ if (externalShuffleServiceEnabled) { new ShuffleMetricsSource(\"ExternalShuffle\", blockStoreClient.shuffleMetrics()) } else { new ShuffleMetricsSource(\"NettyBlockTransfer\", blockStoreClient.shuffleMetrics()) } } private def registerWithExternalShuffleServer(): Unit = { logInfo(\"Registering executor with local external shuffle service.\") val shuffleManagerMeta = if (Utils.isPushBasedShuffleEnabled(conf, isDriver = isDriver, checkSerializer = false)) { s\"${shuffleManager.getClass.getName}:\" + s\"${diskBlockManager.getMergeDirectoryAndAttemptIDJsonString()}}}\" } else { shuffleManager.getClass.getName } val shuffleConfig = new ExecutorShuffleInfo( diskBlockManager.localDirsString, diskBlockManager.subDirsPerLocalDir, shuffleManagerMeta) val MAX_ATTEMPTS = conf.get(config.SHUFFLE_REGISTRATION_MAX_ATTEMPTS) val SLEEP_TIME_SECS = 5 for (i <- 1 to MAX_ATTEMPTS) { try { // Synchronous and will throw an exception if we cannot connect. blockStoreClient.asInstanceOf[ExternalBlockStoreClient].registerWithShuffleServer( shuffleServerId.host, shuffleServerId.port, shuffleServerId.executorId, shuffleConfig) return } catch { case e: Exception if i < MAX_ATTEMPTS => logError(s\"Failed to connect to external shuffle server, will retry ${MAX_ATTEMPTS - i}\" + s\" more times after waiting $SLEEP_TIME_SECS seconds...\", e) Thread.sleep(SLEEP_TIME_SECS * 1000L) case NonFatal(e) => throw SparkCoreErrors.unableToRegisterWithExternalShuffleServerError(e) } } } /** * Report all blocks to the BlockManager again. This may be necessary if we are dropped * by the BlockManager and come back or if we become capable of recovering blocks on disk after * an executor crash. * * This function deliberately fails silently if the master returns false (indicating that * the storage endpoint needs to re-register). The error condition will be detected again by the * next heart beat attempt or new block registration and another try to re-register all blocks * will be made then. */ private def reportAllBlocks(): Unit = { logInfo(s\"Reporting ${blockInfoManager.size} blocks to the master.\") for ((blockId, info) <- blockInfoManager.entries) { val status = getCurrentBlockStatus(blockId, info) if (info.tellMaster && !tryToReportBlockStatus(blockId, status)) { logError(s\"Failed to report $blockId to master; giving up.\") return } } } /** * Re-register with the master and report all blocks to it. This will be called by the heart beat * thread if our heartbeat to the block manager indicates that we were not registered. * * Note that this method must be called without any BlockInfo locks held. */ def reregister(): Unit = { // TODO: We might need to rate limit re-registering. logInfo(s\"BlockManager $blockManagerId re-registering with master\") master.registerBlockManager(blockManagerId, diskBlockManager.localDirsString, maxOnHeapMemory, maxOffHeapMemory, storageEndpoint) reportAllBlocks() } /** * Re-register with the master sometime soon. */ private def asyncReregister(): Unit = { asyncReregisterLock.synchronized { if (asyncReregisterTask == null) { asyncReregisterTask = Future[Unit] { // This is a blocking action and should run in futureExecutionContext which is a cached // thread pool reregister() asyncReregisterLock.synchronized { asyncReregisterTask = null } }(futureExecutionContext) } } } /** * For testing. Wait for any pending asynchronous re-registration; otherwise, do nothing. */ def waitForAsyncReregister(): Unit = { val task = asyncReregisterTask if (task != null) { try { ThreadUtils.awaitReady(task, Duration.Inf) } catch { case NonFatal(t) => throw SparkCoreErrors.waitingForAsyncReregistrationError(t) } } } override def getHostLocalShuffleData( blockId: BlockId, dirs: Array[String]): ManagedBuffer = { shuffleManager.shuffleBlockResolver.getBlockData(blockId, Some(dirs)) } /** * Interface to get local block data. Throws an exception if the block cannot be found or * cannot be read successfully. */ override def getLocalBlockData(blockId: BlockId): ManagedBuffer = { if (blockId.isShuffle) { logDebug(s\"Getting local shuffle block ${blockId}\") try { shuffleManager.shuffleBlockResolver.getBlockData(blockId) } catch { case e: IOException => if (conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) { FallbackStorage.read(conf, blockId) } else { throw e } } } else { getLocalBytes(blockId) match { case Some(blockData) => new BlockManagerManagedBuffer(blockInfoManager, blockId, blockData, true) case None => // If this block manager receives a request for a block that it doesn't have then it's // likely that the master has outdated block statuses for this block. Therefore, we send // an RPC so that this block is marked as being unavailable from this block manager. reportBlockStatus(blockId, BlockStatus.empty) throw SparkCoreErrors.blockNotFoundError(blockId) } } } /** * Put the block locally, using the given storage level. * * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. */ override def putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean = { putBytes(blockId, new ChunkedByteBuffer(data.nioByteBuffer()), level)(classTag) } override def putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID = { checkShouldStore(blockId) if (blockId.isShuffle) { logDebug(s\"Putting shuffle block ${blockId}\") try { return migratableResolver.putShuffleBlockAsStream(blockId, serializerManager) } catch { case e: ClassCastException => throw SparkCoreErrors.unexpectedShuffleBlockWithUnsupportedResolverError(shuffleManager, blockId) } } logDebug(s\"Putting regular block ${blockId}\") // All other blocks val (_, tmpFile) = diskBlockManager.createTempLocalBlock() val channel = new CountingWritableChannel( Channels.newChannel(serializerManager.wrapForEncryption(new FileOutputStream(tmpFile)))) logTrace(s\"Streaming block $blockId to tmp file $tmpFile\") new StreamCallbackWithID { override def getID: String = blockId.name override def onData(streamId: String, buf: ByteBuffer): Unit = { while (buf.hasRemaining) { channel.write(buf) } } override def onComplete(streamId: String): Unit = { logTrace(s\"Done receiving block $blockId, now putting into local blockManager\") // Note this is all happening inside the netty thread as soon as it reads the end of the // stream. channel.close() val blockSize = channel.getCount val blockStored = TempFileBasedBlockStoreUpdater( blockId, level, classTag, tmpFile, blockSize).save() if (!blockStored) { throw SparkCoreErrors.failToStoreBlockOnBlockManagerError(blockManagerId, blockId) } } override def onFailure(streamId: String, cause: Throwable): Unit = { // the framework handles the connection itself, we just need to do local cleanup channel.close() tmpFile.delete() } } } /** * Get the local merged shuffle block data for the given block ID as multiple chunks. * A merged shuffle file is divided into multiple chunks according to the index file. * Instead of reading the entire file as a single block, we split it into smaller chunks * which will be memory efficient when performing certain operations. */ def getLocalMergedBlockData( blockId: ShuffleMergedBlockId, dirs: Array[String]): Seq[ManagedBuffer] = { shuffleManager.shuffleBlockResolver.getMergedBlockData(blockId, Some(dirs)) } /** * Get the local merged shuffle block meta data for the given block ID. */ def getLocalMergedBlockMeta( blockId: ShuffleMergedBlockId, dirs: Array[String]): MergedBlockMeta = { shuffleManager.shuffleBlockResolver.getMergedBlockMeta(blockId, Some(dirs)) } /** * Get the BlockStatus for the block identified by the given ID, if it exists. * NOTE: This is mainly for testing. */ def getStatus(blockId: BlockId): Option[BlockStatus] = { blockInfoManager.get(blockId).map { info => val memSize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L val diskSize = if (diskStore.contains(blockId)) diskStore.getSize(blockId) else 0L BlockStatus(info.level, memSize = memSize, diskSize = diskSize) } } /** * Get the ids of existing blocks that match the given filter. Note that this will * query the blocks stored in the disk block manager (that the block manager * may not know of). */ def getMatchingBlockIds(filter: BlockId => Boolean): Seq[BlockId] = { // The `toArray` is necessary here in order to force the list to be materialized so that we // don't try to serialize a lazy iterator when responding to client requests. (blockInfoManager.entries.map(_._1) ++ diskBlockManager.getAllBlocks()) .filter(filter) .toArray .toSeq } /** * Tell the master about the current storage status of a block. This will send a block update * message reflecting the current status, *not* the desired storage level in its block info. * For example, a block with MEMORY_AND_DISK set might have fallen out to be only on disk. * * droppedMemorySize exists to account for when the block is dropped from memory to disk (so * it is still valid). This ensures that update in master will compensate for the increase in * memory on the storage endpoint. */ private[spark] def reportBlockStatus( blockId: BlockId, status: BlockStatus, droppedMemorySize: Long = 0L): Unit = { val needReregister = !tryToReportBlockStatus(blockId, status, droppedMemorySize) if (needReregister) { logInfo(s\"Got told to re-register updating block $blockId\") // Re-registering will report our new block for free. asyncReregister() } logDebug(s\"Told master about block $blockId\") } /** * Actually send a UpdateBlockInfo message. Returns the master's response, * which will be true if the block was successfully recorded and false if * the storage endpoint needs to re-register. */ private def tryToReportBlockStatus( blockId: BlockId, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean = { val storageLevel = status.storageLevel val inMemSize = Math.max(status.memSize, droppedMemorySize) val onDiskSize = status.diskSize master.updateBlockInfo(blockManagerId, blockId, storageLevel, inMemSize, onDiskSize) } /** * Return the updated storage status of the block with the given ID. More specifically, if * the block is dropped from memory and possibly added to disk, return the new storage level * and the updated in-memory and on-disk sizes. */ private def getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus = { info.synchronized { info.level match { case null => BlockStatus.empty case level => val inMem = level.useMemory && memoryStore.contains(blockId) val onDisk = level.useDisk && diskStore.contains(blockId) val deserialized = if (inMem) level.deserialized else false val replication = if (inMem || onDisk) level.replication else 1 val storageLevel = StorageLevel( useDisk = onDisk, useMemory = inMem, useOffHeap = level.useOffHeap, deserialized = deserialized, replication = replication) val memSize = if (inMem) memoryStore.getSize(blockId) else 0L val diskSize = if (onDisk) diskStore.getSize(blockId) else 0L BlockStatus(storageLevel, memSize, diskSize) } } } /** * Get locations of an array of blocks. */ private def getLocationBlockIds(blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] = { val startTimeNs = System.nanoTime() val locations = master.getLocations(blockIds).toArray logDebug(s\"Got multiple block location in ${Utils.getUsedTimeNs(startTimeNs)}\") locations } /** * Cleanup code run in response to a failed local read. * Must be called while holding a read lock on the block. */ private def handleLocalReadFailure(blockId: BlockId): Nothing = { releaseLock(blockId) // Remove the missing block so that its unavailability is reported to the driver removeBlock(blockId) throw SparkCoreErrors.readLockedBlockNotFoundError(blockId) } /** * Get block from local block manager as an iterator of Java objects. */ def getLocalValues(blockId: BlockId): Option[BlockResult] = { logDebug(s\"Getting local block $blockId\") blockInfoManager.lockForReading(blockId) match { case None => logDebug(s\"Block $blockId was not found\") None case Some(info) => val level = info.level logDebug(s\"Level for block $blockId is $level\") val taskContext = Option(TaskContext.get()) if (level.useMemory && memoryStore.contains(blockId)) { val iter: Iterator[Any] = if (level.deserialized) { memoryStore.getValues(blockId).get } else { serializerManager.dataDeserializeStream( blockId, memoryStore.getBytes(blockId).get.toInputStream())(info.classTag) } // We need to capture the current taskId in case the iterator completion is triggered // from a different thread which does not have TaskContext set; see SPARK-18406 for // discussion. val ci = CompletionIterator[Any, Iterator[Any]](iter, { releaseLock(blockId, taskContext) }) Some(new BlockResult(ci, DataReadMethod.Memory, info.size)) } else if (level.useDisk && diskStore.contains(blockId)) { try { val diskData = diskStore.getBytes(blockId) val iterToReturn: Iterator[Any] = { if (level.deserialized) { val diskValues = serializerManager.dataDeserializeStream( blockId, diskData.toInputStream())(info.classTag) maybeCacheDiskValuesInMemory(info, blockId, level, diskValues) } else { val stream = maybeCacheDiskBytesInMemory(info, blockId, level, diskData) .map { _.toInputStream(dispose = false) } .getOrElse { diskData.toInputStream() } serializerManager.dataDeserializeStream(blockId, stream)(info.classTag) } } val ci = CompletionIterator[Any, Iterator[Any]](iterToReturn, { releaseLockAndDispose(blockId, diskData, taskContext) }) Some(new BlockResult(ci, DataReadMethod.Disk, info.size)) } catch { case ex: KryoException if ex.getCause.isInstanceOf[IOException] => // We need to have detailed log message to catch environmental problems easily. // Further details: https://issues.apache.org/jira/browse/SPARK-37710 processKryoException(ex, blockId) throw ex } } else { handleLocalReadFailure(blockId) } } } private def processKryoException(ex: KryoException, blockId: BlockId): Unit = { var message = \"%s. %s - blockId: %s\".format(ex.getMessage, blockManagerId.toString, blockId) val file = diskBlockManager.getFile(blockId) if (file.exists()) { message = \"%s - blockDiskPath: %s\".format(message, file.getAbsolutePath) } logInfo(message) } /** * Get block from the local block manager as serialized bytes. */ def getLocalBytes(blockId: BlockId): Option[BlockData] = { logDebug(s\"Getting local block $blockId as bytes\") assert(!blockId.isShuffle, s\"Unexpected ShuffleBlockId $blockId\") blockInfoManager.lockForReading(blockId).map { info => doGetLocalBytes(blockId, info) } } /** * Get block from the local block manager as serialized bytes. * * Must be called while holding a read lock on the block. * Releases the read lock upon exception; keeps the read lock upon successful return. */ private def doGetLocalBytes(blockId: BlockId, info: BlockInfo): BlockData = { val level = info.level logDebug(s\"Level for block $blockId is $level\") // In order, try to read the serialized bytes from memory, then from disk, then fall back to // serializing in-memory objects, and, finally, throw an exception if the block does not exist. if (level.deserialized) { // Try to avoid expensive serialization by reading a pre-serialized copy from disk: if (level.useDisk && diskStore.contains(blockId)) { // Note: we purposely do not try to put the block back into memory here. Since this branch // handles deserialized blocks, this block may only be cached in memory as objects, not // serialized bytes. Because the caller only requested bytes, it doesn't make sense to // cache the block's deserialized objects since that caching may not have a payoff. diskStore.getBytes(blockId) } else if (level.useMemory && memoryStore.contains(blockId)) { // The block was not found on disk, so serialize an in-memory copy: new ByteBufferBlockData(serializerManager.dataSerializeWithExplicitClassTag( blockId, memoryStore.getValues(blockId).get, info.classTag), true) } else { handleLocalReadFailure(blockId) } } else { // storage level is serialized if (level.useMemory && memoryStore.contains(blockId)) { new ByteBufferBlockData(memoryStore.getBytes(blockId).get, false) } else if (level.useDisk && diskStore.contains(blockId)) { val diskData = diskStore.getBytes(blockId) maybeCacheDiskBytesInMemory(info, blockId, level, diskData) .map(new ByteBufferBlockData(_, false)) .getOrElse(diskData) } else { handleLocalReadFailure(blockId) } } } /** * Get block from remote block managers. * * This does not acquire a lock on this block in this JVM. */ private[spark] def getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult] = { val ct = implicitly[ClassTag[T]] getRemoteBlock(blockId, (data: ManagedBuffer) => { val values = serializerManager.dataDeserializeStream(blockId, data.createInputStream())(ct) new BlockResult(values, DataReadMethod.Network, data.size) }) } /** * Get the remote block and transform it to the provided data type. * * If the block is persisted to the disk and stored at an executor running on the same host then * first it is tried to be accessed using the local directories of the other executor directly. * If the file is successfully identified then tried to be transformed by the provided * transformation function which expected to open the file. If there is any exception during this * transformation then block access falls back to fetching it from the remote executor via the * network. * * @param blockId identifies the block to get * @param bufferTransformer this transformer expected to open the file if the block is backed by a * file by this it is guaranteed the whole content can be loaded * @tparam T result type */ private[spark] def getRemoteBlock[T]( blockId: BlockId, bufferTransformer: ManagedBuffer => T): Option[T] = { logDebug(s\"Getting remote block $blockId\") require(blockId != null, \"BlockId is null\") // Because all the remote blocks are registered in driver, it is not necessary to ask // all the storage endpoints to get block status. val locationsAndStatusOption = master.getLocationsAndStatus(blockId, blockManagerId.host) if (locationsAndStatusOption.isEmpty) { logDebug(s\"Block $blockId is unknown by block manager master\") None } else { val locationsAndStatus = locationsAndStatusOption.get val blockSize = locationsAndStatus.status.diskSize.max(locationsAndStatus.status.memSize) locationsAndStatus.localDirs.flatMap { localDirs => val blockDataOption = readDiskBlockFromSameHostExecutor(blockId, localDirs, locationsAndStatus.status.diskSize) val res = blockDataOption.flatMap { blockData => try { Some(bufferTransformer(blockData)) } catch { case NonFatal(e) => logDebug(\"Block from the same host executor cannot be opened: \", e) None } } logInfo(s\"Read $blockId from the disk of a same host executor is \" + (if (res.isDefined) \"successful.\" else \"failed.\")) res }.orElse { fetchRemoteManagedBuffer(blockId, blockSize, locationsAndStatus).map(bufferTransformer) } } } private def preferExecutors(locations: Seq[BlockManagerId]): Seq[BlockManagerId] = { val (executors, shuffleServers) = locations.partition(_.port != externalShuffleServicePort) executors ++ shuffleServers } /** * Return a list of locations for the given block, prioritizing the local machine since * multiple block managers can share the same host, followed by hosts on the same rack. * * Within each of the above listed groups (same host, same rack and others) executors are * preferred over the external shuffle service. */ private[spark] def sortLocations(locations: Seq[BlockManagerId]): Seq[BlockManagerId] = { val locs = Random.shuffle(locations) val (preferredLocs, otherLocs) = locs.partition(_.host == blockManagerId.host) val orderedParts = blockManagerId.topologyInfo match { case None => Seq(preferredLocs, otherLocs) case Some(_) => val (sameRackLocs, differentRackLocs) = otherLocs.partition { loc => blockManagerId.topologyInfo == loc.topologyInfo } Seq(preferredLocs, sameRackLocs, differentRackLocs) } orderedParts.map(preferExecutors).reduce(_ ++ _) } /** * Fetch the block from remote block managers as a ManagedBuffer. */ private def fetchRemoteManagedBuffer( blockId: BlockId, blockSize: Long, locationsAndStatus: BlockManagerMessages.BlockLocationsAndStatus): Option[ManagedBuffer] = { // If the block size is above the threshold, we should pass our FileManger to // BlockTransferService, which will leverage it to spill the block; if not, then passed-in // null value means the block will be persisted in memory. val tempFileManager = if (blockSize > maxRemoteBlockToMem) { remoteBlockTempFileManager } else { null } var runningFailureCount = 0 var totalFailureCount = 0 val locations = sortLocations(locationsAndStatus.locations) val maxFetchFailures = locations.size var locationIterator = locations.iterator while (locationIterator.hasNext) { val loc = locationIterator.next() logDebug(s\"Getting remote block $blockId from $loc\") val data = try { val buf = blockTransferService.fetchBlockSync(loc.host, loc.port, loc.executorId, blockId.toString, tempFileManager) if (blockSize > 0 && buf.size() == 0) { throw new IllegalStateException(\"Empty buffer received for non empty block \" + s\"when fetching remote block $blockId from $loc\") } buf } catch { case NonFatal(e) => runningFailureCount += 1 totalFailureCount += 1 if (totalFailureCount >= maxFetchFailures) { // Give up trying anymore locations. Either we've tried all of the original locations, // or we've refreshed the list of locations from the master, and have still // hit failures after trying locations from the refreshed list. logWarning(s\"Failed to fetch remote block $blockId \" + s\"from [${locations.mkString(\", \")}] after $totalFailureCount fetch failures. \" + s\"Most recent failure cause:\", e) return None } logWarning(s\"Failed to fetch remote block $blockId \" + s\"from $loc (failed attempt $runningFailureCount)\", e) // If there is a large number of executors then locations list can contain a // large number of stale entries causing a large number of retries that may // take a significant amount of time. To get rid of these stale entries // we refresh the block locations after a certain number of fetch failures if (runningFailureCount >= maxFailuresBeforeLocationRefresh) { locationIterator = sortLocations(master.getLocations(blockId)).iterator logDebug(s\"Refreshed locations from the driver \" + s\"after ${runningFailureCount} fetch failures.\") runningFailureCount = 0 } // This location failed, so we retry fetch from a different one by returning null here null } if (data != null) { // If the ManagedBuffer is a BlockManagerManagedBuffer, the disposal of the // byte buffers backing it may need to be handled after reading the bytes. // In this case, since we just fetched the bytes remotely, we do not have // a BlockManagerManagedBuffer. The assert here is to ensure that this holds // true (or the disposal is handled). assert(!data.isInstanceOf[BlockManagerManagedBuffer]) return Some(data) } logDebug(s\"The value of block $blockId is null\") } logDebug(s\"Block $blockId not found\") None } /** * Reads the block from the local directories of another executor which runs on the same host. */ private[spark] def readDiskBlockFromSameHostExecutor( blockId: BlockId, localDirs: Array[String], blockSize: Long): Option[ManagedBuffer] = { val file = new File(ExecutorDiskUtils.getFilePath(localDirs, subDirsPerLocalDir, blockId.name)) if (file.exists()) { val managedBuffer = securityManager.getIOEncryptionKey() match { case Some(key) => // Encrypted blocks cannot be memory mapped; return a special object that does decryption // and provides InputStream / FileRegion implementations for reading the data. new EncryptedManagedBuffer( new EncryptedBlockData(file, blockSize, conf, key)) case _ => val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\") new FileSegmentManagedBuffer(transportConf, file, 0, file.length) } Some(managedBuffer) } else { None } } /** * Get block from remote block managers as serialized bytes. */ def getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] = { getRemoteBlock(blockId, (data: ManagedBuffer) => { // SPARK-24307 undocumented \"escape-hatch\" in case there are any issues in converting to // ChunkedByteBuffer, to go back to old code-path. Can be removed post Spark 2.4 if // new path is stable. if (remoteReadNioBufferConversion) { new ChunkedByteBuffer(data.nioByteBuffer()) } else { ChunkedByteBuffer.fromManagedBuffer(data) } }) } /** * Get a block from the block manager (either local or remote). * * This acquires a read lock on the block if the block was stored locally and does not acquire * any locks if the block was fetched from a remote block manager. The read lock will * automatically be freed once the result's `data` iterator is fully consumed. */ def get[T: ClassTag](blockId: BlockId): Option[BlockResult] = { val local = getLocalValues(blockId) if (local.isDefined) { logInfo(s\"Found block $blockId locally\") return local } val remote = getRemoteValues[T](blockId) if (remote.isDefined) { logInfo(s\"Found block $blockId remotely\") return remote } None } /** * Downgrades an exclusive write lock to a shared read lock. */ def downgradeLock(blockId: BlockId): Unit = { blockInfoManager.downgradeLock(blockId) } /** * Release a lock on the given block with explicit TaskContext. * The param `taskContext` should be passed in case we can't get the correct TaskContext, * for example, the input iterator of a cached RDD iterates to the end in a child * thread. */ def releaseLock(blockId: BlockId, taskContext: Option[TaskContext] = None): Unit = { val taskAttemptId = taskContext.map(_.taskAttemptId()) // SPARK-27666. When a task completes, Spark automatically releases all the blocks locked // by this task. We should not release any locks for a task that is already completed. if (taskContext.isDefined && taskContext.get.isCompleted) { logWarning(s\"Task ${taskAttemptId.get} already completed, not releasing lock for $blockId\") } else { blockInfoManager.unlock(blockId, taskAttemptId) } } /** * Registers a task with the BlockManager in order to initialize per-task bookkeeping structures. */ def registerTask(taskAttemptId: Long): Unit = { blockInfoManager.registerTask(taskAttemptId) } /** * Release all locks for the given task. * * @return the blocks whose locks were released. */ def releaseAllLocksForTask(taskAttemptId: Long): Seq[BlockId] = { blockInfoManager.releaseAllLocksForTask(taskAttemptId) } /** * Retrieve the given block if it exists, otherwise call the provided `makeIterator` method * to compute the block, persist it, and return its values. * * @return either a BlockResult if the block was successfully cached, or an iterator if the block * could not be cached. */ def getOrElseUpdate[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], makeIterator: () => Iterator[T]): Either[BlockResult, Iterator[T]] = { // Attempt to read the block from local or remote storage. If it's present, then we don't need // to go through the local-get-or-put path. get[T](blockId)(classTag) match { case Some(block) => return Left(block) case _ => // Need to compute the block. } // Initially we hold no locks on this block. doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = true) match { case None => // doPut() didn't hand work back to us, so the block already existed or was successfully // stored. Therefore, we now hold a read lock on the block. val blockResult = getLocalValues(blockId).getOrElse { // Since we held a read lock between the doPut() and get() calls, the block should not // have been evicted, so get() not returning the block indicates some internal error. releaseLock(blockId) throw SparkCoreErrors.failToGetBlockWithLockError(blockId) } // We already hold a read lock on the block from the doPut() call and getLocalValues() // acquires the lock again, so we need to call releaseLock() here so that the net number // of lock acquisitions is 1 (since the caller will only call release() once). releaseLock(blockId) Left(blockResult) case Some(iter) => // The put failed, likely because the data was too large to fit in memory and could not be // dropped to disk. Therefore, we need to pass the input iterator back to the caller so // that they can decide what to do with the values (e.g. process them without caching). Right(iter) } } /** * @return true if the block was stored or false if an error occurred. */ def putIterator[T: ClassTag]( blockId: BlockId, values: Iterator[T], level: StorageLevel, tellMaster: Boolean = true): Boolean = { require(values != null, \"Values is null\") doPutIterator(blockId, () => values, level, implicitly[ClassTag[T]], tellMaster) match { case None => true case Some(iter) => // Caller doesn't care about the iterator values, so we can close the iterator here // to free resources earlier iter.close() false } } /** * A short circuited method to get a block writer that can write data directly to disk. * The Block will be appended to the File specified by filename. Callers should handle error * cases. */ def getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetricsReporter): DiskBlockObjectWriter = { val syncWrites = conf.get(config.SHUFFLE_SYNC) new DiskBlockObjectWriter(file, serializerManager, serializerInstance, bufferSize, syncWrites, writeMetrics, blockId) } /** * Put a new block of serialized bytes to the block manager. * * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. * * @return true if the block was stored or false if an error occurred. */ def putBytes[T: ClassTag]( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean = { require(bytes != null, \"Bytes is null\") val blockStoreUpdater = ByteBufferBlockStoreUpdater(blockId, level, implicitly[ClassTag[T]], bytes, tellMaster) blockStoreUpdater.save() } /** * Helper method used to abstract common code from [[BlockStoreUpdater.save()]] * and [[doPutIterator()]]. * * @param putBody a function which attempts the actual put() and returns None on success * or Some on failure. */ private def doPut[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_], tellMaster: Boolean, keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T] = { require(blockId != null, \"BlockId is null\") require(level != null && level.isValid, \"StorageLevel is null or invalid\") checkShouldStore(blockId) val putBlockInfo = { val newInfo = new BlockInfo(level, classTag, tellMaster) if (blockInfoManager.lockNewBlockForWriting(blockId, newInfo)) { newInfo } else { logWarning(s\"Block $blockId already exists on this machine; not re-adding it\") if (!keepReadLock) { // lockNewBlockForWriting returned a read lock on the existing block, so we must free it: releaseLock(blockId) } return None } } val startTimeNs = System.nanoTime() var exceptionWasThrown: Boolean = true val result: Option[T] = try { val res = putBody(putBlockInfo) exceptionWasThrown = false if (res.isEmpty) { // the block was successfully stored if (keepReadLock) { blockInfoManager.downgradeLock(blockId) } else { blockInfoManager.unlock(blockId) } } else { removeBlockInternal(blockId, tellMaster = false) logWarning(s\"Putting block $blockId failed\") } res } catch { // Since removeBlockInternal may throw exception, // we should print exception first to show root cause. case NonFatal(e) => logWarning(s\"Putting block $blockId failed due to exception $e.\") throw e } finally { // This cleanup is performed in a finally block rather than a `catch` to avoid having to // catch and properly re-throw InterruptedException. if (exceptionWasThrown) { // If an exception was thrown then it's possible that the code in `putBody` has already // notified the master about the availability of this block, so we need to send an update // to remove this block location. removeBlockInternal(blockId, tellMaster = tellMaster) // The `putBody` code may have also added a new block status to TaskMetrics, so we need // to cancel that out by overwriting it with an empty block status. We only do this if // the finally block was entered via an exception because doing this unconditionally would // cause us to send empty block statuses for every block that failed to be cached due to // a memory shortage (which is an expected failure, unlike an uncaught exception). addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty) } } val usedTimeMs = Utils.getUsedTimeNs(startTimeNs) if (level.replication > 1) { logDebug(s\"Putting block ${blockId} with replication took $usedTimeMs\") } else { logDebug(s\"Putting block ${blockId} without replication took ${usedTimeMs}\") } result } /** * Put the given block according to the given level in one of the block stores, replicating * the values if necessary. * * If the block already exists, this method will not overwrite it. * * @param keepReadLock if true, this method will hold the read lock when it returns (even if the * block already exists). If false, this method will hold no locks when it * returns. * @return None if the block was already present or if the put succeeded, or Some(iterator) * if the put failed. */ private def doPutIterator[T]( blockId: BlockId, iterator: () => Iterator[T], level: StorageLevel, classTag: ClassTag[T], tellMaster: Boolean = true, keepReadLock: Boolean = false): Option[PartiallyUnrolledIterator[T]] = { doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) { info => val startTimeNs = System.nanoTime() var iteratorFromFailedMemoryStorePut: Option[PartiallyUnrolledIterator[T]] = None // Size of the block in bytes var size = 0L if (level.useMemory) { // Put it in memory first, even if it also has useDisk set to true; // We will drop it to disk later if the memory store can't hold it. if (level.deserialized) { memoryStore.putIteratorAsValues(blockId, iterator(), level.memoryMode, classTag) match { case Right(s) => size = s case Left(iter) => // Not enough space to unroll this block; drop to disk if applicable if (level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream(blockId, out, iter)(classTag) } size = diskStore.getSize(blockId) } else { iteratorFromFailedMemoryStorePut = Some(iter) } } } else { // !level.deserialized memoryStore.putIteratorAsBytes(blockId, iterator(), classTag, level.memoryMode) match { case Right(s) => size = s case Left(partiallySerializedValues) => // Not enough space to unroll this block; drop to disk if applicable if (level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) partiallySerializedValues.finishWritingToStream(out) } size = diskStore.getSize(blockId) } else { iteratorFromFailedMemoryStorePut = Some(partiallySerializedValues.valuesIterator) } } } } else if (level.useDisk) { diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream(blockId, out, iterator())(classTag) } size = diskStore.getSize(blockId) } val putBlockStatus = getCurrentBlockStatus(blockId, info) val blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid if (blockWasSuccessfullyStored) { // Now that the block is in either the memory or disk store, tell the master about it. info.size = size if (tellMaster && info.tellMaster) { reportBlockStatus(blockId, putBlockStatus) } addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus) logDebug(s\"Put block $blockId locally took ${Utils.getUsedTimeNs(startTimeNs)}\") if (level.replication > 1) { val remoteStartTimeNs = System.nanoTime() val bytesToReplicate = doGetLocalBytes(blockId, info) // [SPARK-16550] Erase the typed classTag when using default serialization, since // NettyBlockRpcServer crashes when deserializing repl-defined classes. // TODO(ekl) remove this once the classloader issue on the remote end is fixed. val remoteClassTag = if (!serializerManager.canUseKryo(classTag)) { scala.reflect.classTag[Any] } else { classTag } try { replicate(blockId, bytesToReplicate, level, remoteClassTag) } finally { bytesToReplicate.dispose() } logDebug(s\"Put block $blockId remotely took ${Utils.getUsedTimeNs(remoteStartTimeNs)}\") } } assert(blockWasSuccessfullyStored == iteratorFromFailedMemoryStorePut.isEmpty) iteratorFromFailedMemoryStorePut } } /** * Attempts to cache spilled bytes read from disk into the MemoryStore in order to speed up * subsequent reads. This method requires the caller to hold a read lock on the block. * * @return a copy of the bytes from the memory store if the put succeeded, otherwise None. * If this returns bytes from the memory store then the original disk store bytes will * automatically be disposed and the caller should not continue to use them. Otherwise, * if this returns None then the original disk store bytes will be unaffected. */ private def maybeCacheDiskBytesInMemory( blockInfo: BlockInfo, blockId: BlockId, level: StorageLevel, diskData: BlockData): Option[ChunkedByteBuffer] = { require(!level.deserialized) if (level.useMemory) { // Synchronize on blockInfo to guard against a race condition where two readers both try to // put values read from disk into the MemoryStore. blockInfo.synchronized { if (memoryStore.contains(blockId)) { diskData.dispose() Some(memoryStore.getBytes(blockId).get) } else { val allocator = level.memoryMode match { case MemoryMode.ON_HEAP => ByteBuffer.allocate _ case MemoryMode.OFF_HEAP => Platform.allocateDirectBuffer _ } val putSucceeded = memoryStore.putBytes(blockId, diskData.size, level.memoryMode, () => { // https://issues.apache.org/jira/browse/SPARK-6076 // If the file size is bigger than the free memory, OOM will happen. So if we // cannot put it into MemoryStore, copyForMemory should not be created. That's why // this action is put into a `() => ChunkedByteBuffer` and created lazily. diskData.toChunkedByteBuffer(allocator) }) if (putSucceeded) { diskData.dispose() Some(memoryStore.getBytes(blockId).get) } else { None } } } } else { None } } /** * Attempts to cache spilled values read from disk into the MemoryStore in order to speed up * subsequent reads. This method requires the caller to hold a read lock on the block. * * @return a copy of the iterator. The original iterator passed this method should no longer * be used after this method returns. */ private def maybeCacheDiskValuesInMemory[T]( blockInfo: BlockInfo, blockId: BlockId, level: StorageLevel, diskIterator: Iterator[T]): Iterator[T] = { require(level.deserialized) val classTag = blockInfo.classTag.asInstanceOf[ClassTag[T]] if (level.useMemory) { // Synchronize on blockInfo to guard against a race condition where two readers both try to // put values read from disk into the MemoryStore. blockInfo.synchronized { if (memoryStore.contains(blockId)) { // Note: if we had a means to discard the disk iterator, we would do that here. memoryStore.getValues(blockId).get } else { memoryStore.putIteratorAsValues(blockId, diskIterator, level.memoryMode, classTag) match { case Left(iter) => // The memory store put() failed, so it returned the iterator back to us: iter case Right(_) => // The put() succeeded, so we can read the values back: memoryStore.getValues(blockId).get } } }.asInstanceOf[Iterator[T]] } else { diskIterator } } /** * Get peer block managers in the system. */ private[storage] def getPeers(forceFetch: Boolean): Seq[BlockManagerId] = { peerFetchLock.synchronized { val cachedPeersTtl = conf.get(config.STORAGE_CACHED_PEERS_TTL) // milliseconds val diff = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - lastPeerFetchTimeNs) val timeout = diff > cachedPeersTtl if (cachedPeers == null || forceFetch || timeout) { cachedPeers = master.getPeers(blockManagerId).sortBy(_.hashCode) lastPeerFetchTimeNs = System.nanoTime() logDebug(\"Fetched peers from master: \" + cachedPeers.mkString(\"[\", \",\", \"]\")) } if (cachedPeers.isEmpty && conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) { Seq(FallbackStorage.FALLBACK_BLOCK_MANAGER_ID) } else { cachedPeers } } } /** * Replicates a block to peer block managers based on existingReplicas and maxReplicas * * @param blockId blockId being replicate * @param existingReplicas existing block managers that have a replica * @param maxReplicas maximum replicas needed * @param maxReplicationFailures number of replication failures to tolerate before * giving up. * @return whether block was successfully replicated or not */ def replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int, maxReplicationFailures: Option[Int] = None): Boolean = { logInfo(s\"Using $blockManagerId to pro-actively replicate $blockId\") blockInfoManager.lockForReading(blockId).forall { info => val data = doGetLocalBytes(blockId, info) val storageLevel = StorageLevel( useDisk = info.level.useDisk, useMemory = info.level.useMemory, useOffHeap = info.level.useOffHeap, deserialized = info.level.deserialized, replication = maxReplicas) // we know we are called as a result of an executor removal or because the current executor // is getting decommissioned. so we refresh peer cache before trying replication, we won't // try to replicate to a missing executor/another decommissioning executor getPeers(forceFetch = true) try { replicate( blockId, data, storageLevel, info.classTag, existingReplicas, maxReplicationFailures) } finally { logDebug(s\"Releasing lock for $blockId\") releaseLockAndDispose(blockId, data) } } } /** * Replicate block to another node. Note that this is a blocking call that returns after * the block has been replicated. */ private def replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty, maxReplicationFailures: Option[Int] = None): Boolean = { val maxReplicationFailureCount = maxReplicationFailures.getOrElse( conf.get(config.STORAGE_MAX_REPLICATION_FAILURE)) val tLevel = StorageLevel( useDisk = level.useDisk, useMemory = level.useMemory, useOffHeap = level.useOffHeap, deserialized = level.deserialized, replication = 1) val numPeersToReplicateTo = level.replication - 1 val startTime = System.nanoTime val peersReplicatedTo = mutable.HashSet.empty ++ existingReplicas val peersFailedToReplicateTo = mutable.HashSet.empty[BlockManagerId] var numFailures = 0 val initialPeers = getPeers(false).filterNot(existingReplicas.contains) var peersForReplication = blockReplicationPolicy.prioritize( blockManagerId, initialPeers, peersReplicatedTo, blockId, numPeersToReplicateTo) while(numFailures <= maxReplicationFailureCount && !peersForReplication.isEmpty && peersReplicatedTo.size < numPeersToReplicateTo) { val peer = peersForReplication.head try { val onePeerStartTime = System.nanoTime logTrace(s\"Trying to replicate $blockId of ${data.size} bytes to $peer\") // This thread keeps a lock on the block, so we do not want the netty thread to unlock // block when it finishes sending the message. val buffer = new BlockManagerManagedBuffer(blockInfoManager, blockId, data, false, unlockOnDeallocate = false) blockTransferService.uploadBlockSync( peer.host, peer.port, peer.executorId, blockId, buffer, tLevel, classTag) logTrace(s\"Replicated $blockId of ${data.size} bytes to $peer\" + s\" in ${(System.nanoTime - onePeerStartTime).toDouble / 1e6} ms\") peersForReplication = peersForReplication.tail peersReplicatedTo += peer } catch { // Rethrow interrupt exception case e: InterruptedException => throw e // Everything else we may retry case NonFatal(e) => logWarning(s\"Failed to replicate $blockId to $peer, failure #$numFailures\", e) peersFailedToReplicateTo += peer // we have a failed replication, so we get the list of peers again // we don't want peers we have already replicated to and the ones that // have failed previously val filteredPeers = getPeers(true).filter { p => !peersFailedToReplicateTo.contains(p) && !peersReplicatedTo.contains(p) } numFailures += 1 peersForReplication = blockReplicationPolicy.prioritize( blockManagerId, filteredPeers, peersReplicatedTo, blockId, numPeersToReplicateTo - peersReplicatedTo.size) } } logDebug(s\"Replicating $blockId of ${data.size} bytes to \" + s\"${peersReplicatedTo.size} peer(s) took ${(System.nanoTime - startTime) / 1e6} ms\") if (peersReplicatedTo.size < numPeersToReplicateTo) { logWarning(s\"Block $blockId replicated to only \" + s\"${peersReplicatedTo.size} peer(s) instead of $numPeersToReplicateTo peers\") return false } logDebug(s\"block $blockId replicated to ${peersReplicatedTo.mkString(\", \")}\") return true } /** * Read a block consisting of a single object. */ def getSingle[T: ClassTag](blockId: BlockId): Option[T] = { get[T](blockId).map(_.data.next().asInstanceOf[T]) } /** * Write a block consisting of a single object. * * @return true if the block was stored or false if the block was already stored or an * error occurred. */ def putSingle[T: ClassTag]( blockId: BlockId, value: T, level: StorageLevel, tellMaster: Boolean = true): Boolean = { putIterator(blockId, Iterator(value), level, tellMaster) } /** * Drop a block from memory, possibly putting it on disk if applicable. Called when the memory * store reaches its limit and needs to free up space. * * If `data` is not put on disk, it won't be created. * * The caller of this method must hold a write lock on the block before calling this method. * This method does not release the write lock. * * @return the block's new effective StorageLevel. */ private[storage] override def dropFromMemory[T: ClassTag]( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel = { logInfo(s\"Dropping block $blockId from memory\") val info = blockInfoManager.assertBlockIsLockedForWriting(blockId) var blockIsUpdated = false val level = info.level // Drop to disk, if storage level requires if (level.useDisk && !diskStore.contains(blockId)) { logInfo(s\"Writing block $blockId to disk\") data() match { case Left(elements) => diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream( blockId, out, elements.iterator)(info.classTag.asInstanceOf[ClassTag[T]]) } case Right(bytes) => diskStore.putBytes(blockId, bytes) } blockIsUpdated = true } // Actually drop from memory store val droppedMemorySize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L val blockIsRemoved = memoryStore.remove(blockId) if (blockIsRemoved) { blockIsUpdated = true } else { logWarning(s\"Block $blockId could not be dropped from memory as it does not exist\") } val status = getCurrentBlockStatus(blockId, info) if (info.tellMaster) { reportBlockStatus(blockId, status, droppedMemorySize) } if (blockIsUpdated) { addUpdatedBlockStatusToTaskMetrics(blockId, status) } status.storageLevel } /** * Remove all blocks belonging to the given RDD. * * @return The number of blocks removed. */ def removeRdd(rddId: Int): Int = { // TODO: Avoid a linear scan by creating another mapping of RDD.id to blocks. logInfo(s\"Removing RDD $rddId\") val blocksToRemove = blockInfoManager.entries.flatMap(_._1.asRDDId).filter(_.rddId == rddId) blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster = false) } blocksToRemove.size } def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager) private[spark] def decommissionSelf(): Unit = synchronized { decommissioner match { case None => logInfo(\"Starting block manager decommissioning process...\") decommissioner = Some(new BlockManagerDecommissioner(conf, this)) decommissioner.foreach(_.start()) case Some(_) => logDebug(\"Block manager already in decommissioning state\") } } /** * Returns the last migration time and a boolean denoting if all the blocks have been migrated. * If there are any tasks running since that time the boolean may be incorrect. */ private[spark] def lastMigrationInfo(): (Long, Boolean) = { decommissioner.map(_.lastMigrationInfo()).getOrElse((0, false)) } private[storage] def getMigratableRDDBlocks(): Seq[ReplicateBlock] = master.getReplicateInfoForRDDBlocks(blockManagerId) /** * Remove all blocks belonging to the given broadcast. */ def removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int = { logDebug(s\"Removing broadcast $broadcastId\") val blocksToRemove = blockInfoManager.entries.map(_._1).collect { case bid @ BroadcastBlockId(`broadcastId`, _) => bid } blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster) } blocksToRemove.size } /** * Remove a block from both memory and disk. */ def removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit = { logDebug(s\"Removing block $blockId\") blockInfoManager.lockForWriting(blockId) match { case None => // The block has already been removed; do nothing. logWarning(s\"Asked to remove block $blockId, which does not exist\") case Some(info) => removeBlockInternal(blockId, tellMaster = tellMaster && info.tellMaster) addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty) } } /** * Internal version of [[removeBlock()]] which assumes that the caller already holds a write * lock on the block. */ private def removeBlockInternal(blockId: BlockId, tellMaster: Boolean): Unit = { val blockStatus = if (tellMaster) { val blockInfo = blockInfoManager.assertBlockIsLockedForWriting(blockId) Some(getCurrentBlockStatus(blockId, blockInfo)) } else None // Removals are idempotent in disk store and memory store. At worst, we get a warning. val removedFromMemory = memoryStore.remove(blockId) val removedFromDisk = diskStore.remove(blockId) if (!removedFromMemory && !removedFromDisk) { logWarning(s\"Block $blockId could not be removed as it was not found on disk or in memory\") } blockInfoManager.removeBlock(blockId) if (tellMaster) { // Only update storage level from the captured block status before deleting, so that // memory size and disk size are being kept for calculating delta. reportBlockStatus(blockId, blockStatus.get.copy(storageLevel = StorageLevel.NONE)) } } private def addUpdatedBlockStatusToTaskMetrics(blockId: BlockId, status: BlockStatus): Unit = { if (conf.get(config.TASK_METRICS_TRACK_UPDATED_BLOCK_STATUSES)) { Option(TaskContext.get()).foreach { c => c.taskMetrics().incUpdatedBlockStatuses(blockId -> status) } } } def releaseLockAndDispose( blockId: BlockId, data: BlockData, taskContext: Option[TaskContext] = None): Unit = { releaseLock(blockId, taskContext) data.dispose() } def stop(): Unit = { decommissioner.foreach(_.stop()) blockTransferService.close() if (blockStoreClient ne blockTransferService) { // Closing should be idempotent, but maybe not for the NioBlockTransferService. blockStoreClient.close() } remoteBlockTempFileManager.stop() diskBlockManager.stop() rpcEnv.stop(storageEndpoint) blockInfoManager.clear() memoryStore.clear() futureExecutionContext.shutdownNow() logInfo(\"BlockManager stopped\") } } private[spark] object BlockManager { private val ID_GENERATOR = new IdGenerator def blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] = { // blockManagerMaster != null is used in tests assert(env != null || blockManagerMaster != null) val blockLocations: Seq[Seq[BlockManagerId]] = if (blockManagerMaster == null) { env.blockManager.getLocationBlockIds(blockIds) } else { blockManagerMaster.getLocations(blockIds) } val blockManagers = new HashMap[BlockId, Seq[String]] for (i <- 0 until blockIds.length) { blockManagers(blockIds(i)) = blockLocations(i).map { loc => ExecutorCacheTaskLocation(loc.host, loc.executorId).toString } } blockManagers.toMap } private class ShuffleMetricsSource( override val sourceName: String, metricSet: MetricSet) extends Source { override val metricRegistry = new MetricRegistry metricRegistry.registerAll(metricSet) } class RemoteBlockDownloadFileManager( blockManager: BlockManager, encryptionKey: Option[Array[Byte]]) extends DownloadFileManager with Logging { private class ReferenceWithCleanup( file: DownloadFile, referenceQueue: JReferenceQueue[DownloadFile] ) extends WeakReference[DownloadFile](file, referenceQueue) { // we cannot use `file.delete()` here otherwise it won't be garbage-collected val filePath = file.path() def cleanUp(): Unit = { logDebug(s\"Clean up file $filePath\") if (!new File(filePath).delete()) { logDebug(s\"Fail to delete file $filePath\") } } } private val referenceQueue = new JReferenceQueue[DownloadFile] private val referenceBuffer = Collections.newSetFromMap[ReferenceWithCleanup]( new ConcurrentHashMap) private val POLL_TIMEOUT = 1000 @volatile private var stopped = false private val cleaningThread = new Thread() { override def run(): Unit = { keepCleaning() } } cleaningThread.setDaemon(true) cleaningThread.setName(\"RemoteBlock-temp-file-clean-thread\") cleaningThread.start() override def createTempFile(transportConf: TransportConf): DownloadFile = { val file = blockManager.diskBlockManager.createTempLocalBlock()._2 encryptionKey match { case Some(key) => // encryption is enabled, so when we read the decrypted data off the network, we need to // encrypt it when writing to disk. Note that the data may have been encrypted when it // was cached on disk on the remote side, but it was already decrypted by now (see // EncryptedBlockData). new EncryptedDownloadFile(file, key) case None => new SimpleDownloadFile(file, transportConf) } } override def registerTempFileToClean(file: DownloadFile): Boolean = { referenceBuffer.add(new ReferenceWithCleanup(file, referenceQueue)) } def stop(): Unit = { stopped = true cleaningThread.interrupt() cleaningThread.join() } private def keepCleaning(): Unit = { while (!stopped) { try { Option(referenceQueue.remove(POLL_TIMEOUT)) .map(_.asInstanceOf[ReferenceWithCleanup]) .foreach { ref => referenceBuffer.remove(ref) ref.cleanUp() } } catch { case _: InterruptedException => // no-op case NonFatal(e) => logError(\"Error in cleaning thread\", e) } } } } /** * A DownloadFile that encrypts data when it is written, and decrypts when it's read. */ private class EncryptedDownloadFile( file: File, key: Array[Byte]) extends DownloadFile { private val env = SparkEnv.get override def delete(): Boolean = file.delete() override def openForWriting(): DownloadFileWritableChannel = { new EncryptedDownloadWritableChannel() } override def path(): String = file.getAbsolutePath private class EncryptedDownloadWritableChannel extends DownloadFileWritableChannel { private val countingOutput: CountingWritableChannel = new CountingWritableChannel( Channels.newChannel(env.serializerManager.wrapForEncryption(new FileOutputStream(file)))) override def closeAndRead(): ManagedBuffer = { countingOutput.close() val size = countingOutput.getCount new EncryptedManagedBuffer(new EncryptedBlockData(file, size, env.conf, key)) } override def write(src: ByteBuffer): Int = countingOutput.write(src) override def isOpen: Boolean = countingOutput.isOpen() override def close(): Unit = countingOutput.close() } } }",
          "## CLASS: org/apache/spark/util/io/ChunkedByteBufferOutputStream# (implementation)\n*/ private[spark] class ChunkedByteBufferOutputStream( chunkSize: Int, allocator: Int => ByteBuffer) extends OutputStream { private[this] var toChunkedByteBufferWasCalled = false private val chunks = new ArrayBuffer[ByteBuffer] /** Index of the last chunk. Starting with -1 when the chunks array is empty. */ private[this] var lastChunkIndex = -1 /** * Next position to write in the last chunk. * * If this equals chunkSize, it means for next write we need to allocate a new chunk. * This can also never be 0. */ private[this] var position = chunkSize private[this] var _size = 0L private[this] var closed: Boolean = false def size: Long = _size override def close(): Unit = { if (!closed) { super.close() closed = true } } override def write(b: Int): Unit = { require(!closed, \"cannot write to a closed ChunkedByteBufferOutputStream\") allocateNewChunkIfNeeded() chunks(lastChunkIndex).put(b.toByte) position += 1 _size += 1 } override def write(bytes: Array[Byte], off: Int, len: Int): Unit = { require(!closed, \"cannot write to a closed ChunkedByteBufferOutputStream\") var written = 0 while (written < len) { allocateNewChunkIfNeeded() val thisBatch = math.min(chunkSize - position, len - written) chunks(lastChunkIndex).put(bytes, written + off, thisBatch) written += thisBatch position += thisBatch } _size += len } @inline private def allocateNewChunkIfNeeded(): Unit = { if (position == chunkSize) { chunks += allocator(chunkSize) lastChunkIndex += 1 position = 0 } } def toChunkedByteBuffer: ChunkedByteBuffer = { require(closed, \"cannot call toChunkedByteBuffer() unless close() has been called\") require(!toChunkedByteBufferWasCalled, \"toChunkedByteBuffer() can only be called once\") toChunkedByteBufferWasCalled = true if (lastChunkIndex == -1) { new ChunkedByteBuffer(Array.empty[ByteBuffer]) } else { // Copy the first n-1 chunks to the output, and then create an array that fits the last chunk. // An alternative would have been returning an array of ByteBuffers, with the last buffer // bounded to only the last chunk's position. However, given our use case in Spark (to put // the chunks in block manager), only limiting the view bound of the buffer would still // require the block manager to store the whole chunk. val ret = new Array[ByteBuffer](chunks.size) for (i <- 0 until chunks.size - 1) { ret(i) = chunks(i) ret(i).flip() } if (position == chunkSize) { ret(lastChunkIndex) = chunks(lastChunkIndex) ret(lastChunkIndex).flip() } else { ret(lastChunkIndex) = allocator(position) chunks(lastChunkIndex).flip() ret(lastChunkIndex).put(chunks(lastChunkIndex)) ret(lastChunkIndex).flip() StorageUtils.dispose(chunks(lastChunkIndex)) } new ChunkedByteBuffer(ret) } } }\n"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# Spark Memory Management (Project Overview)\n\nThis document summarizes how memory allocation is managed in this repository, based on the code in core and SQL modules.\n\nKey goals of the memory subsystem:\n- Coordinate memory between execution (shuffles, joins, sorts, aggregations) and storage (caching blocks) on-heap and off-heap.\n- Provide per-task memory accounting, spilling, and page allocation APIs for operators.\n- Allow dynamic borrowing between execution and storage in the unified memory model.\n\nCore components and responsibilities\n\n- MemoryManager (core/src/main/scala/org/apache/spark/memory/MemoryManager.scala)\n  - Top-level abstraction managing four pools:\n    - on-heap: StorageMemoryPool, ExecutionMemoryPool\n    - off-heap: StorageMemoryPool, ExecutionMemoryPool\n  - Initializes pool sizes (on-heap via constructor params; off-heap via conf MEMORY_OFFHEAP_SIZE and MEMORY_STORAGE_FRACTION).\n  - Exposes maxOnHeapStorageMemory and maxOffHeapStorageMemory and tracks used memory across pools.\n\n- UnifiedMemoryManager (core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala)\n  - Default implementation used by SparkEnv; unifies execution and storage memory pools so they can borrow from each other.\n  - Dynamically adjusts pool sizes: execution can grow by evicting from storage; storage can grow when execution frees memory.\n  - Key method: acquireExecutionMemory(numBytes, taskAttemptId, memoryMode)\n    - Ensures fairness among tasks and may block until each active task can ramp up to at least ~1/(2N) of the pool before spilling.\n    - Implements logic to \"maybeGrowExecutionPool\" by reclaiming from storage (including eviction when storage grew beyond its region size).\n  - Invariants ensure pool sizes across execution+storage equal the configured max for each memory mode.\n\n- TaskMemoryManager (core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java)\n  - Per-task memory accounting and page management API.\n  - Coordinates with MemoryManager to acquire/release execution memory on behalf of MemoryConsumers.\n  - Maintains registered MemoryConsumers and triggers their spill(size, trigger) callbacks to free memory (fairly, smallest-first selection) when needed.\n  - Provides off/on-heap page allocation: allocatePage(size, consumer) and freePage(page, consumer); exposes pageSizeBytes().\n\n- MemoryConsumer (core/src/main/java/org/apache/spark/memory/MemoryConsumer.java)\n  - Base class for operators and data structures that use execution memory (e.g., sorters, hash maps, shufflers).\n  - Must implement spill(size, trigger): called by TaskMemoryManager to release memory pressure; should free or spill to disk and return bytes released.\n  - Constructed with a TaskMemoryManager and MemoryMode (ON_HEAP/OFF_HEAP).\n\n- MemoryStore (core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala)\n  - Manages storage memory for cached blocks (via BlockManager). Uses MemoryManager for accounting and eviction safety.\n  - Tracks entries in an LRU map; can evict cached blocks to make room for new ones. Includes separate unroll memory tracking per task.\n  - max memory is memoryManager.maxOnHeapStorageMemory + maxOffHeapStorageMemory.\n\n- Pools and modes\n  - StorageMemoryPool and ExecutionMemoryPool are instantiated inside MemoryManager for each MemoryMode.\n  - MemoryMode: ON_HEAP and OFF_HEAP.\n\nHow allocation works by area\n\n- Execution memory (operators):\n  - MemoryConsumers request bytes from TaskMemoryManager -> TaskMemoryManager acquires from MemoryManagers execution pool.\n  - If insufficient memory, TaskMemoryManager chooses other consumers to spill first (based on usage), invoking spill() until enough is freed.\n  - If still insufficient, the request may return fewer bytes than requested or 0, causing the operator to spill itself.\n\n- Storage memory (caching):\n  - MemoryStore asks MemoryManagers storage pools for space to put blocks. If not available, it evicts cached blocks (LRU) to free space.\n  - In the unified manager, storage can shrink if execution needs to grow, and vice versa, within the total region limits.\n\n- Shuffle and SQL operators:\n  - ShuffleExternalSorter, UnsafeExternalSorter, BytesToBytesMap, and other structures extend MemoryConsumer to integrate with TaskMemoryManager.\n  - Many SQL operators obtain the tasks TaskMemoryManager (e.g., TaskContext.get().taskMemoryManager()) to manage pages and spilling.\n\nConfiguration highlights\n\n- Off-heap memory amount and storage fraction are derived from:\n  - spark.memory.offHeap.size (MEMORY_OFFHEAP_SIZE)\n  - spark.memory.storageFraction (MEMORY_STORAGE_FRACTION)\n- UnifiedMemoryManager is created in SparkEnv using the SparkConf and the number of usable cores.\n\nFairness and guarantees\n\n- UnifiedMemoryManager attempts to ensure each active task can ramp to a fair share of the execution pool before spilling.\n- Invariants are asserted to maintain the sum of execution+storage pool sizes equals configured maxima for each mode.\n\nWhere to look in code\n\n- core/src/main/scala/org/apache/spark/memory/MemoryManager.scala\n- core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala\n- core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java\n- core/src/main/java/org/apache/spark/memory/MemoryConsumer.java\n- core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala\n- core/src/main/scala/org/apache/spark/memory/package.scala (subsystem overview and docs)\n\n",
          "prompt_time": 14.433,
          "answer_time": 118,
          "context_tokens": 3657,
          "used_context": [
            "## CLASS: org/apache/spark/memory/UnifiedMemoryManager#\n*/ private[spark] class UnifiedMemoryManager( conf: SparkConf, val maxHeapMemory: Long, onHeapStorageRegionSize: Long, numCores: Int) extends MemoryManager( conf, numCores, onHeapStorageRegionSize, maxHeapMemory - onHeapStorageRegionSize) { private def assertInvariants(): Unit = { assert(onHeapExecutionMemoryPool.poolSize + onHeapStorageMemoryPool.poolSize == maxHeapMemory) assert( offHeapExecutionMemoryPool.poolSize + offHeapStorageMemoryPool.poolSize == maxOffHeapMemory) } assertInvariants() override def maxOnHeapStorageMemory: Long = synchronized { maxHeapMemory - onHeapExecutionMemoryPool.memoryUsed } override def maxOffHeapStorageMemory: Long = synchronized { maxOffHeapMemory - offHeapExecutionMemoryPool.memoryUsed } /** * Try to acquire up to `numBytes` of execution memory for the current task and return the * number of bytes obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. */ override private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized { assertInvariants() assert(numBytes >= 0) val (executionPool, storagePool, storageRegionSize, maxMemory) = memoryMode match { case MemoryMode.ON_HEAP => ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, onHeapStorageRegionSize, maxHeapMemory) case MemoryMode.OFF_HEAP => ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, offHeapStorageMemory, maxOffHeapMemory) } /** * Grow the execution pool by evicting cached blocks, thereby shrinking the storage pool. * * When acquiring memory for a task, the execution pool may need to make multiple * attempts. Each attempt must be able to evict storage in case another task jumps in * and caches a large block between the attempts. This is called once per attempt. */ def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = { if (extraMemoryNeeded > 0) { // There is not enough free memory in the execution pool, so try to reclaim memory from // storage. We can reclaim any free memory from the storage pool. If the storage pool // has grown to become larger than `storageRegionSize`, we can evict blocks and reclaim // the memory that storage has borrowed from execution. val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage > 0) { // Only reclaim as much space as is necessary and available: val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage)) storagePool.decrementPoolSize(spaceToReclaim) executionPool.incrementPoolSize(spaceToReclaim) } } } /** * The size the execution pool would have after evicting storage memory. * * The execution memory pool divides this quantity among the active tasks evenly to cap * the execution memory allocation for each task. It is important to keep this greater * than the execution pool size, which doesn't take into account potential memory that * could be freed by evicting storage. Otherwise we may hit SPARK-12155. * * Additionally, this quantity should be kept below `maxMemory` to arbitrate fairness * in execution memory allocation across tasks, Otherwise, a task may occupy more than * its fair share of execution memory, mistakenly thinking that other tasks can acquire * the portion of storage memory that cannot be evicted. */ def computeMaxExecutionPoolSize(): Long = { maxMemory - math.min(storagePool.memoryUsed, storageRegionSize) } executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, () => computeMaxExecutionPoolSize) } override def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized { assertInvarian...\n",
            "## METHOD: org/apache/spark/memory/UnifiedMemoryManager#maxOffHeapStorageMemory().\noverride def maxOffHeapStorageMemory: Long = synchronized { maxOffHeapMemory - offHeapExecutionMemoryPool.memoryUsed } /** * Try to acquire up to `numBytes` of execution memory for the current task and return the * number of bytes obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. */ override private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long = synchronized { assertInvariants() assert(numBytes >= 0) val (executionPool, storagePool, storageRegionSize, maxMemory) = memoryMode match { case MemoryMode.ON_HEAP => ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, onHeapStorageRegionSize, maxHeapMemory) case MemoryMode.OFF_HEAP => ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, offHeapStorageMemory, maxOffHeapMemory) } /** * Grow the execution pool by evicting cached blocks, thereby shrinking the storage pool. * * When acquiring memory for a task, the execution pool may need to make multiple * attempts. Each attempt must be able to evict storage in case another task jumps in * and caches a large block between the attempts. This is called once per attempt. */ def maybeGrowExecutionPool(extraMemoryNeeded: Long): Unit = { if (extraMemoryNeeded > 0) { // There is not enough free memory in the execution pool, so try to reclaim memory from // storage. We can reclaim any free memory from the storage pool. If the storage pool // has grown to become larger than `storageRegionSize`, we can evict blocks and reclaim // the memory that storage has borrowed from execution. val memoryReclaimableFromStorage = math.max( storagePool.memoryFree, storagePool.poolSize - storageRegionSize) if (memoryReclaimableFromStorage > 0) { // Only reclaim as much space as is necessary and available: val spaceToReclaim = storagePool.freeSpaceToShrinkPool( math.min(extraMemoryNeeded, memoryReclaimableFromStorage)) storagePool.decrementPoolSize(spaceToReclaim) executionPool.incrementPoolSize(spaceToReclaim) } } } /** * The size the execution pool would have after evicting storage memory. * * The execution memory pool divides this quantity among the active tasks evenly to cap * the execution memory allocation for each task. It is important to keep this greater * than the execution pool size, which doesn't take into account potential memory that * could be freed by evicting storage. Otherwise we may hit SPARK-12155. * * Additionally, this quantity should be kept below `maxMemory` to arbitrate fairness * in execution memory allocation across tasks, Otherwise, a task may occupy more than * its fair share of execution memory, mistakenly thinking that other tasks can acquire * the portion of storage memory that cannot be evicted. */ def computeMaxExecutionPoolSize(): Long = { maxMemory - math.min(storagePool.memoryUsed, storageRegionSize) } executionPool.acquireMemory( numBytes, taskAttemptId, maybeGrowExecutionPool, () => computeMaxExecutionPoolSize) } override def acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean = synchronized { assertInvariants() assert(numBytes >= 0) val (executionPool, storagePool, maxMemory) = memoryMode match { case MemoryMode.ON_HEAP => ( onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory) case MemoryMode.OFF_HEAP => ( offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapStorageMemory) } if (numBytes > maxMemory) { // Fail fast if the block simply won't fit logInfo(s\"Will not store $blockId as the required space ($numBytes bytes) exceeds our \" + s\"memory limit ($maxMemory bytes)\") return false } if (numBytes > storagePool.memoryFree) { // There is not enough free memory in the storage po...",
            "## CLASS: org/apache/spark/memory/MemoryManager#\n* * In this context, execution memory refers to that used for computation in shuffles, joins, * sorts and aggregations, while storage memory refers to that used for caching and propagating * internal data across the cluster. There exists one MemoryManager per JVM. */ private[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging { require(onHeapExecutionMemory > 0, \"onHeapExecutionMemory must be > 0\") // -- Methods related to memory allocation policies and bookkeeping ------------------------------ @GuardedBy(\"this\") protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(\"this\") protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP) onHeapStorageMemoryPool.incrementPoolSize(onHeapStorageMemory) onHeapExecutionMemoryPool.incrementPoolSize(onHeapExecutionMemory) protected[this] val maxOffHeapMemory = conf.get(MEMORY_OFFHEAP_SIZE) protected[this] val offHeapStorageMemory = (maxOffHeapMemory * conf.get(MEMORY_STORAGE_FRACTION)).toLong offHeapExecutionMemoryPool.incrementPoolSize(maxOffHeapMemory - offHeapStorageMemory) offHeapStorageMemoryPool.incrementPoolSize(offHeapStorageMemory) /** * Total available on heap memory for storage, in bytes. This amount can vary over time, * depending on the MemoryManager implementation. * In this model, this is equivalent to the amount of memory not occupied by execution. */ def maxOnHeapStorageMemory: Long /** * Total available off heap memory for storage, in bytes. This amount can vary over time, * depending on the MemoryManager implementation. */ def maxOffHeapStorageMemory: Long /** * Set the [[MemoryStore]] used by this manager to evict cached blocks. * This must be set after construction due to initialization ordering constraints. */ final def setMemoryStore(store: MemoryStore): Unit = synchronized { onHeapStorageMemoryPool.setMemoryStore(store) offHeapStorageMemoryPool.setMemoryStore(store) } /** * Acquire N bytes of memory to cache the given block, evicting existing ones if necessary. * * @return whether all N bytes were successfully granted. */ def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean /** * Acquire N bytes of memory to unroll the given block, evicting existing ones if necessary. * * This extra method allows subclasses to differentiate behavior between acquiring storage * memory and acquiring unroll memory. For instance, the memory management model in Spark * 1.5 and before places a limit on the amount of space that can be freed from unrolling. * * @return whether all N bytes were successfully granted. */ def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean /** * Try to acquire up to `numBytes` of execution memory for the current task and return the * number of bytes obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. */ private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long /** * Release numBytes of execution memory belonging to the given task. */ private[memory] def releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized { memoryMode match { case MemoryMode.ON_HEAP => onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP => offHeapExecutionMemoryPool.releaseMem...",
            "## CLASS: org/apache/spark/storage/memory/MemoryStore#\n*/ private[spark] class MemoryStore( conf: SparkConf, blockInfoManager: BlockInfoManager, serializerManager: SerializerManager, memoryManager: MemoryManager, blockEvictionHandler: BlockEvictionHandler) extends Logging { // Note: all changes to memory allocations, notably putting blocks, evicting blocks, and // acquiring or releasing unroll memory, must be synchronized on `memoryManager`! private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]](32, 0.75f, true) // A mapping from taskAttemptId to amount of memory used for unrolling a block (in bytes) // All accesses of this map are assumed to have manually synchronized on `memoryManager` private val onHeapUnrollMemoryMap = mutable.HashMap[Long, Long]() // Note: off-heap unroll memory is only used in putIteratorAsBytes() because off-heap caching // always stores serialized values. private val offHeapUnrollMemoryMap = mutable.HashMap[Long, Long]() // Initial memory to request before unrolling any block private val unrollMemoryThreshold: Long = conf.get(STORAGE_UNROLL_MEMORY_THRESHOLD) /** Total amount of memory available for storage, in bytes. */ private def maxMemory: Long = { memoryManager.maxOnHeapStorageMemory + memoryManager.maxOffHeapStorageMemory } if (maxMemory < unrollMemoryThreshold) { logWarning(s\"Max memory ${Utils.bytesToString(maxMemory)} is less than the initial memory \" + s\"threshold ${Utils.bytesToString(unrollMemoryThreshold)} needed to store a block in \" + s\"memory. Please configure Spark with more memory.\") } logInfo(\"MemoryStore started with capacity %s\".format(Utils.bytesToString(maxMemory))) /** Total storage memory used including unroll memory, in bytes. */ private def memoryUsed: Long = memoryManager.storageMemoryUsed /** * Amount of storage memory, in bytes, used for caching blocks. * This does not include memory used for unrolling. */ private def blocksMemoryUsed: Long = memoryManager.synchronized { memoryUsed - currentUnrollMemory } def getSize(blockId: BlockId): Long = { entries.synchronized { entries.get(blockId).size } } /** * Use `size` to test if there is enough space in MemoryStore. If so, create the ByteBuffer and * put it into MemoryStore. Otherwise, the ByteBuffer won't be created. * * The caller should guarantee that `size` is correct. * * @return true if the put() succeeded, false otherwise. */ def putBytes[T: ClassTag]( blockId: BlockId, size: Long, memoryMode: MemoryMode, _bytes: () => ChunkedByteBuffer): Boolean = { require(!contains(blockId), s\"Block $blockId is already present in the MemoryStore\") if (memoryManager.acquireStorageMemory(blockId, size, memoryMode)) { // We acquired enough memory for the block, so go ahead and put it val bytes = _bytes() assert(bytes.size == size) val entry = new SerializedMemoryEntry[T](bytes, memoryMode, implicitly[ClassTag[T]]) entries.synchronized { entries.put(blockId, entry) } logInfo(\"Block %s stored as bytes in memory (estimated size %s, free %s)\".format( blockId, Utils.bytesToString(size), Utils.bytesToString(maxMemory - blocksMemoryUsed))) true } else { false } } /** * Attempt to put the given block in memory store as values or bytes. * * It's possible that the iterator is too large to materialize and store in memory. To avoid * OOM exceptions, this method will gradually unroll the iterator while periodically checking * whether there is enough free memory. If the block is successfully materialized, then the * temporary unroll memory used during the materialization is \"transferred\" to storage memory, * so we won't acquire more memory than is actually needed to store the block. * * @param blockId The block id. * @param values The values which need be stored. * @param classTag the [[ClassTag]] for the block. * @param memoryMode The values saved memory mode(ON_HEAP or OFF_HEAP). * @param valuesHolder A holder that supports storing record of values into memory store as * values or bytes. * @return if the block is stored successfully, return the stored data size. Else return the * memory has res...\n"
          ],
          "tokens": 0,
          "more_info": "After getting context searched in project for some classes mentioned in context code"
        },
        "without_mcp": {
          "answer": "# How this project manages memory allocation (Spark 3.3.0)\n\nThis repository contains Apache Spark 3.3.0. Spark manages memory using a layered design that separates responsibilities between cluster roles (driver/executors), storage vs. execution needs, and on-heap vs. off-heap allocation. Below is a concise but comprehensive overview tailored to this codebase.\n\n## High-level model\n- Process roles:\n  - Driver process: coordinates the application; also uses memory for planning, dataset metadata, broadcast creation, etc.\n  - Executor processes: run tasks and hold cached data. Most of Sparks managed memory logic applies at the executor level.\n- Memory types by usage:\n  - Execution memory: used while computing (joins, aggregations, sorts, shuffles, window ops). It is shortlived and spillable.\n  - Storage memory: used to cache/persist RDD partitions/DataFrame blocks and to hold broadcast variables.\n  - User & JVM memory: everything outside Sparks managed region (JVM overhead, UDFs, app code objects, thread stacks, code cache, etc.).\n- On-heap vs. off-heap:\n  - On-heap: standard Java objects inside the JVM heap; garbage collected by the JVM.\n  - Off-heap: native memory outside the heap, managed by Spark (Tungsten). Reduces GC pressure and can be faster for I/O and serializationfree formats.\n\n## Unified Memory Manager (default)\nSpark 3.3 uses UnifiedMemoryManager by default to manage executor memory.\n- Total managed region size:\n  - On-heap: spark.memory.fraction (default 0.6) of the executor heap is reserved for Sparks managed memory (both execution + storage). The remainder is \"user/JVM\" memory.\n  - Off-heap (optional): when spark.memory.offHeap.enabled=true, Spark uses a separate, fixed pool sized by spark.memory.offHeap.size for managed memory.\n- Dynamic sharing between execution and storage:\n  - spark.memory.storageFraction (default 0.5) sets the initial split between storage and execution inside the managed region.\n  - The split is elastic: execution can borrow from storage when storage is underutilized and vice versa. Storage borrowing can trigger eviction of cached blocks if space is needed.\n\n## Key components in the codebase\n- MemoryManager (core): Abstracts managed memory. UnifiedMemoryManager is the concrete default; StaticMemoryManager is legacy.\n- TaskMemoryManager (core): A per-task facade that handles fine-grained allocation requests for operators during a task. It allocates memory in fixed-size pages and tracks ownership so memory is reclaimed at task completion or on failure.\n- MemoryConsumer (core): Operators like sort, aggregate, and hash join implement MemoryConsumer to request/release pages from TaskMemoryManager and implement spilling when memory is scarce.\n- MemoryAllocator (core): Backing allocator for on-heap (byte arrays) or off-heap (direct ByteBuffers/native). Page size is controlled by spark.buffer.pageSize.\n- BlockManager (core): Orchestrates storage of blocks in memory and on disk across executors (and the driver). Includes:\n  - MemoryStore: in-JVM or off-heap cached blocks\n  - DiskStore: on-disk persistence and spill files\n  - BlockTransferService/Netty for network transfers\n- Shuffle subsystem (core): Uses execution memory to buffer map outputs and perform merges; spills to disk when necessary (e.g., ExternalSorter, UnsafeExternalSorter, ShuffleWriteProcessor).\n\n## Allocation and spilling during task execution\n- Operators request memory via TaskMemoryManager through MemoryConsumer.acquire(nBytes).\n- If enough space exists in the execution pool, pages are granted.\n- Under pressure, operators implement spill() to write intermediate data to disk and free pages. Sparks external algorithms ensure progress even with limited memory.\n- All per-task allocations are released when the task completes (success, failure, or kill), preventing leaks.\n\n## Storage memory and eviction\n- Caching (RDD/DataFrame persist) and broadcast variables occupy storage memory managed by BlockManager/MemoryStore.\n- When a new block needs storage space:\n  - Spark attempts to use free storage memory or borrow from execution if permissible.\n  - If insufficient, it evicts existing cached blocks based on an LRU-like policy, respecting storage levels and replication.\n- Unroll memory: temporary memory used to materialize a block before putting it in the cache; if unroll cannot be satisfied, the block is not cached and may be stored on disk depending on the chosen StorageLevel.\n\n## On-heap vs. off-heap details\n- On-heap managed memory lives inside the JVM heap and is subject to GC.\n- Off-heap managed memory (spark.memory.offHeap.enabled=true):\n  - Allocated from native memory; not directly visible to the JVM GC.\n  - Requires spark.memory.offHeap.size to be set.\n  - Used heavily by Tungsten/Unsafe operators to store encoded rows and shuffle data efficiently.\n\n## Important configuration knobs\n- Managed region sizing:\n  - spark.memory.fraction (default: 0.6)\n  - spark.memory.storageFraction (default: 0.5 of the managed region)\n  - spark.memory.offHeap.enabled (default: false)\n  - spark.memory.offHeap.size (bytes; required when off-heap is enabled)\n- Page and buffer sizes:\n  - spark.buffer.pageSize (default derived from JVM; commonly 64KB1MB depending on platform)\n  - spark.shuffle.file.buffer (default 32KB)  write-side buffer per shuffle output stream\n  - spark.reducer.maxSizeInFlight / spark.reducer.maxReqsInFlight  controls in-flight shuffle fetches (affects network & memory pressure)\n- Storage/caching behavior:\n  - spark.storage.unrollMemoryThreshold\n  - spark.storage.decommission.* (in newer versions) for graceful decommissioning\n- Networking:\n  - spark.network.io.preferDirectBufs (default: true)  uses direct (off-heap) Netty buffers for IO\n\n## Failure safety and cleanup\n- Per-task memory is released deterministically when tasks end.\n- Spilled files are cleaned up on task completion.\n- BlockManager removes evicted/obsolete blocks and handles replication bookkeeping.\n- Heartbeat + Executor/Task metrics report memory metrics used in the UI for observability.\n\n## Where to look in this repo\n- core/src/main/scala/org/apache/spark/memory/*  MemoryManager, UnifiedMemoryManager, TaskMemoryManager, MemoryConsumer, MemoryAllocator\n- core/src/main/scala/org/apache/spark/storage/*  BlockManager, MemoryStore, DiskStore, StorageLevel\n- core/src/main/scala/org/apache/spark/shuffle/*  shuffle IO and spilling logic\n- sql/core/... execution operators  joins, aggregations, sorts using managed memory\n\n## Summary\nSpark 3.3.0 manages memory with a unified, spill-aware design:\n- A managed memory region (on-heap and/or off-heap) is elastically shared between execution and storage.\n- Task-level page-based allocators provide fast, tracked allocations; operators must spill under pressure.\n- Cached data uses storage memory; eviction and on-disk persistence maintain forward progress.\n- Configuration flags let you tune the split, enable off-heap, and size buffers for workload characteristics.\n",
          "answer_time": 45,
          "tokens": 0
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q009",
      "question": "What is the architecture for logging and error reporting?",
      "category": "general",
      "parameters": {
        "kinds": [
          "CLASS",
          "METHOD",
          "TRAIT"
        ],
        "keywords": [
          "logging",
          "info",
          "debug",
          "error",
          "exception",
          "trace",
          "throwable",
          "logger",
          "throw",
          "message"
        ],
        "top_nodes": 8,
        "max_neighbors": 4
      },
      "ground_truth_contexts": [
        "import org.apache.spark.internal.config._ import org.apache.spark.network.util.JavaUtils import org.apache.spark.util.{ThreadUtils, Utils} private[spark] class DriverLogger(conf: SparkConf) extends Logging { private val UPLOAD_CHUNK_SIZE = 1024 * 1024 private val UPLOAD_INTERVAL_IN_SECS = 5 private val DEFAULT_LAYOUT = \"%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\" private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort) private val localLogFile: String = FileUtils.getFile( Utils.getLocalDir(conf), DriverLogger.DRIVER_LOG_DIR, DriverLogger.DRIVER_LOG_FILE).getAbsolutePath() private var writer: Option[DfsAsyncWriter] = None addLogAppender() private def addLogAppender(): Unit = { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val layout = if (conf.contains(DRIVER_LOG_LAYOUT)) { PatternLayout.newBuilder().withPattern(conf.get(DRIVER_LOG_LAYOUT).get).build() } else { PatternLayout.newBuilder().withPattern(DEFAULT_LAYOUT).build() } val config = logger.getContext.getConfiguration() def log4jFileAppender() = { // SPARK-37853: We can't use the chained API invocation mode because // `AbstractFilterable.Builder.asBuilder()` method will return `Any` in Scala. val builder: Log4jFileAppender.Builder[_] = Log4jFileAppender.newBuilder() builder.withAppend(false) builder.withBufferedIo(false) builder.setConfiguration(config) builder.withFileName(localLogFile) builder.setIgnoreExceptions(false) builder.setLayout(layout) builder.setName(DriverLogger.APPENDER_NAME) builder.build() } val fa = log4jFileAppender() logger.addAppender(fa) fa.start() logInfo(s\"Added a local log appender at: $localLogFile\") } def startSync(hadoopConf: Configuration): Unit = { try { // Setup a writer which moves the local file to hdfs continuously val appId = Utils.sanitizeDirName(conf.getAppId) writer = Some(new DfsAsyncWriter(appId, hadoopConf)) } catch { case e: Exception => logError(s\"Could not persist driver logs to dfs\", e) } } def stop(): Unit = { try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "*/ private[spark] case class ErrorInfo(message: Seq[String], sqlState: Option[String]) { // For compatibility with multi-line error messages @JsonIgnore val messageFormat: String = message.mkString(\"\\n\") } /** * Companion object used by instances of [[SparkThrowable]] to access error class information and * construct error messages. */ private[spark] object SparkThrowableHelper { val errorClassesUrl: URL = Utils.getSparkClassLoader.getResource(\"error/error-classes.json\") val errorClassToInfoMap: SortedMap[String, ErrorInfo] = { val mapper: JsonMapper = JsonMapper.builder() .addModule(DefaultScalaModule) .build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }",
        "private[streaming] case class ReceiverErrorInfo( lastErrorMessage: String = \"\", lastError: String = \"\", lastErrorTime: Long = -1L) /** * Class having information about a receiver. * * @param receiverId the unique receiver id * @param state the current Receiver state * @param scheduledLocations the scheduled locations provided by ReceiverSchedulingPolicy * @param runningExecutor the running executor if the receiver is active * @param name the receiver name * @param endpoint the receiver endpoint. It can be used to send messages to the receiver * @param errorInfo the receiver error information if it fails */ private[streaming] case class ReceiverTrackingInfo( receiverId: Int, state: ReceiverState, scheduledLocations: Option[Seq[TaskLocation]], runningExecutor: Option[ExecutorCacheTaskLocation], name: Option[String] = None, endpoint: Option[RpcEndpointRef] = None, errorInfo: Option[ReceiverErrorInfo] = None) { def toReceiverInfo: ReceiverInfo = ReceiverInfo( receiverId, name.getOrElse(\"\"), state == ReceiverState.ACTIVE, location = runningExecutor.map(_.host).getOrElse(\"\"), executorId = runningExecutor.map(_.executorId).getOrElse(\"\"), lastErrorMessage = errorInfo.map(_.lastErrorMessage).getOrElse(\"\"), lastError = errorInfo.map(_.lastError).getOrElse(\"\"), lastErrorTime = errorInfo.map(_.lastErrorTime).getOrElse(-1L) ) }",
        ".build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }"
      ],
      "key_entities": [
        "org/apache/spark/util/logging/DriverLogger#",
        "org/apache/spark/util/logging/DriverLogger#DfsAsyncWriter#",
        "org/apache/spark/ErrorInfo#",
        "org/apache/spark/streaming/scheduler/ReceiverErrorInfo#",
        "local2:core/src/main/scala/org/apache/spark/ErrorInfo.scala"
      ],
      "claude_stats": {
        "answer": "Based on the code provided, here's the architecture for logging and error reporting in your Spark codebase:\n\n## Driver Logging Architecture\n\nThe **DriverLogger** class manages driver-side logging with local and distributed file system persistence. It creates a Log4j file appender that writes logs to a local file (`driver.log` in the `__driver_logs__` directory), using a configurable pattern layout. When initialized in client mode with DFS persistence enabled, it also starts a **DfsAsyncWriter** that continuously syncs the local log file to HDFS or another distributed filesystem.\n\nThe DfsAsyncWriter runs on a scheduled thread pool, periodically reading chunks from the local log file and writing them to DFS every 5 seconds. It handles stream synchronization using hsync for HDFS streams and hflush for others. On shutdown, the logger removes the appender, closes the async writer, and cleans up local log directories.\n\n## Event Logging Architecture\n\nThe **EventLoggingListener** implements Spark's event logging system by listening to SparkListener events and persisting them as JSON to an event log file. It captures lifecycle events like stage submissions, task completions, job starts/ends, executor additions/removals, and application lifecycle events.\n\nEvents are serialized to JSON using JsonProtocol and written through an EventLogFileWriter. Some events trigger immediate flushes (like job completions, executor changes, and application lifecycle events) while others are buffered. The listener also tracks executor metrics peaks per stage when configured, and supports redacting sensitive properties from logged events.\n\n## File Appending Utilities\n\nThe **FileAppender** class provides the low-level mechanism for continuously reading from an input stream and appending data to a file. It runs a dedicated daemon thread that reads data in configurable buffer-sized chunks and writes to the output file. The system supports rolling file appenders with time-based and size-based rolling policies for executor logs.\n\n## Uncaught Exception Handling\n\nThe **SparkUncaughtExceptionHandler** intercepts uncaught exceptions from threads and logs them with appropriate context. It differentiates between OutOfMemoryErrors and other exceptions, triggering System.exit with specific exit codes. The handler is aware of shutdown states and avoids deadlocks during JVM shutdown by not calling System.exit when shutdown hooks are running.\n\n",
        "prompt_time": 15.273,
        "answer_time": 43,
        "context_tokens": 5813,
        "tokens": 0,
        "used_context": [
          "## CLASS: org/apache/spark/util/logging/DriverLogger#\nimport org.apache.spark.internal.config._ import org.apache.spark.network.util.JavaUtils import org.apache.spark.util.{ThreadUtils, Utils} private[spark] class DriverLogger(conf: SparkConf) extends Logging { private val UPLOAD_CHUNK_SIZE = 1024 * 1024 private val UPLOAD_INTERVAL_IN_SECS = 5 private val DEFAULT_LAYOUT = \"%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\" private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort) private val localLogFile: String = FileUtils.getFile( Utils.getLocalDir(conf), DriverLogger.DRIVER_LOG_DIR, DriverLogger.DRIVER_LOG_FILE).getAbsolutePath() private var writer: Option[DfsAsyncWriter] = None addLogAppender() private def addLogAppender(): Unit = { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val layout = if (conf.contains(DRIVER_LOG_LAYOUT)) { PatternLayout.newBuilder().withPattern(conf.get(DRIVER_LOG_LAYOUT).get).build() } else { PatternLayout.newBuilder().withPattern(DEFAULT_LAYOUT).build() } val config = logger.getContext.getConfiguration() def log4jFileAppender() = { // SPARK-37853: We can't use the chained API invocation mode because // `AbstractFilterable.Builder.asBuilder()` method will return `Any` in Scala. val builder: Log4jFileAppender.Builder[_] = Log4jFileAppender.newBuilder() builder.withAppend(false) builder.withBufferedIo(false) builder.setConfiguration(config) builder.withFileName(localLogFile) builder.setIgnoreExceptions(false) builder.setLayout(layout) builder.setName(DriverLogger.APPENDER_NAME) builder.build() } val fa = log4jFileAppender() logger.addAppender(fa) fa.start() logInfo(s\"Added a local log appender at: $localLogFile\") } def startSync(hadoopConf: Configuration): Unit = { try { // Setup a writer which moves the local file to hdfs continuously val appId = Utils.sanitizeDirName(conf.getAppId) writer = Some(new DfsAsyncWriter(appId, hadoopConf)) } catch { case e: Exception => logError(s\"Could not persist driver logs to dfs\", e) } } def stop(): Unit = { try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
          "## CLASS: org/apache/spark/scheduler/EventLoggingListener#\n private[spark] class EventLoggingListener( appId: String, appAttemptId : Option[String], logBaseDir: URI, sparkConf: SparkConf, hadoopConf: Configuration) extends SparkListener with Logging { import EventLoggingListener._ def this(appId: String, appAttemptId : Option[String], logBaseDir: URI, sparkConf: SparkConf) = this(appId, appAttemptId, logBaseDir, sparkConf, SparkHadoopUtil.get.newConfiguration(sparkConf)) // For testing. private[scheduler] val logWriter: EventLogFileWriter = EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) // For testing. Keep track of all JSON serialized events that have been logged. private[scheduler] val loggedEvents = new mutable.ArrayBuffer[JValue] private val shouldLogBlockUpdates = sparkConf.get(EVENT_LOG_BLOCK_UPDATES) private val shouldLogStageExecutorMetrics = sparkConf.get(EVENT_LOG_STAGE_EXECUTOR_METRICS) private val testing = sparkConf.get(EVENT_LOG_TESTING) // map of (stageId, stageAttempt) to executor metric peaks per executor/driver for the stage private val liveStageExecutorMetrics = mutable.HashMap.empty[(Int, Int), mutable.HashMap[String, ExecutorMetrics]] /** * Creates the log file in the configured log directory.  def start(): Unit = { logWriter.start() initEventLog() } private def initEventLog(): Unit = { val metadata = SparkListenerLogStart(SPARK_VERSION) val eventJson = JsonProtocol.logStartToJson(metadata) val metadataJson = compact(eventJson) logWriter.writeEvent(metadataJson, flushLogger = true) if (testing && loggedEvents != null) { loggedEvents += eventJson } } /** Log the event as JSON.  private def logEvent(event: SparkListenerEvent, flushLogger: Boolean = false): Unit = { val eventJson = JsonProtocol.sparkEventToJson(event) logWriter.writeEvent(compact(render(eventJson)), flushLogger) if (testing) { loggedEvents += eventJson } } // Events that do not trigger a flush override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = { logEvent(event.copy(properties = redactProperties(event.properties))) if (shouldLogStageExecutorMetrics) { // record the peak metrics for the new stage liveStageExecutorMetrics.put((event.stageInfo.stageId, event.stageInfo.attemptNumber()), mutable.HashMap.empty[String, ExecutorMetrics]) } } override def onTaskStart(event: SparkListenerTaskStart): Unit = logEvent(event) override def onTaskGettingResult(event: SparkListenerTaskGettingResult): Unit = logEvent(event) override def onTaskEnd(event: SparkListenerTaskEnd): Unit = { logEvent(event) if (shouldLogStageExecutorMetrics) { val stageKey = (event.stageId, event.stageAttemptId) liveStageExecutorMetrics.get(stageKey).map { metricsPerExecutor => val metrics = metricsPerExecutor.getOrElseUpdate( event.taskInfo.executorId, new ExecutorMetrics()) metrics.compareAndUpdatePeakValues(event.taskExecutorMetrics) } } } override def onEnvironmentUpdate(event: SparkListenerEnvironmentUpdate): Unit = { logEvent(redactEvent(sparkConf, event)) } // Events that trigger a flush override def onStageCompleted(event: SparkListenerStageCompleted): Unit = { if (shouldLogStageExecutorMetrics) { // clear out any previous attempts, that did not have a stage completed event val prevAttemptId = event.stageInfo.attemptNumber() - 1 for (attemptId <- 0 to prevAttemptId) { liveStageExecutorMetrics.remove((event.stageInfo.stageId, attemptId)) } // log the peak executor metrics for the stage, for each live executor, // whether or not the executor is running tasks for the stage val executorOpt = liveStageExecutorMetrics.remove( (event.stageInfo.stageId, event.stageInfo.attemptNumber())) executorOpt.foreach { execMap => execMap.foreach { case (executorId, peakExecutorMetrics) => logEvent(new SparkListenerStageExecutorMetrics(executorId, event.stageInfo.stageId, event.stageInfo.attemptNumber(), peakExecutorMetrics)) } } } // log stage completed event logEvent(event, flushLogger = true) } override def onJobStart(event: SparkListenerJobStart): Unit = { logEvent(event.copy(properties = redactProperties(event.properties)), flushLogger = true) } override def onJobEnd(event: SparkListenerJobEnd): Unit = logEvent(event, flushLogger = true) override def onBlockManagerAdded(event: SparkListenerBlockManagerAdded): Unit = { logEvent(event, flushLogger = true) } override def onBlockManagerRemoved(event: SparkListenerBlockManagerRemoved): Unit = { logEvent(event, flushLogger = true) } override def onUnpersistRDD(event: SparkListenerUnpersistRDD): Unit = { logEvent(event, flushLogger = true) } override def onApplicationStart(event: SparkListenerApplicationStart): Unit = { logEvent(event, flushLogger = true) } override def onApplicationEnd(event: SparkListenerApplicationEnd): Unit = { logEvent(event, flushLogger = true) } override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = { logEvent(event, flushLogger = true) } override def onExecutorRemoved(event: SparkListenerExecutorRemoved): Unit = { logEvent(event, flushLogger = true) } override def onExecutorBlacklisted(event: SparkListenerExecutorBlacklisted): Unit = { logEvent(event, flushLogger = true) } override def onExecutorExcluded(event: SparkListenerExecutorExcluded): Unit = { logEvent(event, flushLogger = true) } override def onExecutorBlacklistedForStage( event: SparkListenerExecutorBlacklistedForStage): Unit = { logEvent(event, flushLogger = true) } override def onExecutorExcludedForStage( event: SparkListenerExecutorExcludedForStage): Unit = { logEvent(event, flushLogger = true) } override def onNodeBlacklistedForStage(event: SparkListenerNodeBlacklistedForStage): Unit = { logEvent(event, flushLogger = true) } override def onNodeExcludedForStage(event: SparkListenerNodeExcludedForStage): Unit = { logEvent(event, flushLogger = true) } override def onExecutorUnblacklisted(event: SparkListenerExecutorUnblacklisted): Unit = { logEvent(event, flushLogger = true) } override def onExecutorUnexcluded(event: SparkListenerExecutorUnexcluded): Unit = { logEvent(event, flushLogger = true) } override def onNodeBlacklisted(event: SparkListenerNodeBlacklisted): Unit = { logEvent(event, flushLogger = true) } override def onNodeExcluded(event: SparkListenerNodeExcluded): Unit = { logEvent(event, flushLogger = true) } override def onNodeUnblacklisted(event: SparkListenerNodeUnblacklisted): Unit = { logEvent(event, flushLogger = true) } override def onNodeUnexcluded(event: SparkListenerNodeUnexcluded): Unit = { logEvent(event, flushLogger = true) } override def onBlockUpdated(event: SparkListenerBlockUpdated): Unit = { if (shouldLogBlockUpdates) { logEvent(event, flushLogger = true) } } override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = { if (shouldLogStageExecutorMetrics) { if (event.execId == SparkContext.DRIVER_IDENTIFIER) { logEvent(event) } event.executorUpdates.foreach { case (stageKey1, newPeaks) => liveStageExecutorMetrics.foreach { case (stageKey2, metricsPerExecutor) => // If the update came from the driver, stageKey1 will be the dummy key (-1, -1), // so record those peaks for all active stages. // Otherwise, record the peaks for the matching stage. if (stageKey1 == DRIVER_STAGE_KEY || stageKey1 == stageKey2) { val metrics = metricsPerExecutor.getOrElseUpdate( event.execId, new ExecutorMetrics()) metrics.compareAndUpdatePeakValues(newPeaks) } } } } } override def onResourceProfileAdded(event: SparkListenerResourceProfileAdded): Unit = { logEvent(event, flushLogger = true) } override def onOtherEvent(event: SparkListenerEvent): Unit = { if (event.logEvent) { logEvent(event, flushLogger = true) } } /** Stop logging events.  def stop(): Unit = { logWriter.stop() } private def redactProperties(properties: Properties): Properties = { if (properties == null) { return properties } val redactedProperties = new Properties // properties may contain some custom local properties such as stage/job description // only properties in sparkConf need to be redacted. val (globalProperties, localProperties) = properties.asScala.toSeq.partition { case (key, _) => sparkConf.contains(key) } (Utils.redact(sparkConf, globalProperties) ++ localProperties).foreach { case (key, value) => redactedProperties.setProperty(key, value) } redactedProperties } } private[spark] object EventLoggingListener extends Logging { val DEFAULT_LOG_DIR = \"/tmp/spark-events\" // Dummy stage key used by driver in executor metrics updates val DRIVER_STAGE_KEY = (-1, -1) private[spark] def redactEvent( sparkConf: SparkConf, event: SparkListenerEnvironmentUpdate): SparkListenerEnvironmentUpdate = { // environmentDetails maps a string descriptor to a set of properties // Similar to: // \"JVM Information\" -> jvmInformation, // \"Spark Properties\" -> sparkProperties, // ... // where jvmInformation, sparkProperties, etc. are sequence of tuples. // We go through the various of properties and redact sensitive information from them. val noRedactProps = Seq(\"Classpath Entries\") val redactedProps = event.environmentDetails.map { case (name, props) if noRedactProps.contains(name) => name -> props case (name, props) => name -> Utils.redact(sparkConf, props) } SparkListenerEnvironmentUpdate(redactedProps) } }",
          "## CLASS: org/apache/spark/util/SparkUncaughtExceptionHandler#\n private[spark] class SparkUncaughtExceptionHandler(val exitOnUncaughtException: Boolean = true) extends Thread.UncaughtExceptionHandler with Logging { override def uncaughtException(thread: Thread, exception: Throwable): Unit = { try { // Make it explicit that uncaught exceptions are thrown when container is shutting down. // It will help users when they analyze the executor logs val inShutdownMsg = if (ShutdownHookManager.inShutdown()) \"[Container in shutdown] \" else \"\" val errMsg = \"Uncaught exception in thread \" logError(inShutdownMsg + errMsg + thread, exception) // We may have been called from a shutdown hook. If so, we must not call System.exit(). // (If we do, we will deadlock.) if (!ShutdownHookManager.inShutdown()) { exception match { case _: OutOfMemoryError => System.exit(SparkExitCode.OOM) case e: SparkFatalException if e.throwable.isInstanceOf[OutOfMemoryError] => // SPARK-24294: This is defensive code, in case that SparkFatalException is // misused and uncaught. System.exit(SparkExitCode.OOM) case _ if exitOnUncaughtException => System.exit(SparkExitCode.UNCAUGHT_EXCEPTION) case _ => // SPARK-30310: Don't System.exit() when exitOnUncaughtException is false } } } catch { case oom: OutOfMemoryError => try { logError(s\"Uncaught OutOfMemoryError in thread $thread, process halted.\", oom) } catch { // absorb any exception/error since we're halting the process case _: Throwable => } Runtime.getRuntime.halt(SparkExitCode.OOM) case t: Throwable => try { logError(s\"Another uncaught exception in thread $thread, process halted.\", t) } catch { case _: Throwable => } Runtime.getRuntime.halt(SparkExitCode.UNCAUGHT_EXCEPTION_TWICE) } } def uncaughtException(exception: Throwable): Unit = { uncaughtException(Thread.currentThread(), exception) } }",
          "## CLASS: org/apache/spark/util/logging/FileAppender#\n private[spark] class FileAppender( inputStream: InputStream, file: File, bufferSize: Int = 8192, closeStreams: Boolean = false ) extends Logging { @volatile private var outputStream: FileOutputStream = null @volatile private var markedForStop = false // has the appender been asked to stopped // Thread that reads the input stream and writes to file private val writingThread = new Thread(\"File appending thread for \" + file) { setDaemon(true) override def run(): Unit = { Utils.logUncaughtExceptions { appendStreamToFile() } } } writingThread.start() /** * Wait for the appender to stop appending, either because input stream is closed * or because of any error in appending  def awaitTermination(): Unit = { writingThread.join() } /** Stop the appender  def stop(): Unit = { markedForStop = true } /** Continuously read chunks from the input stream and append to the file  protected def appendStreamToFile(): Unit = { try { logDebug(\"Started appending thread\") Utils.tryWithSafeFinally { openFile() val buf = new Array[Byte](bufferSize) var n = 0 while (!markedForStop && n != -1) { try { n = inputStream.read(buf) } catch { // An InputStream can throw IOException during read if the stream is closed // asynchronously, so once appender has been flagged to stop these will be ignored case _: IOException if markedForStop => // do nothing and proceed to stop appending } if (n > 0) { appendToFile(buf, n) } } } { try { if (closeStreams) { inputStream.close() } } finally { closeFile() } } } catch { case e: Exception => logError(s\"Error writing stream to file $file\", e) } } /** Append bytes to the file output stream  protected def appendToFile(bytes: Array[Byte], len: Int): Unit = { if (outputStream == null) { openFile() } outputStream.write(bytes, 0, len) } /** Open the file output stream  protected def openFile(): Unit = { outputStream = new FileOutputStream(file, true) logDebug(s\"Opened file $file\") } /** Close the file output stream  protected def closeFile(): Unit = { outputStream.flush() outputStream.close() logDebug(s\"Closed file $file\") } } /** * Companion object to [[org.apache.spark.util.logging.FileAppender]] which has helper * functions to choose the correct type of FileAppender based on SparkConf configuration.  private[spark] object FileAppender extends Logging { /** Create the right appender based on Spark configuration  def apply( inputStream: InputStream, file: File, conf: SparkConf, closeStreams: Boolean = false ) : FileAppender = { val rollingStrategy = conf.get(config.EXECUTOR_LOGS_ROLLING_STRATEGY) val rollingSizeBytes = conf.get(config.EXECUTOR_LOGS_ROLLING_MAX_SIZE) val rollingInterval = conf.get(config.EXECUTOR_LOGS_ROLLING_TIME_INTERVAL) def createTimeBasedAppender(): FileAppender = { val validatedParams: Option[(Long, String)] = rollingInterval match { case \"daily\" => logInfo(s\"Rolling executor logs enabled for $file with daily rolling\") Some((24 * 60 * 60 * 1000L, \"--yyyy-MM-dd\")) case \"hourly\" => logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\") Some((60 * 60 * 1000L, \"--yyyy-MM-dd--HH\")) case \"minutely\" => logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\") Some((60 * 1000L, \"--yyyy-MM-dd--HH-mm\")) case IntParam(seconds) => logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\") Some((seconds * 1000L, \"--yyyy-MM-dd--HH-mm-ss\")) case _ => logWarning(s\"Illegal interval for rolling executor logs [$rollingInterval], \" + s\"rolling logs not enabled\") None } validatedParams.map { case (interval, pattern) => new RollingFileAppender( inputStream, file, new TimeBasedRollingPolicy(interval, pattern), conf, closeStreams = closeStreams) }.getOrElse { new FileAppender(inputStream, file, closeStreams = closeStreams) } } def createSizeBasedAppender(): FileAppender = { rollingSizeBytes match { case IntParam(bytes) => logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\") new RollingFileAppender( inputStream, file, new SizeBasedRollingPolicy(bytes), conf, closeStreams = closeStreams) case _ => logWarning( s\"Illegal size [$rollingSizeBytes] for rolling executor logs, rolling logs not enabled\") new FileAppender(inputStream, file, closeStreams = closeStreams) } } rollingStrategy match { case \"\" => new FileAppender(inputStream, file, closeStreams = closeStreams) case \"time\" => createTimeBasedAppender() case \"size\" => createSizeBasedAppender() case _ => logWarning( s\"Illegal strategy [$rollingStrategy] for rolling executor logs, \" + s\"rolling logs not enabled\") new FileAppender(inputStream, file, closeStreams = closeStreams) } } }",
          "## CLASS: org/apache/spark/util/logging/DriverLogger#DfsAsyncWriter#\ntry { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# Architecture for Logging and Error Reporting\n\nLast updated: 2025-12-27 12:10 (local time)\n\nThis document describes how logging and error reporting are implemented in the visible parts of this Spark 3.3.0 codebase. It consolidates the classes, flows, and configuration touchpoints involved in:\n- Structured event logging (Spark listener events)\n- Driver runtime log capture and distributed filesystem (DFS) persistence\n- SQL plan and rule change logging, including associated metrics\n- Generic stream-to-file appenders and error handling patterns\n\nThe goal is to provide a self-contained reference for contributors who need to understand or extend logging behavior, investigate failures, or add diagnostics.\n\n---\n\n## Core logging abstraction\n\nMany components mix in `org.apache.spark.internal.Logging` (e.g., `EventLoggingListener`, `DriverLogger`, `DfsAsyncWriter`, `PlanChangeLogger`, `FileAppender`).\nThese classes use methods such as `logInfo`, `logDebug`, `logWarning`, `logError`, and `logTrace` to emit messages consistently.\n\nKey properties of `Logging` usage:\n- Uniform log formatting and level control via underlying logging framework (Log4j in Spark 3.x).\n- Structured context in messages where possible (e.g., app IDs, paths, rule names) to improve grep-ability.\n- Guarded, non-fatal error logging via helpers (e.g., `Utils.tryLogNonFatalError`) during cleanup.\n\n---\n\n## 1) Structured event logging (Spark events)\n\nClass: `org.apache.spark.scheduler.EventLoggingListener`\n\nPurpose: Serializes Spark listener events to JSON and writes them to an event log file via an `EventLogFileWriter`. This produces a durable, structured record of application lifecycle and execution events for later analysis and Spark History Server.\n\nKey fields:\n- `logWriter: EventLogFileWriter`  the sink for JSON events.\n- `loggedEvents: ArrayBuffer[JValue]`  test-only buffer of serialized events.\n- Flags from `SparkConf`:\n  - `EVENT_LOG_BLOCK_UPDATES`\n  - `EVENT_LOG_STAGE_EXECUTOR_METRICS`\n  - `EVENT_LOG_TESTING`\n\nPrimary flow:\n1. `start()` calls `logWriter.start()` then `initEventLog()` to emit a `SparkListenerLogStart` metadata record (via `JsonProtocol.logStartToJson`), written using `logWriter.writeEvent(...)`.\n2. `logEvent(event, flushLogger)` converts a `SparkListenerEvent` to JSON with `JsonProtocol.sparkEventToJson` and writes it with optional flush.\n3. Various `onXxx` listener callbacks call `logEvent(...)`, forming a consistent JSON event stream for later analysis.\n\nOperational notes:\n- Event log files can be compressed or rolled depending on configuration.\n- These logs are primarily consumed by Spark History Server and tools that parse `JsonProtocol` output.\n\n---\n\n## 2) Driver runtime log capture and DFS persistence\n\nClass: `org.apache.spark.util.logging.DriverLogger`\n\nPurpose: Attaches a Log4j appender to the root logger to write driver logs to a local file, and optionally syncs that file to a distributed filesystem (e.g., HDFS) periodically.\n\nKey fields/constants:\n- `localLogFile: String`  file path under a local directory (`Utils.getLocalDir(conf)` / `DRIVER_LOG_DIR` / `DRIVER_LOG_FILE`).\n- `UPLOAD_CHUNK_SIZE`, `UPLOAD_INTERVAL_IN_SECS`, `DEFAULT_LAYOUT`, `LOG_FILE_PERMISSIONS`.\n- `writer: Option[DfsAsyncWriter]`  manages DFS syncing.\n\nLifecycle and flow:\n- On construction, `addLogAppender()` builds a Log4j File Appender (using a layout from `DRIVER_LOG_LAYOUT` or a default) and registers it with the root logger. Logs start being written to `localLogFile` immediately.\n- `startSync(hadoopConf)` creates a `DfsAsyncWriter` (with sanitized app id) to push local logs to DFS. On failure, it logs the exception via `logError(\"Could not persist driver logs to dfs\", e)`.\n- `stop()` removes the appender, stops it, closes the DFS writer if present, and deletes the local log directory; any exception is reported via `logError(\"Error in persisting driver logs\", e)`. Cleanup uses `Utils.tryLogNonFatalError` guards.\n\nInner class: `DriverLogger.DfsAsyncWriter` (Runnable)\n\nPurpose: Periodically copies newly written bytes from the local log file to a DFS file using a scheduled executor.\n\nInitialization (`init()`):\n- Resolves `rootDir` from `DRIVER_LOG_DFS_DIR`; validates directory exists (otherwise throws `RuntimeException` with a clear message).\n- Opens `inStream` on the local log file and an `FSDataOutputStream` for the DFS target via `SparkHadoopUtil.createFile(...)` and sets permissions.\n- Starts a daemon single-threaded scheduler (`ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\")`) to run `this` at a fixed delay of `UPLOAD_INTERVAL_IN_SECS` seconds.\n- Logs start with `logInfo`, including the DFS path.\n\nSync loop (`run()`):\n- If streams are open, checks `inStream.available()` and writes available bytes in chunks (`UPLOAD_CHUNK_SIZE`) to the DFS output stream.\n- On termination/close (via `closeWriter()` in surrounding code), streams are closed and the scheduler is shut down. Errors propagate to caller, which logs them in `DriverLogger.stop()`.\n\nOperational notes:\n- Ensures driver logs are durable and discoverable even when drivers run outside the cluster file system namespace.\n- Chunked sync avoids large memory use; interval-based scheduling trades off durability lag vs. overhead.\n\n---\n\n## 3) SQL plan/rule change logging and metrics\n\nClass: `org.apache.spark.sql.catalyst.rules.PlanChangeLogger[TreeType <: TreeNode[_]]`\n\nPurpose: Logs optimizer and plan changes at a configured level, including before/after plan diffs and rule metrics, to aid understanding and debugging of query optimization.\n\nConfig-driven behavior:\n- Fetches `planChangeLogLevel`, `planChangeRules`, and `planChangeBatches` from `SQLConf`.\n\nCapabilities:\n- `logRule(ruleName, oldPlan, newPlan)`: if the plan changed and rule is enabled, logs a side-by-side diff via `logBasedOnLevel`.\n- `logBatch(batchName, oldPlan, newPlan)`: logs batch result or no effect.\n- `logMetrics(metrics: QueryExecutionMetrics)`: logs counts and times derived from the provided metrics.\n\nLog level mapping:\n- `logBasedOnLevel(f)` maps configured level strings to `logTrace`, `logDebug`, `logInfo`, `logWarning`, or `logError` at runtime, providing flexible verbosity.\n\n---\n\n## 4) Generic stream-to-file appending with error reporting\n\nClass: `org.apache.spark.util.logging.FileAppender`\n\nPurpose: Runs a dedicated daemon thread to continuously copy bytes from an `InputStream` to a file, with lifecycle control and robust error logging.\n\nKey elements:\n- `outputStream: FileOutputStream`\n- `markedForStop: Boolean`\n- `writingThread` that wraps execution in `Utils.logUncaughtExceptions`\n\nFlow and behavior:\n- `appendStreamToFile()` reads from the input stream in a loop and writes to the file. It handles `IOException` during read when marked to stop, and ensures streams are closed in a `finally` block.\n- Errors during the copy are reported via `logError(s\"Error writing stream to file $file\", e)`.\n- Exposes `stop()` and `awaitTermination()` for lifecycle management.\n\n---\n\n## Error reporting patterns observed\n\n- Try/catch blocks log failures using `logError` with context and the exception object, e.g. in `DriverLogger.startSync`, `DriverLogger.stop`, and `FileAppender.appendStreamToFile`.\n- Resource cleanup is consistently protected with `try/finally` or helpers like `Utils.tryLogNonFatalError` and `JavaUtils.closeQuietly`.\n- Some initialization failures are signaled with explicit exceptions (e.g., `DfsAsyncWriter.init()` throws a `RuntimeException` if the DFS directory does not exist), making startup issues visible to users/operators.\n\n---\n\n## Configuration touchpoints (as seen)\n\n- Logging layout/pattern and appender: `DRIVER_LOG_LAYOUT`; appender name `DriverLogger.APPENDER_NAME`.\n- DFS persistence: `DRIVER_LOG_DFS_DIR`, `DRIVER_LOG_ALLOW_EC`, and permissions set via `FsPermission`.\n- SQL plan logging: `SQLConf.planChangeLogLevel`, `SQLConf.planChangeRules`, `SQLConf.planChangeBatches`.\n- Event logging behaviors: `EVENT_LOG_BLOCK_UPDATES`, `EVENT_LOG_STAGE_EXECUTOR_METRICS`, `EVENT_LOG_TESTING`.\n\n---\n\n## Overall flow summary\n\n- Runtime logs: Root logger writes to a local file via `DriverLogger`s Log4j appender; optionally synced to DFS at a fixed interval by `DfsAsyncWriter`.\n- Structured events: `EventLoggingListener` captures Spark listener events, serializes them to JSON, and writes them to an event log file for post-run analysis.\n- SQL-specific diagnostics: `PlanChangeLogger` logs optimizer changes and metrics at a configurable verbosity.\n- Utilities like `FileAppender` and the `Logging` mixin provide thread-based copying and consistent error reporting via `logError`.\n\n---\n\n## Practical tips for contributors\n\n- Prefer structured, contextual messages: include identifiers (app ID, stage ID, rule name) in log lines.\n- Choose log levels intentionally: use `DEBUG`/`TRACE` for high-volume internals; reserve `WARN`/`ERROR` for actionable issues.\n- When adding new DFS or I/O interactions, follow the patterns in `DriverLogger.DfsAsyncWriter` for chunked copying and safe cleanup.\n- For SQL optimizer work, leverage `PlanChangeLogger` to provide diffs and metrics to users diagnosing plan changes.\n- Ensure exceptions contain actionable messages and are logged with stack traces where appropriate.\n\n---\n\n## Related files in this repository (paths)\n\n- `core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala`\n- `core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala`\n- `core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala`\n- `sql/core/src/main/scala/org/apache/spark/sql/catalyst/rules/PlanChangeLogger.scala`\n\n",
          "prompt_time": 14.127,
          "answer_time": 128,
          "context_tokens": 7028,
          "used_context": [
            "## CLASS: org/apache/spark/scheduler/EventLoggingListener#\n private[spark] class EventLoggingListener( appId: String, appAttemptId : Option[String], logBaseDir: URI, sparkConf: SparkConf, hadoopConf: Configuration) extends SparkListener with Logging { import EventLoggingListener._ def this(appId: String, appAttemptId : Option[String], logBaseDir: URI, sparkConf: SparkConf) = this(appId, appAttemptId, logBaseDir, sparkConf, SparkHadoopUtil.get.newConfiguration(sparkConf)) // For testing. private[scheduler] val logWriter: EventLogFileWriter = EventLogFileWriter(appId, appAttemptId, logBaseDir, sparkConf, hadoopConf) // For testing. Keep track of all JSON serialized events that have been logged. private[scheduler] val loggedEvents = new mutable.ArrayBuffer[JValue] private val shouldLogBlockUpdates = sparkConf.get(EVENT_LOG_BLOCK_UPDATES) private val shouldLogStageExecutorMetrics = sparkConf.get(EVENT_LOG_STAGE_EXECUTOR_METRICS) private val testing = sparkConf.get(EVENT_LOG_TESTING) // map of (stageId, stageAttempt) to executor metric peaks per executor/driver for the stage private val liveStageExecutorMetrics = mutable.HashMap.empty[(Int, Int), mutable.HashMap[String, ExecutorMetrics]] /** * Creates the log file in the configured log directory.  def start(): Unit = { logWriter.start() initEventLog() } private def initEventLog(): Unit = { val metadata = SparkListenerLogStart(SPARK_VERSION) val eventJson = JsonProtocol.logStartToJson(metadata) val metadataJson = compact(eventJson) logWriter.writeEvent(metadataJson, flushLogger = true) if (testing && loggedEvents != null) { loggedEvents += eventJson } } /** Log the event as JSON.  private def logEvent(event: SparkListenerEvent, flushLogger: Boolean = false): Unit = { val eventJson = JsonProtocol.sparkEventToJson(event) logWriter.writeEvent(compact(render(eventJson)), flushLogger) if (testing) { loggedEvents += eventJson } } // Events that do not trigger a flush override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = { logEvent(event.copy(properties = redactProperties(event.properties))) if (shouldLogStageExecutorMetrics) { // record the peak metrics for the new stage liveStageExecutorMetrics.put((event.stageInfo.stageId, event.stageInfo.attemptNumber()), mutable.HashMap.empty[String, ExecutorMetrics]) } } override def onTaskStart(event: SparkListenerTaskStart): Unit = logEvent(event) override def onTaskGettingResult(event: SparkListenerTaskGettingResult): Unit = logEvent(event) override def onTaskEnd(event: SparkListenerTaskEnd): Unit = { logEvent(event) if (shouldLogStageExecutorMetrics) { val stageKey = (event.stageId, event.stageAttemptId) liveStageExecutorMetrics.get(stageKey).map { metricsPerExecutor => val metrics = metricsPerExecutor.getOrElseUpdate( event.taskInfo.executorId, new ExecutorMetrics()) metrics.compareAndUpdatePeakValues(event.taskExecutorMetrics) } } } override def onEnvironmentUpdate(event: SparkListenerEnvironmentUpdate): Unit = { logEvent(redactEvent(sparkConf, event)) } // Events that trigger a flush override def onStageCompleted(event: SparkListenerStageCompleted): Unit = { if (shouldLogStageExecutorMetrics) { // clear out any previous attempts, that did not have a stage completed event val prevAttemptId = event.stageInfo.attemptNumber() - 1 for (attemptId <- 0 to prevAttemptId) { liveStageExecutorMetrics.remove((event.stageInfo.stageId, attemptId)) } // log the peak executor metrics for the stage, for each live executor, // whether or not the executor is running tasks for the stage val executorOpt = liveStageExecutorMetrics.remove( (event.stageInfo.stageId, event.stageInfo.attemptNumber())) executorOpt.foreach { execMap => execMap.foreach { case (executorId, peakExecutorMetrics) => logEvent(new SparkListenerStageExecutorMetrics(executorId, event.stageInfo.stageId, event.stageInfo.attemptNumber(), peakExecutorMetrics)) } } } // log stage completed event logEvent(event, flushLogger = true) } override def onJobStart(event: SparkListenerJobStart): Unit = { logEvent(event.copy(properties = redactProperties(event.properties)), flushLogger = true) } override def onJobEnd(event: SparkListenerJobEnd): Unit = logEvent(event, flushLogger = true) override def onBlockManagerAdded(event: SparkListenerBlockManagerAdded): Unit = { logEvent(event, flushLogger = true) } override def onBlockManagerRemoved(event: SparkListenerBlockManagerRemoved): Unit = { logEvent(event, flushLogger = true) } override def onUnpersistRDD(event: SparkListenerUnpersistRDD): Unit = { logEvent(event, flushLogger = true) } override def onApplicationStart(event: SparkListenerApplicationStart): Unit = { logEvent(event, flushLogger = true) } override def onApplicationEnd(event: SparkListenerApplicationEnd): Unit = { logEvent(event, flushLogger = true) } override def onExecutorAdded(event: SparkListenerExecutorAdded): Unit = { logEvent(event, flushLogger = true) } override def onExecutorRemoved(event: SparkListenerExecutorRemoved): Unit = { logEvent(event, flushLogger = true) } override def onExecutorBlacklisted(event: SparkListenerExecutorBlacklisted): Unit = { logEvent(event, flushLogger = true) } override def onExecutorExcluded(event: SparkListenerExecutorExcluded): Unit = { logEvent(event, flushLogger = true) } override def onExecutorBlacklistedForStage( event: SparkListenerExecutorBlacklistedForStage): Unit = { logEvent(event, flushLogger = true) } override def onExecutorExcludedForStage( event: SparkListenerExecutorExcludedForStage): Unit = { logEvent(event, flushLogger = true) } override def onNodeBlacklistedForStage(event: SparkListenerNodeBlacklistedForStage): Unit = { logEvent(event, flushLogger = true) } override def onNodeExcludedForStage(event: SparkListenerNodeExcludedForStage): Unit = { logEvent(event, flushLogger = true) } override def onExecutorUnblacklisted(event: SparkListenerExecutorUnblacklisted): Unit = { logEvent(event, flushLogger = true) } override def onExecutorUnexcluded(event: SparkListenerExecutorUnexcluded): Unit = { logEvent(event, flushLogger = true) } override def onNodeBlacklisted(event: SparkListenerNodeBlacklisted): Unit = { logEvent(event, flushLogger = true) } override def onNodeExcluded(event: SparkListenerNodeExcluded): Unit = { logEvent(event, flushLogger = true) } override def onNodeUnblacklisted(event: SparkListenerNodeUnblacklisted): Unit = { logEvent(event, flushLogger = true) } override def onNodeUnexcluded(event: SparkListenerNodeUnexcluded): Unit = { logEvent(event, flushLogger = true) } override def onBlockUpdated(event: SparkListenerBlockUpdated): Unit = { if (shouldLogBlockUpdates) { logEvent(event, flushLogger = true) } } override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = { if (shouldLogStageExecutorMetrics) { if (event.execId == SparkContext.DRIVER_IDENTIFIER) { logEvent(event) } event.executorUpdates.foreach { case (stageKey1, newPeaks) => liveStageExecutorMetrics.foreach { case (stageKey2, metricsPerExecutor) => // If the update came from the driver, stageKey1 will be the dummy key (-1, -1), // so record those peaks for all active stages. // Otherwise, record the peaks for the matching stage. if (stageKey1 == DRIVER_STAGE_KEY || stageKey1 == stageKey2) { val metrics = metricsPerExecutor.getOrElseUpdate( event.execId, new ExecutorMetrics()) metrics.compareAndUpdatePeakValues(newPeaks) } } } } } override def onResourceProfileAdded(event: SparkListenerResourceProfileAdded): Unit = { logEvent(event, flushLogger = true) } override def onOtherEvent(event: SparkListenerEvent): Unit = { if (event.logEvent) { logEvent(event, flushLogger = true) } } /** Stop logging events.  def stop(): Unit = { logWriter.stop() } private def redactProperties(properties: Properties): Properties = { if (properties == null) { return properties } val redactedProperties = new Properties // properties may contain some custom local properties such as stage/job description // only properties in sparkConf need to be redacted. val (globalProperties, localProperties) = properties.asScala.toSeq.partition { case (key, _) => sparkConf.contains(key) } (Utils.redact(sparkConf, globalProperties) ++ localProperties).foreach { case (key, value) => redactedProperties.setProperty(key, value) } redactedProperties } } private[spark] object EventLoggingListener extends Logging { val DEFAULT_LOG_DIR = \"/tmp/spark-events\" // Dummy stage key used by driver in executor metrics updates val DRIVER_STAGE_KEY = (-1, -1) private[spark] def redactEvent( sparkConf: SparkConf, event: SparkListenerEnvironmentUpdate): SparkListenerEnvironmentUpdate = { // environmentDetails maps a string descriptor to a set of properties // Similar to: // \"JVM Information\" -> jvmInformation, // \"Spark Properties\" -> sparkProperties, // ... // where jvmInformation, sparkProperties, etc. are sequence of tuples. // We go through the various of properties and redact sensitive information from them. val noRedactProps = Seq(\"Classpath Entries\") val redactedProps = event.environmentDetails.map { case (name, props) if noRedactProps.contains(name) => name -> props case (name, props) => name -> Utils.redact(sparkConf, props) } SparkListenerEnvironmentUpdate(redactedProps) } }",
            "## CLASS: org/apache/spark/util/logging/DriverLogger#\nimport org.apache.spark.internal.config._ import org.apache.spark.network.util.JavaUtils import org.apache.spark.util.{ThreadUtils, Utils} private[spark] class DriverLogger(conf: SparkConf) extends Logging { private val UPLOAD_CHUNK_SIZE = 1024 * 1024 private val UPLOAD_INTERVAL_IN_SECS = 5 private val DEFAULT_LAYOUT = \"%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\" private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort) private val localLogFile: String = FileUtils.getFile( Utils.getLocalDir(conf), DriverLogger.DRIVER_LOG_DIR, DriverLogger.DRIVER_LOG_FILE).getAbsolutePath() private var writer: Option[DfsAsyncWriter] = None addLogAppender() private def addLogAppender(): Unit = { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val layout = if (conf.contains(DRIVER_LOG_LAYOUT)) { PatternLayout.newBuilder().withPattern(conf.get(DRIVER_LOG_LAYOUT).get).build() } else { PatternLayout.newBuilder().withPattern(DEFAULT_LAYOUT).build() } val config = logger.getContext.getConfiguration() def log4jFileAppender() = { // SPARK-37853: We can't use the chained API invocation mode because // `AbstractFilterable.Builder.asBuilder()` method will return `Any` in Scala. val builder: Log4jFileAppender.Builder[_] = Log4jFileAppender.newBuilder() builder.withAppend(false) builder.withBufferedIo(false) builder.setConfiguration(config) builder.withFileName(localLogFile) builder.setIgnoreExceptions(false) builder.setLayout(layout) builder.setName(DriverLogger.APPENDER_NAME) builder.build() } val fa = log4jFileAppender() logger.addAppender(fa) fa.start() logInfo(s\"Added a local log appender at: $localLogFile\") } def startSync(hadoopConf: Configuration): Unit = { try { // Setup a writer which moves the local file to hdfs continuously val appId = Utils.sanitizeDirName(conf.getAppId) writer = Some(new DfsAsyncWriter(appId, hadoopConf)) } catch { case e: Exception => logError(s\"Could not persist driver logs to dfs\", e) } } def stop(): Unit = { try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
            "## CLASS: org/apache/spark/util/logging/DriverLogger#DfsAsyncWriter#\ntry { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
            "## CLASS: org/apache/spark/sql/catalyst/rules/PlanChangeLogger#\nclass PlanChangeLogger[TreeType <: TreeNode[_]] extends Logging { private val logLevel = SQLConf.get.planChangeLogLevel private val logRules = SQLConf.get.planChangeRules.map(Utils.stringToSeq) private val logBatches = SQLConf.get.planChangeBatches.map(Utils.stringToSeq) def logRule(ruleName: String, oldPlan: TreeType, newPlan: TreeType): Unit = { if (!newPlan.fastEquals(oldPlan)) { if (logRules.isEmpty || logRules.get.contains(ruleName)) { def message(): String = { s\"\"\" |=== Applying Rule $ruleName === |${sideBySide(oldPlan.treeString, newPlan.treeString).mkString(\"\\n\")} \"\"\".stripMargin } logBasedOnLevel(message) } } } def logBatch(batchName: String, oldPlan: TreeType, newPlan: TreeType): Unit = { if (logBatches.isEmpty || logBatches.get.contains(batchName)) { def message(): String = { if (!oldPlan.fastEquals(newPlan)) { s\"\"\" |=== Result of Batch $batchName === |${sideBySide(oldPlan.treeString, newPlan.treeString).mkString(\"\\n\")} \"\"\".stripMargin } else { s\"Batch $batchName has no effect.\" } } logBasedOnLevel(message) } } def logMetrics(metrics: QueryExecutionMetrics): Unit = { val totalTime = metrics.time / NANOS_PER_SECOND.toDouble val totalTimeEffective = metrics.timeEffective / NANOS_PER_SECOND.toDouble val message = s\"\"\" |=== Metrics of Executed Rules === |Total number of runs: ${metrics.numRuns} |Total time: $totalTime seconds |Total number of effective runs: ${metrics.numEffectiveRuns} |Total time of effective runs: $totalTimeEffective seconds \"\"\".stripMargin logBasedOnLevel(message) } private def logBasedOnLevel(f: => String): Unit = { logLevel match { case \"TRACE\" => logTrace(f) case \"DEBUG\" => logDebug(f) case \"INFO\" => logInfo(f) case \"WARN\" => logWarning(f) case \"ERROR\" => logError(f) case _ => logTrace(f) } } } abstract class RuleExecutor[TreeType <: TreeNode[_]] extends Logging { /** * An execution strategy for rules that indicates the maximum number of executions. If the * execution reaches fix point (i.e. converge) before maxIterations, it will stop.  abstract class Strategy { /** The maximum number of executions.  def maxIterations: Int /** Whether to throw exception when exceeding the maximum number.  def errorOnExceed: Boolean = false /** The key of SQLConf setting to tune maxIterations  def maxIterationsSetting: String = null } /** A strategy that is run once and idempotent.  case object Once extends Strategy { val maxIterations = 1 } /** * A strategy that runs until fix point or maxIterations times, whichever comes first. * Especially, a FixedPoint(1) batch is supposed to run only once.  case class FixedPoint( override val maxIterations: Int, override val errorOnExceed: Boolean = false, override val maxIterationsSetting: String = null) extends Strategy /** A batch of rules.  protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*) /** Defines a sequence of rule batches, to be overridden by the implementation.  protected def batches: Seq[Batch] /** Once batches that are excluded in the idempotence checker  protected val excludedOnceBatches: Set[String] = Set.empty /** * Defines a check function that checks for structural integrity of the plan after the execution * of each rule. For example, we can check whether a plan is still resolved after each rule in * `Optimizer`, so we can catch rules that return invalid plans. The check function returns * `false` if the given plan doesn't pass the structural integrity check.  protected def isPlanIntegral(previousPlan: TreeType, currentPlan: TreeType): Boolean = true /** * Util method for checking whether a plan remains the same if re-optimized.  private def checkBatchIdempotence(batch: Batch, plan: TreeType): Unit = { val reOptimized = batch.rules.foldLeft(plan) { case (p, rule) => rule(p) } if (!plan.fastEquals(reOptimized)) { throw QueryExecutionErrors.onceStrategyIdempotenceIsBrokenForBatchError( batch.name, plan, reOptimized) } } /** * Executes the batches of rules defined by the subclass, and also tracks timing info for each * rule using the provided tracker. * @see [[execute]]  def executeAndTrack(plan: TreeType, tracker: QueryPlanningTracker): TreeType = { QueryPlanningTracker.withTracker(tracker) { execute(plan) } } /** * Executes the batches of rules defined by the subclass. The batches are executed serially * using the defined execution strategy. Within each batch, rules are also executed serially.  def execute(plan: TreeType): TreeType = { var curPlan = plan val queryExecutionMetrics = RuleExecutor.queryExecutionMeter val planChangeLogger = new PlanChangeLogger[TreeType]() val tracker: Option[QueryPlanningTracker] = QueryPlanningTracker.get val beforeMetrics = RuleExecutor.getCurrentMetrics() // Run the structural integrity checker against the initial input if (!isPlanIntegral(plan, plan)) { throw QueryExecutionErrors.structuralIntegrityOfInputPlanIsBrokenInClassError( this.getClass.getName.stripSuffix(\"$\")) } batches.foreach { batch => val batchStartPlan = curPlan var iteration = 1 var lastPlan = curPlan var continue = true // Run until fix point (or the max number of iterations as specified in the strategy. while (continue) { curPlan = batch.rules.foldLeft(curPlan) { case (plan, rule) => val startTime = System.nanoTime() val result = rule(plan) val runTime = System.nanoTime() - startTime val effective = !result.fastEquals(plan) if (effective) { queryExecutionMetrics.incNumEffectiveExecution(rule.ruleName) queryExecutionMetrics.incTimeEffectiveExecutionBy(rule.ruleName, runTime) planChangeLogger.logRule(rule.ruleName, plan, result) } queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime) queryExecutionMetrics.incNumExecution(rule.ruleName) // Record timing information using QueryPlanningTracker tracker.foreach(_.recordRuleInvocation(rule.ruleName, runTime, effective)) // Run the structural integrity checker against the plan after each rule. if (effective && !isPlanIntegral(plan, result)) { throw QueryExecutionErrors.structuralIntegrityIsBrokenAfterApplyingRuleError( rule.ruleName, batch.name) } result } iteration += 1 if (iteration > batch.strategy.maxIterations) { // Only log if this is a rule that is supposed to run more than once. if (iteration != 2) { val endingMsg = if (batch.strategy.maxIterationsSetting == null) { \".\" } else { s\", please set '${batch.strategy.maxIterationsSetting}' to a larger value.\" } val message = s\"Max iterations (${iteration - 1}) reached for batch ${batch.name}\" + s\"$endingMsg\" if (Utils.isTesting || batch.strategy.errorOnExceed) { throw new RuntimeException(message) } else { logWarning(message) } } // Check idempotence for Once batches. if (batch.strategy == Once && Utils.isTesting && !excludedOnceBatches.contains(batch.name)) { checkBatchIdempotence(batch, curPlan) } continue = false } if (curPlan.fastEquals(lastPlan)) { logTrace( s\"Fixed point reached for batch ${batch.name} after ${iteration - 1} iterations.\") continue = false } lastPlan = curPlan } planChangeLogger.logBatch(batch.name, batchStartPlan, curPlan) } planChangeLogger.logMetrics(RuleExecutor.getCurrentMetrics() - beforeMetrics) curPlan } }",
            "## CLASS: org/apache/spark/util/logging/FileAppender#\n private[spark] class FileAppender( inputStream: InputStream, file: File, bufferSize: Int = 8192, closeStreams: Boolean = false ) extends Logging { @volatile private var outputStream: FileOutputStream = null @volatile private var markedForStop = false // has the appender been asked to stopped // Thread that reads the input stream and writes to file private val writingThread = new Thread(\"File appending thread for \" + file) { setDaemon(true) override def run(): Unit = { Utils.logUncaughtExceptions { appendStreamToFile() } } } writingThread.start() /** * Wait for the appender to stop appending, either because input stream is closed * or because of any error in appending  def awaitTermination(): Unit = { writingThread.join() } /** Stop the appender  def stop(): Unit = { markedForStop = true } /** Continuously read chunks from the input stream and append to the file  protected def appendStreamToFile(): Unit = { try { logDebug(\"Started appending thread\") Utils.tryWithSafeFinally { openFile() val buf = new Array[Byte](bufferSize) var n = 0 while (!markedForStop && n != -1) { try { n = inputStream.read(buf) } catch { // An InputStream can throw IOException during read if the stream is closed // asynchronously, so once appender has been flagged to stop these will be ignored case _: IOException if markedForStop => // do nothing and proceed to stop appending } if (n > 0) { appendToFile(buf, n) } } } { try { if (closeStreams) { inputStream.close() } } finally { closeFile() } } } catch { case e: Exception => logError(s\"Error writing stream to file $file\", e) } } /** Append bytes to the file output stream  protected def appendToFile(bytes: Array[Byte], len: Int): Unit = { if (outputStream == null) { openFile() } outputStream.write(bytes, 0, len) } /** Open the file output stream  protected def openFile(): Unit = { outputStream = new FileOutputStream(file, true) logDebug(s\"Opened file $file\") } /** Close the file output stream  protected def closeFile(): Unit = { outputStream.flush() outputStream.close() logDebug(s\"Closed file $file\") } } /** * Companion object to [[org.apache.spark.util.logging.FileAppender]] which has helper * functions to choose the correct type of FileAppender based on SparkConf configuration.  private[spark] object FileAppender extends Logging { /** Create the right appender based on Spark configuration  def apply( inputStream: InputStream, file: File, conf: SparkConf, closeStreams: Boolean = false ) : FileAppender = { val rollingStrategy = conf.get(config.EXECUTOR_LOGS_ROLLING_STRATEGY) val rollingSizeBytes = conf.get(config.EXECUTOR_LOGS_ROLLING_MAX_SIZE) val rollingInterval = conf.get(config.EXECUTOR_LOGS_ROLLING_TIME_INTERVAL) def createTimeBasedAppender(): FileAppender = { val validatedParams: Option[(Long, String)] = rollingInterval match { case \"daily\" => logInfo(s\"Rolling executor logs enabled for $file with daily rolling\") Some((24 * 60 * 60 * 1000L, \"--yyyy-MM-dd\")) case \"hourly\" => logInfo(s\"Rolling executor logs enabled for $file with hourly rolling\") Some((60 * 60 * 1000L, \"--yyyy-MM-dd--HH\")) case \"minutely\" => logInfo(s\"Rolling executor logs enabled for $file with rolling every minute\") Some((60 * 1000L, \"--yyyy-MM-dd--HH-mm\")) case IntParam(seconds) => logInfo(s\"Rolling executor logs enabled for $file with rolling $seconds seconds\") Some((seconds * 1000L, \"--yyyy-MM-dd--HH-mm-ss\")) case _ => logWarning(s\"Illegal interval for rolling executor logs [$rollingInterval], \" + s\"rolling logs not enabled\") None } validatedParams.map { case (interval, pattern) => new RollingFileAppender( inputStream, file, new TimeBasedRollingPolicy(interval, pattern), conf, closeStreams = closeStreams) }.getOrElse { new FileAppender(inputStream, file, closeStreams = closeStreams) } } def createSizeBasedAppender(): FileAppender = { rollingSizeBytes match { case IntParam(bytes) => logInfo(s\"Rolling executor logs enabled for $file with rolling every $bytes bytes\") new RollingFileAppender( inputStream, file, new SizeBasedRollingPolicy(bytes), conf, closeStreams = closeStreams) case _ => logWarning( s\"Illegal size [$rollingSizeBytes] for rolling executor logs, rolling logs not enabled\") new FileAppender(inputStream, file, closeStreams = closeStreams) } } rollingStrategy match { case \"\" => new FileAppender(inputStream, file, closeStreams = closeStreams) case \"time\" => createTimeBasedAppender() case \"size\" => createSizeBasedAppender() case _ => logWarning( s\"Illegal strategy [$rollingStrategy] for rolling executor logs, \" + s\"rolling logs not enabled\") new FileAppender(inputStream, file, closeStreams = closeStreams) } } }"
          ],
          "tokens": 0,
          "more_info": "Returned basic answer with 3 sentences, needed to be asked for full solution in .md file"
        },
        "without_mcp": {
          "answer": "# Logging and Error Reporting Architecture (Spark 3.3.x)\n\nLast updated: 2025-12-27 13:29\n\n## Overview\nApache Spark uses a layered approach for logging and error reporting that separates API usage from the concrete logging backend and provides both runtime logs and structured event data for post-mortem analysis.\n\n- Logging facade: SLF4J\n- Default backend: Log4j 2 (configurable)\n- Code-level API: org.apache.spark.internal.Logging trait\n- Runtime log streams: driver and executor logs (stdout/stderr + loggers)\n- Structured events: SparkListenerBus  event logs  Spark History Server\n- Error taxonomy: SparkException hierarchy with domain-specific exceptions (e.g., AnalysisException); many errors tagged with error classes and SQLSTATE-like info in SQL modules\n\n## Key Components\n\n1. SLF4J (Facade)\n   - Spark code references org.slf4j.Logger via the internal Logging trait, avoiding compile-time coupling to a specific backend.\n\n2. Log4j 2 (Default Backend)\n   - Configured via conf/log4j2.properties.template (copy/rename to log4j2.properties to activate custom settings).\n   - Supports appenders (console, rolling file, socket, etc.), layouts, and per-logger levels.\n\n3. Spark \"Logging\" Trait\n   - org.apache.spark.internal.Logging mixed into many classes (drivers, executors, SQL components, etc.).\n   - Provides lazily evaluated log methods (logInfo, logWarn, logError, etc.) and consistent logger naming.\n\n4. Driver and Executor Log Streams\n   - Driver: logs emitted by the applications main process.\n   - Executors: logs for task execution; surfaced by the cluster manager (YARN, Kubernetes, Standalone) via log aggregation.\n   - Both include JVM stdout/stderr and Log4j 2 logger outputs.\n\n5. Spark Listener Bus and Event Logs\n   - The SparkListenerBus publishes structured events (job start/end, stage, task metrics, SQL query execution events, etc.).\n   - When spark.eventLog.enabled=true, these are serialized to storage (spark.eventLog.dir) for offline analysis.\n   - The Spark History Server reads event logs to reconstruct UIs and timelines for completed applications.\n\n6. Exception and Error Reporting\n   - Core exception: org.apache.spark.SparkException; subsystem-specific exceptions include AnalysisException, ParseException, TaskNotSerializableException, FileAlreadyExistsException (in SQL/DataSource paths), etc.\n   - SQL and some core modules annotate errors with error classes and parameters to produce consistent, actionable messages.\n   - Failures propagate through the scheduler (DAGScheduler/TaskScheduler) with retries and backoff for transient issues.\n\n## Data Flow\n\n1) Application code calls logX methods via the Logging trait  delegated to SLF4J  implemented by Log4j 2.\n2) Logs are emitted in driver and executor JVMs per process; appenders handle output (console, files, rolling, remote).\n3) Structured runtime events are emitted on the listener bus and optionally persisted as event logs.\n4) History Server consumes event logs to present post-mortem UIs and timelines; raw logs remain available via cluster manager UIs.\n\n## Configuration and Tuning\n\n- Backend and Levels\n  - Place conf/log4j2.properties in the classpath (copy from conf/log4j2.properties.template) to customize levels and appenders.\n  - Common categories:\n    - org.apache.spark (application logs)\n    - org.apache.hadoop (I/O and filesystem)\n    - org.eclipse.jetty / org.sparkproject.jetty (web UI)\n  - Use WARN/ERROR in production to reduce noise; DEBUG/TRACE for diagnostics.\n\n- Rolling and Retention\n  - Configure Log4j 2 RollingFileAppender for both driver and executor logs to prevent disk exhaustion.\n  - For cluster managers, leverage built-in log aggregation and retention policies (e.g., YARN log aggregation, Kubernetes sidecar or container logging).\n\n- Event Logging\n  - spark.eventLog.enabled=true to enable event persistence.\n  - spark.eventLog.dir to a durable, shared storage (HDFS/S3/ADLS/GCS) for History Server.\n  - Optionally enable compression: spark.eventLog.compress=true.\n\n- Executor/Driver Specific\n  - Executor log size/rotation can be controlled by Log4j 2 file appenders in executor classpath.\n  - Driver logs depend on the deployment mode (client vs. cluster) and cluster manager.\n\n## Error Handling Semantics\n\n- Scheduler-Level\n  - Task failures trigger retries up to spark.task.maxFailures; stage retries managed by DAGScheduler.\n  - Speculative execution may mitigate stragglers but not logic errors.\n\n- Exception Propagation\n  - Fatal driver errors typically abort the application; executor errors are reported back to the driver with failure reasons.\n  - The drivers uncaught exception handler logs stack traces via the logging backend before shutdown.\n\n- Structured Errors (SQL)\n  - Many SQL errors include an error class ID and message parameters to aid triage and documentation lookup.\n\n## Observability and Integrations\n\n- Metrics (complementary to logs)\n  - Sparks metrics system can emit to sinks (CSV, JMX, Graphite, Prometheus via JMX exporter), aiding correlation with logs.\n\n- External Shipping\n  - Use Log4j 2 SocketAppender/HTTP appenders, or cluster-native log collectors (e.g., Kubernetes logging drivers, YARN log aggregation) to forward logs to ELK/OpenSearch, Splunk, or cloud-native logging stacks.\n\n## Operational Guidance\n\n- For Production\n  - Enable event logging and deploy a Spark History Server pointed to spark.eventLog.dir.\n  - Keep org.apache.spark at INFO or WARN; elevate to DEBUG temporarily during incident analysis.\n  - Configure rolling policies and retention; verify executor-classpath logging config is applied on all nodes.\n\n- During Incidents\n  - Collect: driver log, relevant executor logs, and the applications event log.\n  - Correlate timestamps between logs and event timeline (stages/tasks) in the History Server.\n  - Look for error class IDs (SQL) and root-cause exceptions; check retries/backoffs to distinguish transient vs. permanent failures.\n\n## File Locations in This Repository\n\n- Logging configuration template: conf/log4j2.properties.template\n- Source references to logging API: widespread imports of org.apache.spark.internal.Logging across core and SQL modules.\n\n## Glossary\n\n- Driver: the process running the Spark applications main function and DAG scheduling.\n- Executor: worker JVMs that run tasks.\n- Event Log: structured JSON-like event stream written by the driver for offline analysis.\n- History Server: service that renders UIs from event logs for completed applications.\n\n## References\n\n- Apache Spark Documentation: Monitoring and Instrumentation\n- Log4j 2 Documentation\n- SLF4J Documentation\n",
          "answer_time": 92,
          "tokens": 0
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q010",
      "question": "What are all classes that class TaskSetManager calls?",
      "category": "top",
      "ground_truth_contexts": [
        " Neighbors (sorted by combined metric importance) of Node: org/apache/spark/scheduler/TaskSetManager#, with name: tasksetmanager kind: CLASS located in: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala:\n1.Node_id: org/apache/spark/scheduler/TaskInfo#, name: taskinfo, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, relations: CALL\n2.Node_id: org/apache/spark/scheduler/ShuffleMapTask#, name: shufflemaptask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, relations: CALL\n3.Node_id: org/apache/spark/scheduler/PendingTasksByLocality#, name: pendingtasksbylocality, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n4.Node_id: org/apache/spark/scheduler/TaskSetExcludelist#, name: tasksetexcludelist, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala, relations: CALL\n5.Node_id: org/apache/spark/util/collection/MedianHeap#, name: medianheap, kind: CLASS, uri: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, relations: CALL\n6.Node_id: org/apache/spark/scheduler/BarrierPendingLaunchTask#, name: barrierpendinglaunchtask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n"
      ],
      "key_entities": [
        "org/apache/spark/scheduler/TaskInfo#",
        "org/apache/spark/scheduler/ShuffleMapTask#",
        "org/apache/spark/scheduler/PendingTasksByLocality#",
        "org/apache/spark/scheduler/TaskSetExcludelist#",
        "org/apache/spark/util/collection/MedianHeap#",
        "org/apache/spark/scheduler/BarrierPendingLaunchTask#"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, the `TaskSetManager` class calls the following classes:\n\n1. **TaskInfo** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala`\n2. **ShuffleMapTask** - located in `core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala`\n3. **PendingTasksByLocality** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala` (same file as TaskSetManager)\n4. **TaskSetExcludelist** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala`\n5. **MedianHeap** - located in `core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala`\n6. **BarrierPendingLaunchTask** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala` (same file as TaskSetManager)\n\nThese classes are sorted by their combined metric importance.",
        "prompt_time": 0.347,
        "answer_time": 14,
        "context_tokens": 560,
        "tokens": 0,
        "used_context": [
          "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/scheduler/TaskSetManager#, with name: tasksetmanager kind: CLASS located in: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala:\n1.Node_id: org/apache/spark/scheduler/TaskInfo#, name: taskinfo, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, relations: CALL\n2.Node_id: org/apache/spark/scheduler/ShuffleMapTask#, name: shufflemaptask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, relations: CALL\n3.Node_id: org/apache/spark/scheduler/PendingTasksByLocality#, name: pendingtasksbylocality, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n4.Node_id: org/apache/spark/scheduler/TaskSetExcludelist#, name: tasksetexcludelist, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala, relations: CALL\n5.Node_id: org/apache/spark/util/collection/MedianHeap#, name: medianheap, kind: CLASS, uri: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, relations: CALL\n6.Node_id: org/apache/spark/scheduler/BarrierPendingLaunchTask#, name: barrierpendinglaunchtask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Full Solution: Classes called by org.apache.spark.scheduler.TaskSetManager\n\nGenerated: 2025-12-27 11:19 (local time)\n\nThis document provides the complete answer to the question: \"What are all classes that class TaskSetManager calls?\" based on the repositorys code graph analysis of Spark 3.3.0 within this project.\n\nSource of truth: Repository code graph (CALL relations) as queried during this session.\n\n---\n\n### Result: Called classes\n\nTaskSetManager calls the following classes:\n\n1) org.apache.spark.scheduler.TaskInfo  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala\n\n2) org.apache.spark.scheduler.ShuffleMapTask  \n   - File: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala\n\n3) org.apache.spark.scheduler.PendingTasksByLocality  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala\n\n4) org.apache.spark.scheduler.TaskSetExcludelist  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala\n\n5) org.apache.spark.util.collection.MedianHeap  \n   - File: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala\n\n6) org.apache.spark.scheduler.BarrierPendingLaunchTask  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala\n\n---\n\n### Notes and context\n\n- The list reflects direct CALL relations from TaskSetManager to other classes in the project, as returned by the code graph tool used in this environment. \n- Some of the listed classes (e.g., PendingTasksByLocality, BarrierPendingLaunchTask) are defined in the same source file as TaskSetManager but are separate classes referenced/called by it.\n- This document intentionally focuses on class-level calls (not methods, fields, or inheritance relationships).\n\n---\n\n### How this was derived\n\n- We queried the repositorys static code graph for CALL relations originating from org.apache.spark.scheduler.TaskSetManager.  \n- The tool enumerated each class that TaskSetManager invokes, along with their source file locations.  \n- The six classes above were returned by the graph and constitute the complete set for this query within the current project snapshot.\n\n---\n\n### Optional next steps\n\n",
          "prompt_time": 0.272,
          "answer_time": 60,
          "context_tokens": 347,
          "used_context": [
            "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/scheduler/TaskSetManager#, with name: tasksetmanager kind: CLASS located in: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala:\n1.Node_id: org/apache/spark/scheduler/TaskInfo#, name: taskinfo, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, relations: CALL\n2.Node_id: org/apache/spark/scheduler/ShuffleMapTask#, name: shufflemaptask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, relations: CALL\n3.Node_id: org/apache/spark/scheduler/PendingTasksByLocality#, name: pendingtasksbylocality, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n4.Node_id: org/apache/spark/scheduler/TaskSetExcludelist#, name: tasksetexcludelist, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala, relations: CALL\n5.Node_id: org/apache/spark/util/collection/MedianHeap#, name: medianheap, kind: CLASS, uri: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, relations: CALL\n6.Node_id: org/apache/spark/scheduler/BarrierPendingLaunchTask#, name: barrierpendinglaunchtask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL"
          ],
          "tokens": 0,
          "more_info": "Returned basic list with parameters, needed to be asked for .md file"
        },
        "without_mcp": {
          "answer": "# Classes called/used by org.apache.spark.scheduler.TaskSetManager\n\nThis list enumerates classes, traits, case classes, and singleton objects that TaskSetManager directly constructs, references, extends, or calls methods on within Spark 3.3.0 core source (core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala).\n\nNotes:\n- Includes inner classes defined in the same file that TaskSetManager uses.\n- Includes Scala/Java standard library classes where they are explicitly instantiated or referenced.\n- Includes Spark classes whose methods are invoked via fields (e.g., sched, sched.dagScheduler), even if not explicitly imported.\n\nSpark Scheduler and execution-related:\n- org.apache.spark.scheduler.TaskSchedulerImpl (constructor parameter `sched`; calls methods such as newTaskId, getRacksForHosts, getExecutorsAliveOnHost, getRackForHost, CPUS_PER_TASK, backend.killTask, dagScheduler..., isExecutorAlive, hasExecutorsAliveOnHost, hasHostAliveOnRack, mapOutputTracker)\n- org.apache.spark.scheduler.Schedulable (trait implemented)\n- org.apache.spark.internal.Logging (trait mixed in)\n- org.apache.spark.scheduler.DAGScheduler (accessed via `sched.dagScheduler`: taskStarted, taskGettingResult, taskEnded, taskSetFailed, speculativeTaskSubmitted)\n- org.apache.spark.scheduler.SchedulerBackend (accessed via `sched.backend`: killTask)\n- org.apache.spark.MapOutputTracker (accessed via `sched.mapOutputTracker.getEpoch`)\n- org.apache.spark.scheduler.TaskSet (managed task set)\n- org.apache.spark.scheduler.Task (elements of `taskSet.tasks`; serialized, named, etc.)\n- org.apache.spark.scheduler.ShuffleMapTask (type-checked via isInstanceOf)\n- org.apache.spark.scheduler.TaskInfo (constructed)\n- org.apache.spark.scheduler.TaskDescription (constructed)\n- org.apache.spark.scheduler.TaskLocality (enum/ADT constants and types)\n- org.apache.spark.scheduler.TaskLocation.ExecutorCacheTaskLocation (pattern matched)\n- org.apache.spark.scheduler.TaskLocation.HDFSCacheTaskLocation (pattern matched)\n- org.apache.spark.scheduler.Pool (parent pool reference type)\n\nSpark health/exclusion and speculation:\n- org.apache.spark.scheduler.HealthTracker (optional; methods like updateExcludedForFetchFailure)\n- org.apache.spark.scheduler.TaskSetExcludelist (constructed via helper; updates on failures)\n\nSpark resources and environment:\n- org.apache.spark.SparkEnv (to obtain serializer)\n- org.apache.spark.serializer.SerializerInstance (via SparkEnv.closureSerializer.newInstance)\n- org.apache.spark.resource.ResourceInformation (used in resource assignment maps)\n- org.apache.spark.resource.ResourceProfile (obtained from ResourceProfileManager via SparkContext)\n\nSpark errors, states, reasons, and failures:\n- org.apache.spark.TaskState.TaskState (task state type)\n- org.apache.spark.errors.SparkCoreErrors (failToSerializeTaskError)\n- org.apache.spark.scheduler.FetchFailed (matched)\n- org.apache.spark.scheduler.ExceptionFailure (matched; uses accums, metricPeaks, className, description, exception)\n- org.apache.spark.scheduler.TaskKilled (matched; uses accums, metricPeaks)\n- org.apache.spark.scheduler.ExecutorLostFailure (matched)\n- org.apache.spark.scheduler.TaskFailedReason (matched; includes TaskResultLost and others)\n- org.apache.spark.TaskOutputFileAlreadyExistException (matched via classOf)\n- org.apache.spark.scheduler.ExecutorExited (matched)\n- org.apache.spark.scheduler.ExecutorKilled (matched)\n- org.apache.spark.scheduler.ExecutorDecommission (matched)\n- org.apache.spark.scheduler.ExecutorProcessLost (matched)\n\nSpark utilities and accumulators/metrics:\n- org.apache.spark.util.Utils (timeTakenMs, logging helpers)\n- org.apache.spark.util.Clock (constructor parameter; used for timing)\n- org.apache.spark.util.SystemClock (default clock)\n- org.apache.spark.util.collection.MedianHeap (constructed for successful task durations)\n- org.apache.spark.util.AccumulatorV2 (handled from results/failures)\n- org.apache.spark.util.LongAccumulator (cast/read for result size tracking)\n\nSpark internal configuration entries used:\n- org.apache.spark.internal.config (namespace via wildcard import)\n  - SPECULATION_ENABLED, SPECULATION_QUANTILE, SPECULATION_MULTIPLIER,\n    SPECULATION_TASK_DURATION_THRESHOLD\n  - EXECUTOR_DECOMMISSION_KILL_INTERVAL\n  - LOCALITY_WAIT_PROCESS, LOCALITY_WAIT_NODE, LOCALITY_WAIT_RACK\n\nStandard library and Java classes:\n- scala.collection.mutable.HashMap\n- scala.collection.mutable.HashSet\n- scala.collection.mutable.ArrayBuffer\n- scala.collection.immutable.Map\n- java.util.concurrent.ConcurrentLinkedQueue\n- java.util.concurrent.TimeUnit\n- java.nio.ByteBuffer\n- scala.util.control.NonFatal\n- java.io.NotSerializableException\n\nInner classes/case classes within TaskSetManager.scala used by TaskSetManager:\n- org.apache.spark.scheduler.TaskSetManager.PendingTasksByLocality (constructed)\n- org.apache.spark.scheduler.TaskSetManager.BarrierPendingLaunchTask (constructed and stored)\n\nThis list was compiled by examining calls, constructions (new), pattern matches, and member method invocations appearing in TaskSetManager.scala of Spark 3.3.0.",
          "answer_time": 350,
          "tokens": 0,
          "more_info": "Answer is to precise, include Java, Scala libraries"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q011",
      "question": "What are all related entities of method merge?",
      "category": "top",
      "ground_truth_contexts": [
        "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/sql/execution/aggregate/ScalaUDAF#merge()., with name: merge. kind: METHOD located in: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala: 1.Node_id: org/apache/spark/sql/expressions/UserDefinedAggregateFunction#merge()., name: merge, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala, relations: CALL 2.Node_id: org/apache/spark/sql/execution/aggregate/ScalaUDAF#, name: scalaudaf, kind: CLASS, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, relations: DECLARATION_BY"
      ],
      "key_entities": [
        "org/apache/spark/sql/expressions/UserDefinedAggregateFunction#merge()",
        "org/apache/spark/sql/execution/aggregate/ScalaUDAF#"
      ],
      "claude_stats": {
        "answer": "Based on the search results, here are all the entities related to the **method `merge`** (from `DifferentiableLossAggregator`):\n\n**Entities called by `merge`:**\n1. **`blas` (OBJECT)** - from `org/apache/spark/ml/linalg/BLAS`\n2. **`getblas()` (METHOD)** - from `org/apache/spark/ml/linalg/BLAS`\n\n**Parent/Container:**\n3. **`DifferentiableLossAggregator` (TRAIT)** - the trait that declares the `merge` method\n\n**Type-related:**\n4. **`Agg` (TYPE_PARAMETER)** - the type parameter used as the return type of `merge`\n\n**Entities that call `merge`:**\n5. **`calculate()` (METHOD)** - from `RDDLossFunction`, which calls this `merge` method\n\n**Summary:** The `merge` method is part of the `DifferentiableLossAggregator` trait. It calls BLAS operations (`blas` object and `getblas()` method) for linear algebra computations, returns a type based on the `Agg` type parameter, and is invoked by the `calculate()` method in `RDDLossFunction`.\n\n",
        "prompt_time": 0.427,
        "answer_time": 17,
        "context_tokens": 348,
        "tokens": 0,
        "used_context": [
          "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()., with name: merge kind: METHOD located in: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala:\n1.Node_id: org/apache/spark/ml/linalg/BLAS., name: blas, kind: OBJECT, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n2.Node_id: org/apache/spark/ml/linalg/BLAS.getBLAS()., name: getblas, kind: METHOD, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n3.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#, name: differentiablelossaggregator, kind: TRAIT, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: DECLARATION_BY\n4.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#[Agg], name: agg, kind: TYPE_PARAMETER, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: RETURN_TYPE\n5.Node_id: org/apache/spark/ml/optim/loss/RDDLossFunction#calculate()., name: calculate, kind: METHOD, uri: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala, relations: CALL_BY"
        ],
        "more_info": "Chose wrong merge method"
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Related entities of method `merge`\n\nTimestamp: 2025-12-27 11:24 local time\n\nBased on the repositorys code graph, the method `merge` refers to:\n\n- org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()\n- File: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala\n\nAll related entities and their relation types are:\n\n- CALL  org/apache/spark/ml/linalg/BLAS (object)\n  - File: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala\n- CALL  org/apache/spark/ml/linalg/BLAS.getBLAS() (method)\n  - File: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala\n- DECLARATION_BY  org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator (trait)\n  - File: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala\n- RETURN_TYPE  Type parameter `[Agg]` of `DifferentiableLossAggregator`\n  - File: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala\n- CALL_BY  org/apache/spark/ml/optim/loss/RDDLossFunction#calculate() (method)\n  - File: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala\n\n",
          "prompt_time": 0.351,
          "answer_time": 81,
          "context_tokens": 348,
          "used_context": [
            "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()., with name: merge kind: METHOD located in: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala:\n1.Node_id: org/apache/spark/ml/linalg/BLAS., name: blas, kind: OBJECT, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n2.Node_id: org/apache/spark/ml/linalg/BLAS.getBLAS()., name: getblas, kind: METHOD, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n3.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#, name: differentiablelossaggregator, kind: TRAIT, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: DECLARATION_BY\n4.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#[Agg], name: agg, kind: TYPE_PARAMETER, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: RETURN_TYPE\n5.Node_id: org/apache/spark/ml/optim/loss/RDDLossFunction#calculate()., name: calculate, kind: METHOD, uri: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala, relations: CALL_BY\n"
          ],
          "tokens": 0,
          "more_info": "Returned basic list with parameters, needed to be asked for .md file, same query 2 times, got wrong merge method"
        },
        "without_mcp": {
          "answer": "# Related entities of the method: merge\n\nThis document enumerates the primary code entities related to the method named \"merge\" in the Apache Spark 3.3.0 codebase, focusing on the central and broadly used contract AccumulatorV2.merge and its concrete overrides and representative call sites. File paths are relative to the repository root.\n\nNotes\n- Scope: Given the large number of different methods named merge across the codebase, this report centers on the AccumulatorV2[IN, OUT]#merge method, which is core to Sparks metrics and accumulators. Other unrelated merges (e.g., statistics, shuffle file merges) are out of scope unless they are direct overrides/call sites of AccumulatorV2.merge.\n\n\n## Declaration (primary subject)\n- core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala\n  - abstract class AccumulatorV2[IN, OUT]\n    - def merge(other: AccumulatorV2[IN, OUT]): Unit  (lines ~150154)\n    - Purpose: Merge another accumulator of the same type into this one (merge-in-place contract).\n\n\n## Built-in concrete overrides of AccumulatorV2.merge\n- core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala\n  - class LongAccumulator extends AccumulatorV2[java.lang.Long, java.lang.Long]\n    - override def merge(other: AccumulatorV2[jl.Long, jl.Long]): Unit (lines ~374381)\n    - Behavior: Adds sum and count from the other LongAccumulator; rejects mismatched types.\n  - class DoubleAccumulator extends AccumulatorV2[java.lang.Double, java.lang.Double]\n    - override def merge(other: AccumulatorV2[jl.Double, jl.Double]): Unit (lines ~452459)\n    - Behavior: Adds sum and count from the other DoubleAccumulator; rejects mismatched types.\n  - class CollectionAccumulator[T] extends AccumulatorV2[T, java.util.List[T]]\n    - override def merge(other: AccumulatorV2[T, java.util.List[T]]): Unit (lines ~501505)\n    - Behavior: Appends all elements from the other CollectionAccumulator; rejects mismatched types.\n\n\n## SQL module concrete overrides of AccumulatorV2.merge\n- sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala\n  - class SQLMetric extends AccumulatorV2[Long, Long]\n    - override def merge(other: AccumulatorV2[Long, Long]): Unit (lines ~5662)\n    - Behavior: Adds values with handling for initial placeholder values; throws on type mismatch.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala\n  - class SetAccumulator[T] extends AccumulatorV2[T, java.util.Set[T]] (nested in DebugExec)\n    - override def merge(other: AccumulatorV2[T, java.util.Set[T]]): Unit (lines ~222224)\n    - Behavior: Adds all elements from the others set.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/AggregatingAccumulator.scala\n  - class AggregatingAccumulator extends AccumulatorV2[InternalRow, InternalRow]\n    - override def merge(other: AccumulatorV2[InternalRow, InternalRow]): Unit (around lines ~152200)\n    - Behavior: Merges aggregation buffers using mergeExpressions/projections; applies SQLConf on driver.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala\n  - case class EventTimeStats and an accumulator subclass\n    - override def merge(other: AccumulatorV2[Long, EventTimeStats]): Unit (around lines ~7785)\n    - Behavior: Merges watermark statistics across tasks/partitions.\n\n\n## Python API accumulator override\n- core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala\n  - class PythonAccumulatorV2 extends AccumulatorV2[Array[Byte], java.util.List[Array[Byte]]] (name as used in code)\n    - override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit (around line ~714)\n    - Behavior: Synchronized merge that may delegate to super.merge for same-type accumulators (see around line ~720).\n\n\n## Representative call sites of AccumulatorV2.merge\nThese are places in Spark that merge accumulator updates on the driver or combine task metrics by calling merge on AccumulatorV2 instances.\n\n- core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala\n  - acc.merge(updates.asInstanceOf[AccumulatorV2[Any, Any]]) (line ~1610)\n  - Context: Driver receiving task accumulator updates merges them into the driver-side accumulator.\n\n- core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala\n  - tmAcc.merge(acc.asInstanceOf[AccumulatorV2[Any, Any]]) (line ~322)\n  - Context: Merge per-task metrics accumulators.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala\n  - SQLMetric is merged via standard accumulator mechanisms in the SQL UI update pipeline; the merge implementation is above.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala\n  - SetAccumulator.merge used when collating debug stats from tasks to driver.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala\n  - The EventTimeStats accumulator is merged to compute global watermarks.\n\n- core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala\n  - PythonAccumulatorV2.merge used to merge Python-side accumulator updates.\n\n\n## Types related to the AccumulatorV2.merge contract\n- org.apache.spark.util.AccumulatorV2[IN, OUT]\n  - Methods: isZero, copy, reset, add, merge, value; serialization hooks (withBufferSerialized, writeReplace/readObject).\n  - Metadata/registration helpers: AccumulatorContext, SparkContext.register, TaskContext.registerAccumulator.\n\n- Concrete accumulator classes listed above, plus their value/element types:\n  - LongAccumulator  IN=java.lang.Long, OUT=java.lang.Long\n  - DoubleAccumulator  IN=java.lang.Double, OUT=java.lang.Double\n  - CollectionAccumulator[T]  IN=T, OUT=java.util.List[T]\n  - SQLMetric  IN=Long, OUT=Long\n  - SetAccumulator[T]  IN=T, OUT=java.util.Set[T]\n  - AggregatingAccumulator  IN=InternalRow, OUT=InternalRow\n  - EventTimeWatermark accumulator  IN=Long, OUT=EventTimeStats\n  - PythonAccumulatorV2  IN=Array[Byte], OUT=java.util.List[Array[Byte]]\n\n\n## Out-of-scope merges (non-AccumulatorV2)\nFor clarity, there are many other merge methods in Spark that are unrelated to AccumulatorV2.merge, such as:\n- util/StatCounter.merge (combines statistics)\n- shuffle writers and spill mergers (merge of files/buffers)\nThese are not included as \"related entities\" here since they do not override or invoke AccumulatorV2.merge.\n\n\n## Quick index of files and lines\n- core/util/AccumulatorV2.scala\n  - Declaration: ~150154\n  - LongAccumulator.merge: ~374381\n  - DoubleAccumulator.merge: ~452459\n  - CollectionAccumulator.merge: ~501505\n- sql/execution/metric/SQLMetrics.scala\n  - SQLMetric.merge: ~5662\n- sql/execution/debug/package.scala\n  - SetAccumulator.merge: ~222224\n- sql/execution/AggregatingAccumulator.scala\n  - AggregatingAccumulator.merge: around ~152200\n- sql/execution/streaming/EventTimeWatermarkExec.scala\n  - Accumulator merge for EventTimeStats: around ~7785\n- core/api/python/PythonRDD.scala\n  - PythonAccumulatorV2.merge: around ~714; super.merge invoked around ~720\n- core/scheduler/DAGScheduler.scala\n  - Call site: ~1610\n- core/executor/TaskMetrics.scala\n  - Call site: ~322\n\n\n## Selection rationale\nThe AccumulatorV2.merge method forms the core contract by which Spark combines per-task partial results and metrics on the driver. The entities listed are either concrete implementations of this contract or key call sites where this merging happens.\n",
          "answer_time": 192,
          "tokens": 0,
          "more_info": "Chose wrong merge method"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    },
    {
      "id": "Q012",
      "question": "What are all related entities of class objecthashaggregateexe?",
      "category": "top",
      "ground_truth_contexts": [
        "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#, with name: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec# kind: CLASS located in: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala:\n1.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics., name: sqlmetrics, kind: OBJECT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n2.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createMetric()., name: createmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n3.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createTimingMetric()., name: createtimingmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n4.Node_id: org/apache/spark/sql/execution/SparkPlan#sparkContext()., name: sparkcontext, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, relations: CALL\n5.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createSizeMetric()., name: createsizemetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n6.Node_id: org/apache/spark/sql/catalyst/plans/QueryPlan#output()., name: output, kind: METHOD, uri: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala, relations: CALL\n7.Node_id: org/apache/spark/sql/execution/aggregate/BaseAggregateExec#, name: baseaggregateexec, kind: TRAIT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala, relations: EXTEND\n8.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#toString()., name: tostring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n9.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#doExecute()., name: doexecute, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n10.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#verboseString()., name: verbosestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n11.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#withNewChildInternal()., name: withnewchildinternal, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION, RETURN_TYPE_BY\n12.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#simpleString()., name: simplestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n13.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#copy()., name: copy, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n14.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#`<init>`()., name: <init>, kind: CONSTRUCTOR, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION"
      ],
      "key_entities": [
        "org/apache/spark/sql/execution/metric/SQLMetrics.",
        "org/apache/spark/sql/execution/metric/SQLMetrics.createMetric().",
        "org/apache/spark/sql/execution/metric/SQLMetrics.createTimingMetric().",
        "org/apache/spark/sql/execution/SparkPlan#sparkContext().",
        "org/apache/spark/sql/execution/metric/SQLMetrics.createSizeMetric().",
        "org/apache/spark/sql/catalyst/plans/QueryPlan#output().",
        "org/apache/spark/sql/execution/aggregate/BaseAggregateExec#",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#toString().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#doExecute().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#verboseString().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#withNewChildInternal().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#simpleString().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#copy().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#`<init>`()."
      ],
      "claude_stats": {
        "answer": "Based on the analysis of the `ObjectHashAggregateExec` class, here are all its related entities:\n\n## External Dependencies (Called/Extended)\n\n1. **SQLMetrics** (OBJECT) - Used for metrics creation\n2. **createMetric()** (METHOD) - Called from SQLMetrics for metric creation\n3. **createTimingMetric()** (METHOD) - Called from SQLMetrics for timing metrics\n4. **sparkContext()** (METHOD) - Called from SparkPlan\n5. **createSizeMetric()** (METHOD) - Called from SQLMetrics for size metrics\n6. **output()** (METHOD) - Called from QueryPlan\n7. **BaseAggregateExec** (TRAIT) - Extended by ObjectHashAggregateExec\n\n## Internal Methods (Declared in ObjectHashAggregateExec)\n\n8. **toString()** - String representation method\n9. **doExecute()** - Execution method\n10. **verboseString()** - Verbose string representation\n11. **withNewChildInternal()** - Method for creating new instance with different child (also used as return type)\n12. **simpleString()** - Simple string representation\n13. **copy()** - Copy method\n14. **<init>()** - Constructor\n\nThe class primarily interacts with Spark's SQL execution metrics system (SQLMetrics) and extends the BaseAggregateExec trait, implementing various methods for execution and string representation.\n\n",
        "prompt_time": 0.163,
        "answer_time": 18,
        "context_tokens": 869,
        "tokens": 0,
        "used_context": [
          "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#, with name: objecthashaggregateexec kind: CLASS located in: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala:\n1.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics., name: sqlmetrics, kind: OBJECT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n2.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createMetric()., name: createmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n3.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createTimingMetric()., name: createtimingmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n4.Node_id: org/apache/spark/sql/execution/SparkPlan#sparkContext()., name: sparkcontext, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, relations: CALL\n5.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createSizeMetric()., name: createsizemetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n6.Node_id: org/apache/spark/sql/catalyst/plans/QueryPlan#output()., name: output, kind: METHOD, uri: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala, relations: CALL\n7.Node_id: org/apache/spark/sql/execution/aggregate/BaseAggregateExec#, name: baseaggregateexec, kind: TRAIT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala, relations: EXTEND\n8.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#toString()., name: tostring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n9.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#doExecute()., name: doexecute, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n10.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#verboseString()., name: verbosestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n11.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#withNewChildInternal()., name: withnewchildinternal, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION, RETURN_TYPE_BY\n12.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#simpleString()., name: simplestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n13.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#copy()., name: copy, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n14.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#`<init>`()., name: <init>, kind: CONSTRUCTOR, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "",
          "prompt_time": 0.143,
          "answer_time": 240,
          "context_tokens": 869,
          "used_context": [
            "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()., with name: merge kind: METHOD located in: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala:\n1.Node_id: org/apache/spark/ml/linalg/BLAS., name: blas, kind: OBJECT, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n2.Node_id: org/apache/spark/ml/linalg/BLAS.getBLAS()., name: getblas, kind: METHOD, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n3.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#, name: differentiablelossaggregator, kind: TRAIT, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: DECLARATION_BY\n4.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#[Agg], name: agg, kind: TYPE_PARAMETER, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: RETURN_TYPE\n5.Node_id: org/apache/spark/ml/optim/loss/RDDLossFunction#calculate()., name: calculate, kind: METHOD, uri: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala, relations: CALL_BY\n"
          ],
          "tokens": 0,
          "more_info": "Infinite loop in which it queried mcp and then tried to access backend file that stores questions for test (for which access was denied), so no answer"
        },
        "without_mcp": {
          "answer": "# Related entities of the class: ObjectHashAggregateExec\n\nThis document enumerates the primary code entities related to the physical operator ObjectHashAggregateExec in the Apache Spark 3.3.0 codebase. File paths are relative to the repository root.\n\n\n## Declaration (primary subject)\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala\n  - case class ObjectHashAggregateExec(...)\n  - Extends: BaseAggregateExec\n  - Purpose: Hash-based aggregate operator that supports TypedImperativeAggregate functions with arbitrary JVM object states; can fall back to sort-based aggregation.\n\n\n## Direct supertype and foundational traits/classes\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala\n  - trait BaseAggregateExec extends UnaryExecNode with AliasAwareOutputPartitioning\n  - Provides common machinery for aggregate exec nodes: inputAttributes ordering, aggregate buffer handling, distribution, and output partitioning contracts.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala\n  - SparkPlan: base of execution operators; ObjectHashAggregateExec is a SparkPlan node via BaseAggregateExec  UnaryExecNode.\n\n\n## Immediate collaborators used by ObjectHashAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala\n  - class ObjectAggregationIterator\n  - Used inside ObjectHashAggregateExec.doExecute to perform the core aggregation over partitions and to handle fallback to sort-based aggregation.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala\n  - class ObjectAggregationMap\n  - Hash map backing used by ObjectAggregationIterator; stores grouping keys and safe-row aggregation buffers compatible with JVM object states.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala\n  - Abstract iterator discussed in comments; documents relation with ObjectAggregationIterator and sort-based fallback path.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n  - ObjectHashAggregateExec is constructed here from logical aggregates; utility chooses appropriate physical aggregate (hash/object-hash/sort) based on expressions and configuration.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala\n  - Rule that may replace HashAggregateExec or ObjectHashAggregateExec with SortAggregateExec under certain preconditions (e.g., when child output is already sorted or for memory considerations).\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/RemoveRedundantProjects.scala\n  - Mentions BaseAggregateExec: keeps necessary Project nodes for correct column ordering required by aggregate execs (including ObjectHashAggregateExec) in Final or PartialMerge modes.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala\n  - References ObjectHashAggregateExec in streaming execution; handles stateful aggregates and tagging/planning nuances for streaming queries.\n\n\n## Expressions and types commonly used with ObjectHashAggregateExec\n- sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala\n  - TypedImperativeAggregate\n  - Note: ObjectHashAggregateExec supports TypedImperativeAggregate functions (e.g., percentile_approx, typed_count) by holding JVM object states in safe buffers.\n\n- sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/\n  - Expression, NamedExpression, Attribute, AttributeSeq, AggregateExpression, MutableProjection\n  - These are imported and used directly in ObjectHashAggregateExec for schema/row handling and codegen-free projection creation.\n\n\n## Configuration keys controlling ObjectHashAggregateExec\n- sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala\n  - OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD (\"spark.sql.objectHashAggregate.sortBased.fallbackThreshold\")\n    - Integer threshold of hash map entry count to trigger fallback to sort-based aggregation.\n  - USE_OBJECT_HASH_AGG (\"spark.sql.execution.useObjectHashAggregateExec\")\n    - Boolean flag to enable/disable ObjectHashAggregateExec.\n\n\n## Metrics and instrumentation\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala\n  - SQLMetrics used: numOutputRows, aggTime, spillSize, numTasksFallBacked\n  - Exposed via longMetric and tracked in doExecute.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala\n  - Helpers to create timing/size/count metrics consumed by ObjectHashAggregateExec.\n\n\n## Planner and rule interactions\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n  - Selects ObjectHashAggregateExec when query involves TypedImperativeAggregate or when configuration makes it favorable.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala\n  - May convert ObjectHashAggregateExec to SortAggregateExec based on ordering and partial/final aggregation structure.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala\n  - Interacts with BaseAggregateExec (indirectly includes ObjectHashAggregateExec) regarding distribution/partitioning assumptions.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala\n  - Mentions BaseAggregateExec to propagate empty relations; applies to ObjectHashAggregateExec when grouping keys are non-empty and children are empty.\n\n\n## Test suites and benchmarks referencing ObjectHashAggregateExec\n- sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala\n  - Ensures ObjectHashAggregateExec appears in plans when TypedImperativeAggregate functions are used; includes helper assertions and plan introspection.\n\n- sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala\n  - Comprehensive suite for ObjectHashAggregateExec behavior, enabling/disabling via SQLConf and checking fallback behavior.\n\n- sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveUDAFSuite.scala\n  - Captures cases where Hive UDAFs trigger ObjectHashAggregateExec.\n\n- sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala\n  - Verifies logical plan tags on ObjectHashAggregateExec for final aggregation detection.\n\n- sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala\n  - Asserts that planner can generate ObjectHashAggregateExec under suitable conditions.\n\n- sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingAggregationSuite.scala\n  - Discusses behavior where ObjectHashAggregateExec may fall back to sort-based in streaming aggregations.\n\n- Benchmarks:\n  - sql/hive/src/test/scala/org/apache/spark/sql/execution/benchmark/ObjectHashAggregateExecBenchmark.scala\n  - sql/hive/benchmarks/ObjectHashAggregateExecBenchmark-results.txt\n  - sql/hive/benchmarks/ObjectHashAggregateExecBenchmark-jdk11-results.txt\n  - sql/hive/benchmarks/ObjectHashAggregateExecBenchmark-jdk17-results.txt\n\n\n## Representative creation and usage sites\n- Construction:\n  - sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n    - ObjectHashAggregateExec(...) is instantiated from logical aggregates when appropriate.\n\n- Execution:\n  - sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala#doExecute\n    - Creates ObjectAggregationIterator with child.execute() iterator, grouping/aggregate expressions, attributes, and fallback threshold.\n\n- Streaming specialization:\n  - sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala\n    - Pattern-matches ObjectHashAggregateExec when a.isStreaming.\n\n\n## Related/alternative physical aggregate operators\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala\n  - The standard unsafe-row hash aggregate; may be chosen instead of ObjectHashAggregateExec when no TypedImperativeAggregate requiring object state is present.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala\n  - Sort-based aggregate; target of fallback and of the ReplaceHashWithSortAgg rule.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/MergingSessionsExec.scala\n  - Another BaseAggregateExec implementation (session window merges), illustrating the family of aggregate exec nodes.\n\n\n## Quick index of key files and lines\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala\n  - Declaration and doExecute: ~lines 60145\n- sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala\n  - OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD: ~20952106\n  - USE_OBJECT_HASH_AGG: ~21082114\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala\n  - Core iterator used by ObjectHashAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala\n  - Hash map for aggregation state\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n  - Chooses and constructs ObjectHashAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala\n  - Replacement logic for ObjectHashAggregateExec/HashAggregateExec  SortAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala\n  - Pattern match for streaming ObjectHashAggregateExec\n- sql/core/src/test/scala and sql/hive/src/test/scala\n  - Multiple suites asserting presence/behavior of ObjectHashAggregateExec\n\n\n## Selection rationale\nThe entities listed above either define ObjectHashAggregateExec, construct it from logical plans, provide the iterator/map implementations it relies on at runtime, configure its behavior via SQLConf, instrument its execution via SQLMetrics, or test/benchmark it directly. Together, they form the ecosystem necessary to understand and evolve ObjectHashAggregateExec in Spark 3.3.0.\n",
          "answer_time": 104,
          "time": 0,
          "tokens": 0
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 0,
          "score_b": 0,
          "reasoning": "Answer A is more complete and polished. It provides a comprehensive overview with well-structured tables, covers all constructors clearly, explains key methods including validation logic, and discusses update restrictions. Answer B appears incomplete (cuts off mid-sentence) and while it provides good technical detail about the class structure, it lacks the completeness and clear organization that makes Answer A superior for describing the DefaultAccount class."
        }
      }
    }
  ]
}